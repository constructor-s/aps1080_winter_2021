{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/A1/A1_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iS40R9okStg"
      },
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVX1AjRWkueO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03416b2-b93d-48cc-ff87-486848350ff3"
      },
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6jcjEHoIQ2"
      },
      "source": [
        "---------\r\n",
        "Data in this structure represents the *dynamics function $p$*, a mapping from (state index, action (action is represented by four possible integer values corresponding to $\\leftarrow$, $\\downarrow$, $\\rightarrow$, $\\uparrow$)) pairs to: \r\n",
        "\r\n",
        "1. Probability of transitioning to the next state (in this environment it is deterministic, i.e. always equal to 1)\r\n",
        "\r\n",
        "1. The next state (index)\r\n",
        "\r\n",
        "1. The reward (1.0 for state 15, 0.0 otherwise)\r\n",
        "\r\n",
        "1. Whether the state terminates the episode (`True`, hole or goal) or not (`False`, safe or frozen)\r\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyn_w3ulkyZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09cfa381-261c-4f20-9beb-1eda46ef7530"
      },
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zND5ArI8k_qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943e518f-cdd1-4e36-9af6-65fb65e17a0f"
      },
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tp9YzRljnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd8836c-15b2-4058-918f-91a3d0dc58fb"
      },
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFGNZNowluz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a9d7a7-36f0-4189-8be5-d75dafa7d5a7"
      },
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample from S: 5  ...  sample from A: 2\n",
            "sample from S: 15  ...  sample from A: 1\n",
            "sample from S: 11  ...  sample from A: 2\n",
            "sample from S: 7  ...  sample from A: 0\n",
            "sample from S: 1  ...  sample from A: 3\n",
            "sample from S: 8  ...  sample from A: 3\n",
            "sample from S: 2  ...  sample from A: 2\n",
            "sample from S: 6  ...  sample from A: 1\n",
            "sample from S: 15  ...  sample from A: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQL5JxsmcEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51228ac0-1be0-4207-dc23-b4aef347345a"
      },
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV6e43mmwx1"
      },
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "# Question: Explain what the objective of the agent is in this environment?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3jKoaobtZSx"
      },
      "source": [
        "------------\r\n",
        "\r\n",
        "| Index | Action |\r\n",
        "| ----------- | ----------- |\r\n",
        "| `0` | LEFT |\r\n",
        "| `1` | DOWN | \r\n",
        "| `2` | RIGHT | \r\n",
        "| `3` | UP | \r\n",
        "\r\n",
        "------------\r\n",
        "\r\n",
        "| Character | State |\r\n",
        "| ----------- | ----------- |\r\n",
        "| `S` | starting point, safe |\r\n",
        "| `F` | frozen surface, safe |\r\n",
        "| `H` | hole, fall to your doom |\r\n",
        "| `G` | goal, where the frisbee is located |\r\n",
        "\r\n",
        "------------\r\n",
        "\r\n",
        "The objective of the agent is to reach the goal (`G`) state\r\n",
        "\r\n",
        "------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWI3h6s7qqdq"
      },
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded)."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRwGwPoqw0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8ffb13-cc54-46ab-9396-73491a49c35f"
      },
      "source": [
        "#%% An example AI that takes random moves and renders the result at each step\r\n",
        "def play_agent(env, policy):\r\n",
        "    \"\"\"\r\n",
        "    Function to test drive an agent against the environment. \r\n",
        "    Terminates after the same state is repeated. \r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    env : Env\r\n",
        "        Gym environment\r\n",
        "    policy : callable\r\n",
        "        A function that takes `env` as input and returns a valid action\r\n",
        "    \"\"\"\r\n",
        "    env.reset()\r\n",
        "    previous_observation = None\r\n",
        "\r\n",
        "    env.render()\r\n",
        "    while True:\r\n",
        "        # env.render()\r\n",
        "\r\n",
        "        # Sample from action space using the built-in function\r\n",
        "        action = policy(env)\r\n",
        "        (observation, reward, compute, probability) = env.step(action)\r\n",
        "        print(\"--> The result of taking action\",action,\"is:\")\r\n",
        "        print(\"     S=\",observation)\r\n",
        "        print(\"     R=\",reward)\r\n",
        "        print(\"     p=\",probability)\r\n",
        "        env.render()\r\n",
        "\r\n",
        "        # Terminate if stuck in a state\r\n",
        "        if observation == previous_observation:\r\n",
        "            print(\"This episode has terminated\")\r\n",
        "            break\r\n",
        "        else:\r\n",
        "            previous_observation = observation\r\n",
        "\r\n",
        "env.seed(5)\r\n",
        "policy = lambda env: env.action_space.sample()\r\n",
        "play_agent(env, policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "This episode has terminated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "# (Optional): Repeat the above for q.\n",
        "#\n",
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDQD_OS-xdSK"
      },
      "source": [
        "----------\r\n",
        "\r\n",
        "The agent's policy function can be implemented as an $N$-element array lookup table of integer action indices, representing $p=100\\%$ choosing the that action at that state. \r\n",
        "\r\n",
        "Similarly, the value function can be implemented as an $N$-element array lookup table of float values. \r\n",
        "\r\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrAwRZNDyHyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a90b6c4-8f4e-4ad5-f1b3-69817434d0f4"
      },
      "source": [
        "# Practical: revise the above AI solver to use a policy function in which you\r\n",
        "# code the random action selections in the policy function. Test this.\r\n",
        "\r\n",
        "manual_policy = [\r\n",
        "    1, 2, 1, 0, \r\n",
        "    1, 0, 1, 0, \r\n",
        "    2, 2, 1, 2, \r\n",
        "    2, 2, 2, 2\r\n",
        "]\r\n",
        "policy = lambda env: manual_policy[env.env.s]\r\n",
        "play_agent(env, policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "This episode has terminated\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}