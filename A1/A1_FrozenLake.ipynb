{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1_FrozenLake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/A1/A1_FrozenLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGqubiDkQnd"
      },
      "source": [
        "import gym"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iS40R9okStg"
      },
      "source": [
        "gym.envs.register(\n",
        "    id='FrozenLakeNotSlippery-v0',\n",
        "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
        "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
        "    max_episode_steps=100,\n",
        "    reward_threshold=0.74\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVX1AjRWkueO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17fe8f9b-67f7-49cb-f7ff-f938ec3befb5"
      },
      "source": [
        "# Create the gridworld-like environment\n",
        "env=gym.make('FrozenLakeNotSlippery-v0')\n",
        "# Let's look at the model of the environment (i.e., P):\n",
        "env.env.P\n",
        "# Question: what is the data in this structure saying? Relate this to the course\n",
        "# presentation of P"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 4, 0.0, False)],\n",
              "  2: [(1.0, 1, 0.0, False)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 1: {0: [(1.0, 0, 0.0, False)],\n",
              "  1: [(1.0, 5, 0.0, True)],\n",
              "  2: [(1.0, 2, 0.0, False)],\n",
              "  3: [(1.0, 1, 0.0, False)]},\n",
              " 2: {0: [(1.0, 1, 0.0, False)],\n",
              "  1: [(1.0, 6, 0.0, False)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 3: {0: [(1.0, 2, 0.0, False)],\n",
              "  1: [(1.0, 7, 0.0, True)],\n",
              "  2: [(1.0, 3, 0.0, False)],\n",
              "  3: [(1.0, 3, 0.0, False)]},\n",
              " 4: {0: [(1.0, 4, 0.0, False)],\n",
              "  1: [(1.0, 8, 0.0, False)],\n",
              "  2: [(1.0, 5, 0.0, True)],\n",
              "  3: [(1.0, 0, 0.0, False)]},\n",
              " 5: {0: [(1.0, 5, 0, True)],\n",
              "  1: [(1.0, 5, 0, True)],\n",
              "  2: [(1.0, 5, 0, True)],\n",
              "  3: [(1.0, 5, 0, True)]},\n",
              " 6: {0: [(1.0, 5, 0.0, True)],\n",
              "  1: [(1.0, 10, 0.0, False)],\n",
              "  2: [(1.0, 7, 0.0, True)],\n",
              "  3: [(1.0, 2, 0.0, False)]},\n",
              " 7: {0: [(1.0, 7, 0, True)],\n",
              "  1: [(1.0, 7, 0, True)],\n",
              "  2: [(1.0, 7, 0, True)],\n",
              "  3: [(1.0, 7, 0, True)]},\n",
              " 8: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 12, 0.0, True)],\n",
              "  2: [(1.0, 9, 0.0, False)],\n",
              "  3: [(1.0, 4, 0.0, False)]},\n",
              " 9: {0: [(1.0, 8, 0.0, False)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 10, 0.0, False)],\n",
              "  3: [(1.0, 5, 0.0, True)]},\n",
              " 10: {0: [(1.0, 9, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 11, 0.0, True)],\n",
              "  3: [(1.0, 6, 0.0, False)]},\n",
              " 11: {0: [(1.0, 11, 0, True)],\n",
              "  1: [(1.0, 11, 0, True)],\n",
              "  2: [(1.0, 11, 0, True)],\n",
              "  3: [(1.0, 11, 0, True)]},\n",
              " 12: {0: [(1.0, 12, 0, True)],\n",
              "  1: [(1.0, 12, 0, True)],\n",
              "  2: [(1.0, 12, 0, True)],\n",
              "  3: [(1.0, 12, 0, True)]},\n",
              " 13: {0: [(1.0, 12, 0.0, True)],\n",
              "  1: [(1.0, 13, 0.0, False)],\n",
              "  2: [(1.0, 14, 0.0, False)],\n",
              "  3: [(1.0, 9, 0.0, False)]},\n",
              " 14: {0: [(1.0, 13, 0.0, False)],\n",
              "  1: [(1.0, 14, 0.0, False)],\n",
              "  2: [(1.0, 15, 1.0, True)],\n",
              "  3: [(1.0, 10, 0.0, False)]},\n",
              " 15: {0: [(1.0, 15, 0, True)],\n",
              "  1: [(1.0, 15, 0, True)],\n",
              "  2: [(1.0, 15, 0, True)],\n",
              "  3: [(1.0, 15, 0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a6jcjEHoIQ2"
      },
      "source": [
        "---------\r\n",
        "Data in this structure represents the *dynamics function $p$*, a mapping from (state index, action (action is represented by four possible integer values corresponding to $\\leftarrow$, $\\downarrow$, $\\rightarrow$, $\\uparrow$)) pairs to: \r\n",
        "\r\n",
        "1. Probability of transitioning to the next state (in this environment it is deterministic, i.e. always equal to 1)\r\n",
        "\r\n",
        "1. The next state (index)\r\n",
        "\r\n",
        "1. The reward (1.0 for state 15, 0.0 otherwise)\r\n",
        "\r\n",
        "1. Whether the state terminates the episode (`True`, hole or goal) or not (`False`, safe or frozen)\r\n",
        "---------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyn_w3ulkyZI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6024e126-fe33-4a2e-91c5-e41728bbc1a3"
      },
      "source": [
        "# Now let's investigate the observation space (i.e., S using our nomenclature),\n",
        "# and confirm we see it is a discrete space with 16 locations\n",
        "print(env.observation_space)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zND5ArI8k_qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3c0e09-264e-4d5c-fe05-5754a6249ccd"
      },
      "source": [
        "stateSpaceSize = env.observation_space.n\n",
        "print(stateSpaceSize)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_tp9YzRljnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fbb82e-4262-40e7-b2b5-3bd1a3d6c661"
      },
      "source": [
        "# Now let's investigate the action space (i.e., A) for the agent->environment\n",
        "# channel\n",
        "print(env.action_space)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFGNZNowluz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de8f816-d613-488d-ae18-12f6ce961653"
      },
      "source": [
        "# The gym environment has ...sample() functions that allow us to sample\n",
        "# from the above spaces:\n",
        "for g in range(1,10,1):\n",
        "  print(\"sample from S:\",env.observation_space.sample(),\" ... \",\"sample from A:\",env.action_space.sample())\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample from S: 0  ...  sample from A: 0\n",
            "sample from S: 1  ...  sample from A: 3\n",
            "sample from S: 12  ...  sample from A: 0\n",
            "sample from S: 2  ...  sample from A: 3\n",
            "sample from S: 4  ...  sample from A: 1\n",
            "sample from S: 12  ...  sample from A: 1\n",
            "sample from S: 7  ...  sample from A: 3\n",
            "sample from S: 5  ...  sample from A: 1\n",
            "sample from S: 12  ...  sample from A: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOQL5JxsmcEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96771e86-9588-4711-a6b9-f900fec4fb20"
      },
      "source": [
        "# The enviroment also provides a helper to render (visualize) the environment\n",
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLV6e43mmwx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b8b0c3-efbf-4916-9013-63128c151953"
      },
      "source": [
        "# We can act as the agent, by selecting actions and stepping the environment\n",
        "# through time to see its responses to our actions\n",
        "env.reset()\n",
        "exitCommand=False\n",
        "while not(exitCommand):\n",
        "  env.render()\n",
        "  print(\"Enter the action as an integer from 0 to\",env.action_space.n,\" (or exit): \")\n",
        "  userInput=input()\n",
        "  if userInput==\"exit\":\n",
        "    break\n",
        "  action=int(userInput)\n",
        "  (observation, reward, compute, probability) = env.step(action)\n",
        "  print(\"--> The result of taking action\",action,\"is:\")\n",
        "  print(\"     S=\",observation)\n",
        "  print(\"     R=\",reward)\n",
        "  print(\"     p=\",probability)\n",
        "\n",
        "  env.render()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Enter the action as an integer from 0 to 4  (or exit): \n",
            "exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tBpeiuRnyih"
      },
      "source": [
        "# Question: draw a table indicating the correspondence between the action\n",
        "# you input (a number) and the logic action performed.\n",
        "# Question: draw a table that illustrates what the symbols on the render image\n",
        "# mean?\n",
        "# Question: Explain what the objective of the agent is in this environment?"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3jKoaobtZSx"
      },
      "source": [
        "------------\r\n",
        "\r\n",
        "| Index | Action |\r\n",
        "| ----------- | ----------- |\r\n",
        "| `0` | LEFT |\r\n",
        "| `1` | DOWN | \r\n",
        "| `2` | RIGHT | \r\n",
        "| `3` | UP | \r\n",
        "\r\n",
        "------------\r\n",
        "\r\n",
        "| Character | State |\r\n",
        "| ----------- | ----------- |\r\n",
        "| `S` | starting point, safe |\r\n",
        "| `F` | frozen surface, safe |\r\n",
        "| `H` | hole, fall to your doom |\r\n",
        "| `G` | goal, where the frisbee is located |\r\n",
        "\r\n",
        "------------\r\n",
        "\r\n",
        "The objective of the agent is to reach the goal (`G`) state\r\n",
        "\r\n",
        "------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWI3h6s7qqdq"
      },
      "source": [
        "# Practical: Code up an AI that will employ random action selection in order\n",
        "# to drive the agent. Test this random action selection agent with the\n",
        "# above environment (i.e., code up a loop as I did above, but instead\n",
        "# of taking input from a human user, take it from the AI you coded)."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmRwGwPoqw0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0302f8fd-6c03-4c72-fd83-0f5edf002e22"
      },
      "source": [
        "#%% An example AI that takes random moves and renders the result at each step\r\n",
        "def play_agent(env, policy, terminate_after_n=5):\r\n",
        "    \"\"\"\r\n",
        "    Function to test drive an agent against the environment. \r\n",
        "    Terminates after the same state is repeated n times. \r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    env : Env\r\n",
        "        Gym environment\r\n",
        "    policy : callable\r\n",
        "        A function that takes `env` as input and returns a valid action. \r\n",
        "        Assumes the policy only returns one action with 100% probability.\r\n",
        "    \"\"\"\r\n",
        "    env.reset()\r\n",
        "    previous_observation = None\r\n",
        "    repeats = 0\r\n",
        "\r\n",
        "    env.render()\r\n",
        "    while True:\r\n",
        "        # env.render()\r\n",
        "\r\n",
        "        # Sample from action space using the built-in function\r\n",
        "        action = policy(env)\r\n",
        "        (observation, reward, compute, probability) = env.step(action)\r\n",
        "        print(\"--> The result of taking action\",action,\"is:\")\r\n",
        "        print(\"     S=\",observation)\r\n",
        "        print(\"     R=\",reward)\r\n",
        "        print(\"     p=\",probability)\r\n",
        "        env.render()\r\n",
        "\r\n",
        "        # Terminate if stuck in a state\r\n",
        "        if observation == previous_observation:\r\n",
        "            repeats += 1\r\n",
        "            if repeats >= terminate_after_n:\r\n",
        "                break\r\n",
        "        else:\r\n",
        "            repeats = 0\r\n",
        "            previous_observation = observation\r\n",
        "\r\n",
        "env.seed(5)\r\n",
        "policy = lambda env: env.action_space.sample()\r\n",
        "play_agent(env, policy)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 10\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 11\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 11\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 11\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 11\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 11\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 11\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "FFF\u001b[41mH\u001b[0m\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQLKuubzrl4L"
      },
      "source": [
        "# Now towards dynamic programming. Note that env.env.P has the model\n",
        "# of the environment.\n",
        "#\n",
        "# Question: How would you represent the agent's policy function and value function?\n",
        "# Practical: revise the above AI solver to use a policy function in which you\n",
        "# code the random action selections in the policy function. Test this.\n",
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\n",
        "# randomly initialize your policy function, and compute its value function.\n",
        "# Report your results: policy and value function. Ensure your prediction\n",
        "# algo reports how many iterations it took.\n",
        "#\n",
        "# (Optional): Repeat the above for q.\n",
        "#\n",
        "# Policy Improvement:\n",
        "# Question: How would you use P and your value function to improve an arbitrary\n",
        "# policy, pi, per Chapter 4?\n",
        "# Practical: Code the policy iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, and ensure\n",
        "# it reports the number of iterations for each step: (a) overall policy\n",
        "# iteration steps and (b) evaluation steps.\n",
        "# Practical: Code the value iteration process, and employ it to arrive at a\n",
        "# policy that solves this problem. Show your testing results, reporting\n",
        "# the iteration counts.\n",
        "# Comment on the difference between the iterations required for policy vs\n",
        "# value iteration.\n",
        "#\n",
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\n",
        "# env = gym.make(\"FrozenLake-v0\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDQD_OS-xdSK"
      },
      "source": [
        "----------\r\n",
        "\r\n",
        "The agent's policy function can be implemented as an $N$-element array lookup table of integer action indices, representing $p=100\\%$ choosing the that action at that state. \r\n",
        "\r\n",
        "Similarly, the value function can be implemented as an $N$-element array lookup table of float values. \r\n",
        "\r\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrAwRZNDyHyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf734f36-e1c2-44c3-ee51-9ec8bb59455b"
      },
      "source": [
        "# Practical: revise the above AI solver to use a policy function in which you\r\n",
        "# code the random action selections in the policy function. Test this.\r\n",
        "\r\n",
        "manual_policy = [\r\n",
        "    1, 2, 1, 0, \r\n",
        "    1, 0, 1, 0, \r\n",
        "    2, 1, 1, 0, \r\n",
        "    0, 2, 2, 0\r\n",
        "]\r\n",
        "policy = lambda env: manual_policy[env.env.s]\r\n",
        "play_agent(env, policy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygdGwX3aJCV-",
        "outputId": "9acc8738-f2a4-48a8-e618-c3aca2f81c3e"
      },
      "source": [
        "# Practical: Code the C-4 Policy Evaluation (Prediction) algorithm. You may use\r\n",
        "# either the inplace or ping-pong buffer (as described in the lecture). Now\r\n",
        "# randomly initialize your policy function, and compute its value function.\r\n",
        "# Report your results: policy and value function. Ensure your prediction\r\n",
        "# algo reports how many iterations it took.\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def policy_evaluation(policy, theta, gamma, env, V=None, print_n_iter=False):\r\n",
        "    \"\"\"\r\n",
        "    Iterative policy evaluation, for estimating $V\\approx v_\\pi$, on Page 75\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    policy : array\r\n",
        "        A array that takes state index as index and maps to a valid action. \r\n",
        "        Assumes the policy only returns one action with 100% probability.\r\n",
        "\r\n",
        "    theta : float\r\n",
        "        a small threshold theta > 0 determining accuracy of estimation\r\n",
        "\r\n",
        "    gamma : float\r\n",
        "        reward discount rate\r\n",
        "\r\n",
        "    env : Env\r\n",
        "        Gym environment\r\n",
        "\r\n",
        "    V : array\r\n",
        "        current value function table. If None, initialize arbitrarily to all zeros.\r\n",
        "\r\n",
        "    print_n_iter : bool\r\n",
        "        print information about number of iterations took\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -----------\r\n",
        "    V : array\r\n",
        "        value function table\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # inplace buffer\r\n",
        "    # Initialize V (s), for all s in S+, arbitrarily except that V (terminal) = 0\r\n",
        "    S = env.observation_space.n\r\n",
        "    if V is None:\r\n",
        "        V = np.zeros(S)\r\n",
        "\r\n",
        "    n_iter = 0\r\n",
        "    while True:\r\n",
        "        n_iter += 1\r\n",
        "\r\n",
        "        delta = 0\r\n",
        "        for s in range(S):\r\n",
        "            v = V[s]\r\n",
        "\r\n",
        "            # Simplified logic for the simple case in our environment\r\n",
        "            a = policy[s]\r\n",
        "            V[s] = 0\r\n",
        "            for p, s_, r, terminate in env.P[s][a]: # s_ is next state s'\r\n",
        "                V[s] += p * (r + gamma * V[s_])\r\n",
        "            \r\n",
        "            delta = max(delta, abs(v - V[s]))\r\n",
        "        \r\n",
        "        if delta < theta:\r\n",
        "            if print_n_iter:\r\n",
        "                print(f\"policy_evaluation converged in {n_iter} itreations\")\r\n",
        "            return V\r\n",
        "\r\n",
        "def initialize_v(env, seed=None):\r\n",
        "    # Intialize V(s) arbitrarily except that V(terminal) = 0\r\n",
        "    S = env.observation_space.n\r\n",
        "    V = np.random.RandomState(seed).rand(16)\r\n",
        "    for i, letter in enumerate(env.desc.ravel()):\r\n",
        "        if letter in b'GH':\r\n",
        "            V[i] = 0\r\n",
        "    return V\r\n",
        "\r\n",
        "with np.printoptions(precision=3, suppress=True):\r\n",
        "    print(\"Randomly initialized policy:\")\r\n",
        "    policy = np.random.RandomState(0).randint(0, 4, size=16) # Random policy\r\n",
        "    print(policy.reshape(4, 4))\r\n",
        "    print(np.array([[\"\\u2190\", \"\\u2193\", \"\\u2192\", \"\\u2191\"][a] for a in policy]).reshape(4, 4))\r\n",
        "\r\n",
        "    V_init = initialize_v(env, seed=0)\r\n",
        "    # print(V_init.reshape(4, 4))\r\n",
        "    V = policy_evaluation(policy, 1e-6, 0.9, env, V=V_init, print_n_iter=True)\r\n",
        "\r\n",
        "    print(\"Final values:\")\r\n",
        "    print(V.reshape(4, 4))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Randomly initialized policy:\n",
            "[[0 3 1 0]\n",
            " [3 3 3 3]\n",
            " [1 3 1 2]\n",
            " [0 3 2 0]]\n",
            "[['←' '↑' '↓' '←']\n",
            " ['↑' '↑' '↑' '↑']\n",
            " ['↓' '↑' '↓' '→']\n",
            " ['←' '↑' '→' '←']]\n",
            "policy_evaluation converged in 56 itreations\n",
            "Final values:\n",
            "[[0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0. ]\n",
            " [0.  0.  0.9 0. ]\n",
            " [0.  0.  1.  0. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQwxRguPQz1K"
      },
      "source": [
        "> Policy Improvement:\r\n",
        ">\r\n",
        "> Question: How would you use P and your value function to improve an arbitrary\r\n",
        "> policy, pi, per Chapter 4?\r\n",
        "--------------------\r\n",
        "\r\n",
        "Using the *policy iteration* approach: iteratively interleave policy evaluation and policy improvement until convergence of value and policy functions.\r\n",
        "\r\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000J4Q_ZJCKH",
        "outputId": "9c0677e5-2856-4fff-9fad-1c06b0f1eaa9"
      },
      "source": [
        "# Practical: Code the policy iteration process, and employ it to arrive at a\r\n",
        "# policy that solves this problem. Show your testing results, and ensure\r\n",
        "# it reports the number of iterations for each step: (a) overall policy\r\n",
        "# iteration steps and (b) evaluation steps.\r\n",
        "\r\n",
        "REWARD = 2\r\n",
        "def policy_iteration(theta, gamma, env, seed=None, print_n_iter=False):\r\n",
        "    \"\"\"\r\n",
        "    Policy iteration (using iterative policy evaluation)\r\n",
        "    for estimating $\\pi \\approx \\pi_*$\r\n",
        "    page 80\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    theta : float\r\n",
        "        a small threshold theta > 0 determining accuracy of estimation\r\n",
        "        used in policy evaluation\r\n",
        "\r\n",
        "    gamma : float\r\n",
        "        reward discount rate\r\n",
        "\r\n",
        "    env : Env\r\n",
        "        Gym environment\r\n",
        "\r\n",
        "    seed : int\r\n",
        "        Seed for random number generation\r\n",
        "\r\n",
        "    print_n_iter : bool\r\n",
        "        print information about number of iterations took\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -----------\r\n",
        "    V : array\r\n",
        "        Value function table\r\n",
        "\r\n",
        "    policy : array\r\n",
        "        A array that takes state index as index and maps to a valid action. \r\n",
        "        Assumes the policy only returns one action with 100% probability.\r\n",
        "    \"\"\"\r\n",
        "    S = env.observation_space.n\r\n",
        "    A = env.action_space.n\r\n",
        "\r\n",
        "    # Initialization\r\n",
        "    V = initialize_v(env, seed=seed)\r\n",
        "    pi = np.random.RandomState(seed=seed).randint(0, A, size=S) # Random policy\r\n",
        "    \r\n",
        "    policy_iteration_iter_i = 0\r\n",
        "    while True:\r\n",
        "        policy_iteration_iter_i += 1\r\n",
        "        if print_n_iter:\r\n",
        "            print(f\"Policy iteration: iteration {policy_iteration_iter_i}\")\r\n",
        "\r\n",
        "        # policy evaluation\r\n",
        "        # print(V.reshape(4, 4))\r\n",
        "        V = policy_evaluation(pi, theta, gamma, env, V=V, print_n_iter=print_n_iter)\r\n",
        "        # print(V.reshape(4, 4))\r\n",
        "\r\n",
        "        # policy improvement\r\n",
        "        # print(pi)\r\n",
        "        policy_stable = True\r\n",
        "        for s in range(S):\r\n",
        "            old_action = pi[s]\r\n",
        "\r\n",
        "            values_after_action = np.zeros(A)\r\n",
        "            for a in range(A):\r\n",
        "                for p, s_, r, terminate in env.P[s][a]:\r\n",
        "                    values_after_action[a] += p * (r + gamma * V[s_])\r\n",
        "            pi[s] = np.argmax(values_after_action)\r\n",
        "\r\n",
        "            if old_action != pi[s]:\r\n",
        "                policy_stable = False\r\n",
        "        # print(pi)\r\n",
        "        if policy_stable:\r\n",
        "            return V, pi\r\n",
        "\r\n",
        "with np.printoptions(precision=3, suppress=True):\r\n",
        "    V_star, pi_star = policy_iteration(1e-2, 0.9, env, seed=0, print_n_iter=True)\r\n",
        "    print(\"Final value:\")\r\n",
        "    print(V_star.reshape(4, 4))\r\n",
        "    print(\"Final policy:\")\r\n",
        "    print(pi_star.reshape(4, 4))\r\n",
        "    print(np.array([[\"\\u2190\", \"\\u2193\", \"\\u2192\", \"\\u2191\"][a] for a in pi_star]).reshape(4, 4))\r\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy iteration: iteration 1\n",
            "policy_evaluation converged in 12 itreations\n",
            "Policy iteration: iteration 2\n",
            "policy_evaluation converged in 3 itreations\n",
            "Policy iteration: iteration 3\n",
            "policy_evaluation converged in 2 itreations\n",
            "Policy iteration: iteration 4\n",
            "policy_evaluation converged in 2 itreations\n",
            "Policy iteration: iteration 5\n",
            "policy_evaluation converged in 2 itreations\n",
            "Final value:\n",
            "[[0.59  0.656 0.729 0.656]\n",
            " [0.656 0.    0.81  0.   ]\n",
            " [0.729 0.81  0.9   0.   ]\n",
            " [0.    0.9   1.    0.   ]]\n",
            "Final policy:\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "[['↓' '→' '↓' '←']\n",
            " ['↓' '←' '↓' '←']\n",
            " ['→' '↓' '↓' '←']\n",
            " ['←' '→' '→' '←']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eneJUmqOXGIe",
        "outputId": "aba68f73-56e4-4954-ed99-0d9c5053555f"
      },
      "source": [
        "print(\"Testing: \")\r\n",
        "play_agent(env, lambda env: pi_star[env.env.s])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing: \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMLnYwfjbL0v",
        "outputId": "bd7a3c50-618e-48c0-bacb-47fb89004912"
      },
      "source": [
        "# Practical: Code the value iteration process, and employ it to arrive at a\r\n",
        "# policy that solves this problem. Show your testing results, reporting\r\n",
        "# the iteration counts.\r\n",
        "\r\n",
        "def value_iteration(theta, gamma, env, seed=None, print_n_iter=False):\r\n",
        "    \"\"\"\r\n",
        "    Value iteration for estimating $\\pi \\approx \\pi_*$\r\n",
        "    page 83\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ----------\r\n",
        "    theta : float\r\n",
        "        a small threshold theta > 0 determining accuracy of estimation\r\n",
        "        used in policy evaluation\r\n",
        "\r\n",
        "    gamma : float\r\n",
        "        reward discount rate\r\n",
        "\r\n",
        "    env : Env\r\n",
        "        Gym environment\r\n",
        "\r\n",
        "    seed : int\r\n",
        "        Seed for random number generation\r\n",
        "\r\n",
        "    print_n_iter : bool\r\n",
        "        print information about number of iterations took\r\n",
        "\r\n",
        "    Returns\r\n",
        "    -----------\r\n",
        "    V : array\r\n",
        "        Value function table\r\n",
        "\r\n",
        "    policy : array\r\n",
        "        A array that takes state index as index and maps to a valid action. \r\n",
        "        Assumes the policy only returns one action with 100% probability.\r\n",
        "    \"\"\"\r\n",
        "    S = env.observation_space.n\r\n",
        "    A = env.action_space.n\r\n",
        "\r\n",
        "    # Initialization\r\n",
        "    V = initialize_v(env, seed=seed)\r\n",
        "\r\n",
        "    # Value iteration\r\n",
        "    n_iter = 0\r\n",
        "    while True:\r\n",
        "        n_iter += 1\r\n",
        "\r\n",
        "        delta = 0\r\n",
        "        for s in range(S):\r\n",
        "            v = V[s]\r\n",
        "\r\n",
        "            values_after_action = np.zeros(A)\r\n",
        "            for a in range(A):\r\n",
        "                for p, s_, r, terminate in env.P[s][a]: # s_ is next state s'\r\n",
        "                    values_after_action[a] += p * (r + gamma * V[s_])\r\n",
        "            V[s] = max(values_after_action)\r\n",
        "\r\n",
        "            delta = max(delta, abs(v - V[s]))\r\n",
        "        \r\n",
        "        if delta < theta:\r\n",
        "            if print_n_iter:\r\n",
        "                print(f\"Value iteration converged in {n_iter} itreations\")\r\n",
        "            break\r\n",
        "\r\n",
        "    pi = np.zeros(S, dtype=np.int)\r\n",
        "    for s in range(S):\r\n",
        "        values_after_action = np.zeros(A)\r\n",
        "        for a in range(A):\r\n",
        "            for p, s_, r, terminate in env.P[s][a]:\r\n",
        "                values_after_action[a] += p * (r + gamma * V[s_])\r\n",
        "        pi[s] = np.argmax(values_after_action)\r\n",
        "\r\n",
        "    return V, pi\r\n",
        "\r\n",
        "with np.printoptions(precision=3, suppress=True):\r\n",
        "    V_star, pi_star = value_iteration(1e-2, 0.9, env, seed=0, print_n_iter=True)\r\n",
        "    print(\"Final value:\")\r\n",
        "    print(V_star.reshape(4, 4))\r\n",
        "    print(\"Final policy:\")\r\n",
        "    print(pi_star.reshape(4, 4))\r\n",
        "    print(np.array([[\"\\u2190\", \"\\u2193\", \"\\u2192\", \"\\u2191\"][a] for a in pi_star]).reshape(4, 4))\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value iteration converged in 7 itreations\n",
            "Final value:\n",
            "[[0.59  0.656 0.729 0.656]\n",
            " [0.656 0.    0.81  0.   ]\n",
            " [0.729 0.81  0.9   0.   ]\n",
            " [0.    0.9   1.    0.   ]]\n",
            "Final policy:\n",
            "[[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "[['↓' '→' '↓' '←']\n",
            " ['↓' '←' '↓' '←']\n",
            " ['→' '↓' '↓' '←']\n",
            " ['←' '→' '→' '←']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhyCAL_rtqIU",
        "outputId": "b2561c63-c02a-4a1f-89f7-45d8b0f94a4e"
      },
      "source": [
        "print(\"Testing: \")\r\n",
        "play_agent(env, lambda env: pi_star[env.env.s])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing: \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 1.0}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhVCrl9tpa7Q"
      },
      "source": [
        "\r\n",
        "> Comment on the difference between the iterations required for policy vs\r\n",
        "> value iteration.\r\n",
        "\r\n",
        "----------------------\r\n",
        "\r\n",
        "Policy iteration required 5 iterations, each of which consisted of 2 to 12 policy iterations to converge to the solution. In comparison, value iteration only took 7 iterations in total to converge, much faster than the policy iteration approach.\r\n",
        "\r\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xA2kuQoEHdn",
        "outputId": "cb39952d-934d-4565-ba70-212f63b39107"
      },
      "source": [
        "# Optional: instead of the above environment, use the \"slippery\" Frozen Lake via\r\n",
        "env = gym.make(\"FrozenLake-v0\")\r\n",
        "\r\n",
        "# policy_iteration\r\n",
        "with np.printoptions(precision=3, suppress=True):\r\n",
        "    V_star, pi_star = policy_iteration(1e-64, 0.9, env, seed=0, print_n_iter=True)\r\n",
        "    print(\"Final value:\")\r\n",
        "    print(V_star.reshape(4, 4))\r\n",
        "    print(\"Final policy:\")\r\n",
        "    print(pi_star.reshape(4, 4))\r\n",
        "    print(np.array([[\"\\u2190\", \"\\u2193\", \"\\u2192\", \"\\u2191\"][a] for a in pi_star]).reshape(4, 4))\r\n",
        "\r\n",
        "print(\"Testing: \")\r\n",
        "play_agent(env, lambda env: pi_star[env.env.s])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Policy iteration: iteration 1\n",
            "policy_evaluation converged in 123 itreations\n",
            "Policy iteration: iteration 2\n",
            "policy_evaluation converged in 44 itreations\n",
            "Policy iteration: iteration 3\n",
            "policy_evaluation converged in 49 itreations\n",
            "Final value:\n",
            "[[0.008 0.01  0.02  0.006]\n",
            " [0.018 0.    0.055 0.   ]\n",
            " [0.049 0.108 0.164 0.   ]\n",
            " [0.    0.147 0.382 0.   ]]\n",
            "Final policy:\n",
            "[[1 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 2 0]]\n",
            "[['↓' '↑' '←' '↑']\n",
            " ['←' '←' '←' '←']\n",
            " ['↑' '↓' '←' '←']\n",
            " ['←' '→' '→' '←']]\n",
            "Testing: \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 1\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRO7ChcuEU_3",
        "outputId": "7ac4ef2b-6236-4633-99a2-d26ccd1396e6"
      },
      "source": [
        "# value_iteration\r\n",
        "\r\n",
        "with np.printoptions(precision=3, suppress=True):\r\n",
        "    V_star, pi_star = value_iteration(1e-64, 0.9, env, seed=0, print_n_iter=True)\r\n",
        "    print(\"Final value:\")\r\n",
        "    print(V_star.reshape(4, 4))\r\n",
        "    print(\"Final policy:\")\r\n",
        "    print(pi_star.reshape(4, 4))\r\n",
        "    print(np.array([[\"\\u2190\", \"\\u2193\", \"\\u2192\", \"\\u2191\"][a] for a in pi_star]).reshape(4, 4))\r\n",
        "\r\n",
        "print(\"Testing: \")\r\n",
        "play_agent(env, lambda env: pi_star[env.env.s])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value iteration converged in 202 itreations\n",
            "Final value:\n",
            "[[0.069 0.061 0.074 0.056]\n",
            " [0.092 0.    0.112 0.   ]\n",
            " [0.145 0.247 0.3   0.   ]\n",
            " [0.    0.38  0.639 0.   ]]\n",
            "Final policy:\n",
            "[[0 3 0 3]\n",
            " [0 0 0 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            "[['←' '↑' '←' '↑']\n",
            " ['←' '←' '←' '←']\n",
            " ['↑' '↓' '←' '←']\n",
            " ['←' '→' '↓' '←']]\n",
            "Testing: \n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 0\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 4\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "--> The result of taking action 0 is:\n",
            "     S= 8\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "--> The result of taking action 3 is:\n",
            "     S= 9\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Up)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 13\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "--> The result of taking action 2 is:\n",
            "     S= 14\n",
            "     R= 0.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "--> The result of taking action 1 is:\n",
            "     S= 15\n",
            "     R= 1.0\n",
            "     p= {'prob': 0.3333333333333333}\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "--> The result of taking action 0 is:\n",
            "     S= 15\n",
            "     R= 0\n",
            "     p= {'prob': 1.0}\n",
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vShXmNFwQv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}