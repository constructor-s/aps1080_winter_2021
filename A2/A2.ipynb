{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/A2/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOcmu-k8RS2"
      },
      "source": [
        "# A2: Monte Carlo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnZ2Ue2j8NcF"
      },
      "source": [
        "<!-- Lesson -->\r\n",
        "<!-- ------ -->\r\n",
        "\r\n",
        "### Recap\r\n",
        "\r\n",
        "> Where are we so far in the course? We, in the introduction, mentioned that the RL problem is the problem of designing the intelligence that resides inside an agent (M) enabling it to control an environment (E). The agent is coupled to the environment via: (a) an actuation signal, A\\_t, from M to E, and (b) state, S\\_t, and reward, R\\_T, feedback signals from E to M. The agent must control the environment by selection appropriate actions, A\\_k, based on the feedback it receives, S\\_t and R\\_t. The agent selects its action via a _policy_. The policy may be deterministic or stochastic.\r\n",
        "> \r\n",
        "> This is exactly a classical control theoretic formulation of controller and plant. What, then, is so special that we call this \"reinforcement learning\"? In contrast to classical control theory, we do not assume that we have a full model of E; hence, we must _learn_ something about the environment in order to control it.\r\n",
        "> \r\n",
        "> With dynamic programming, we had perfect knowledge of the environment; i.e., we had access to p(s',r|s,a), the state transition dynamics of E. Using this, we posed the problem of prediction (or policy evaluation, Eval): given a policy pi, and an environment (specified by p), we can iteratively execute a process to obtain, in the limit as n (the number of iterations) goes to infinity, V\\_pi (the state value function of the policy). The foundation of the iterative process was the recursive definition of return and the Bellman relationship. We could do the same of Q\\_pi.\r\n",
        "> \r\n",
        "> We then posed a strategy for policy improvement (Impr), where by if we have a policy pi\\_k, and the associated value function V\\_k, we could propose an improved policy such that in any state, s, we chose the action, a, such that the next state of E, s', would have maximum V\\_pi\\_k(s'). Alternatively, we could simply chose, if we had Q\\_pi\\_k, a=argmax\\_a Q\\_pi\\_k(s, a).\r\n",
        "> \r\n",
        "> Finally, proposed a strategy for control of this environment by an agent. The agent (a) starts with a policy, pi\\_0, (b) executes Eval to obtain the value function, (c) executes Impr to give us pi\\_1, the improved policy, (d) and continues to iterate, alternating Eval and Iter. Since, theoretically, Eval is a process that converges to the value function in the limit (n->\\\\Infty), we illustrated the pragmatic compromise of truncated Eval (Eval\\_trunc) instead of full Eval, and commented that this will also converge to the optimal policy, pi\\_\\*.\r\n",
        "> \r\n",
        "> We note here that for DynProg we could have presented a very straightforward analytical alternative that did not require any of the above; since E is well known (via p), we could have formulated the synthesis of pi\\_\\* via a set of nonlinear equations. However, this is of limited use for cases where we don't have p. The structure of the iterative solution mechanism is generally applicable to a variety of cases, as we will see.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th2YO-Cb-zNo"
      },
      "source": [
        "### Monte Carlo Part A\r\n",
        "\r\n",
        "> The first part of our Monte Carlo presentation showed us how the situation changes when we do not have a model (i.e., p). In this case, there is no chance of an analytical synthesis method, and we must employ an iterative approach that is structurally aligned with what we did for DynProg.\r\n",
        "> \r\n",
        "> Monte Carlo methods, in general, are rooted in the idea that we can sample a phenomenon of interest many times, and average the returns to obtain meaningful data. In the context of Reinforcement Learning, we are sampling E's rewards, and averaging them, to estimate the return, and hence the value function. We presented how to do this to solve the policy evaluation (Eval) problem.\r\n",
        "> \r\n",
        "> Policy improvement (Impr) of course relies on a value function to improve policy pi\\_k to pi\\_{k+1}. Since we don't have access to the transition dynamics, p, of E, V\\_pi\\_k is not of much use in Impr. Rather, we need Q\\_pi\\_k so that we can derive the improved policy such that at state s, it choses action a=argmax\\_a Q\\_pi\\_k(s, a). So here we see how Q and V are distinct.\r\n",
        "> \r\n",
        "> We also see, with Monte Carlo, how the notion of RL being a paradigm for control of unknown environments where the agent \"learns through experience\". Monte Carlo methods are based on random sampling of a phenomenon and computation of averages to learn about the efficacy of the agent's action selection policy (i.e., learn the value functions).\r\n",
        "> \r\n",
        "> We also obtain our motivation for stochastic policies at this point. Since we are learning an action value function (state-action value function, Q(s, a)), we must sample the entire space of the domain, i.e. S x A. That is, our agent must -- to learn effectively -- experience the full domain, S x A. In other words, pi(s, a) > 0 for all s, for all a \\\\in A(s) --- all actions that are possible, must have a non-zero probability of being chosen. This is only possible with a stochastic policy.\r\n",
        "> \r\n",
        "> One can also pose the requirement as Monte Carlo (to this point) expects exploring states in the set of episodes that it has to learn from. This means that the episodes must have sufficient coverage of all (s, a) pairs. This is a very onerous requirement; the stochastic policy is far more reasonable comparatively. This leads us to part B of MC. But for now, regardless of these requirements, we can see that we have the elements of a \"learning-by-experience\" AI, with no \"magical thinking\" (i.e., no appeal to undefined, amorphous concepts) required. Nice!\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejj2A1Ip-zcY"
      },
      "source": [
        "#### Exercises\r\n",
        "\r\n",
        "> 1\\. Explain clearly why V\\_pi is not useful in the MC development above?\r\n",
        "> \r\n",
        "> 2\\. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q\\_pi\\_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:\r\n",
        "> \r\n",
        "> a. Will we obtain Q\\_pi\\_k from eval?\r\n",
        "> \r\n",
        "> b. If not why are we able to truncate Eval? Explain clearly.\r\n",
        "> \r\n",
        "> c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval\\_trunc, is it possible to converge on a sub-optimal policy pi\\_c? Is this a stable fixed point of the GPI for MC? Explain clearly.\r\n",
        "> \r\n",
        "> 3\\. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvBTGvrG-zmF"
      },
      "source": [
        "### Monte Carlo Part B-1: Stochastic Policies\r\n",
        "\r\n",
        "> Let's remove the requirement for the exploring starts requirement. The straightforward way to do this is to resort to what we mentioned above -- stochastic policies. Let's make this introduction of \"randomness\" concrete and purposeful. In a state s, we have a set of actions, A(s), that the action can take. We also have the greedy action, a = argmax\\_a(Q(s, a)). We now allow all actions to be selected with some epsilon probability, but bias in favor of selecting the greedy action. We select all actions with some probability epsilon/|A(s)| (where |A(s)| is the size of the set A(s)), while selecting the greedy action with probability 1-epsilon + epsilon/|A(s)|. You can clearly see that this scheme extends to cases when multiple equiprobable greedy actions exist.\r\n",
        "> \r\n",
        "> This scheme of formulating a stochastic policy, the epsilon-soft policy, can be used to find a quasi-optimal policy, without the onerous requirements of exploring starts. A good result, that makes MC more practical.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nARxsm9-zoU"
      },
      "source": [
        "### Monte Carlo Part B-2: Off Policy Methods\r\n",
        "\r\n",
        "#### On Policy Methods\r\n",
        "\r\n",
        "> The final element of this topic is the notion of on and off policy learning. On Policy learning is what we've been doing so far, and is a special case of Off Policy, as we'll see soon. In On Policy learning, we are executing GPI to identify the optimal control policy, pi\\_\\*, and we're using the instantaneous policy, pi\\_k, as the behaviour generation policy.\r\n",
        "> \r\n",
        "> We should realize, however, that there is a conflict in the above. Let's say we start with a stochastic initial policy, pi\\_0 (recall, we know at least one means to obtain this). We can see that as the GPI iteration proceeds through the interleaved iteration of Eval\\_trunc and Impr, we will get better and better policies.\r\n",
        "> \r\n",
        "> Now, on the one hand these better policies -- **to be useful for control** (i.e., our ultimate goal) -- must be progressively more strongly deterministic (barring ties, which introduce the only necessary randomness, if we have no deterministic way to break the tie).\r\n",
        "> \r\n",
        "> On the other hand, however, these policies -- **to be useful for evaluation** (our sub-goal GPI) -- must be effective samplers (\"explorers\") of the S x A space, which suggests randomness (specifically, for all s, pi(s, a) > 0 for all a \\\\in A(s)). Recall: we need evaluation to succeed, because it is the foundation for greedy improvement.\r\n",
        "> \r\n",
        "> This is a very real problem that must be addressed, which we'll do by generalizing to the use of two policies.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04ACdu7-zrE"
      },
      "source": [
        "#### Off Policy Methods\r\n",
        "\r\n",
        "> In the general case of off-policy learning we have two policies of interest, pi and b. Pi is the policy we are trying to solve for in GPI -- the optimal policy that will allow our agent to control the environment optimally. Policy b is a behaviour generation (\"exploratory\") policy: it has, as per the argument so far, freedom to sample the SxA space due to its stochastic nature. Policy b is perfectly suited to its job: allowing M to sample E as required to, in a monte carlo sense, estimate Q\\_b. Policy pi gets refined (improved) using this Q\\_b. But how can we improve policy pi using the Q from another policy b?\r\n",
        "> \r\n",
        "> Clearly we must have some constraints on pi and b; that constraint is that of _coverage_. Policy b must cover pi, which means that in a state s, pi(a|s) > 0 => b(a|s) > 0. That is, actions taken under pi must also be taken under b. Policy b is thus free to be stochastic and exploratory, given that it is guaranteed to take actions of pi.\r\n",
        "> \r\n",
        "> In (5.3) and (5.4) we see that there is a scaling factor that will relate the value functions computed via b and that of pi. This is an excellent result that enables the use of Off Policy methods: we can indeed look at one policies behaviour in order to optimize another (closely related, per _coverage_) policy. This is the fundamental lesson of section 5.5 in the textbook. The textbook shows two strategies of obtaining the required ratio relating the returns from b vs those of pi: ordinary importance sampling (5.3) and weighted importance sampling (5.6). For this course, it is not important to understand the theoretical foundations for these (largely because it is an area of research for MC, which is out of scope): you must understand how to compute these different ratios, operationally, in an algorithms.\r\n",
        "> \r\n",
        "> Further to the goal of computation (our goal in this course), the textbook in section 5.6 covers recursive formulations of the computation of an average. We must remember that with RL (in contrast to supervised learning that you may have encouraged in the past; if you haven't it is fine, it is not necessary for this course) training is _on-line_. That is, we are learning from experience, i.e., learning _while_ in contact with the environment. (One may also consider simulation-oriented training but even with simulations, the agent is still operating as if it is coupled to the problem space, and learning on-line).\r\n",
        "> \r\n",
        "> Hence, we must be able to compute averages on a sample-by-sample basis, versus the bulk computation that we're normally accustomed to. Section 5.6 presents this incremental computation of the average, and p 110 shows the algorithm for MC Off Policy prediction.\r\n",
        "> \r\n",
        "> With this, we can, as we did for all prior cases, obtain the GPI algorithm for MC Off Policy control on p 111 (of section 5.7).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaKcZt4-zts"
      },
      "source": [
        "#### Comments\r\n",
        "\r\n",
        "> MC learning has many open issues that require theoretical justification, that are still an open area of research. It represents the opposite case to Dynamic Programming in many respects. Clearly with DynProg, we had p() while with MC we don't. Beyond this, the solution of DynProg refined old estimates to form new estimates of V or Q; this is called _bootstrapping._ This is _not_ the case with MC. With MC, our evaluation is based on the episodes, and is not based directly on refining a prior estimate of the value function (justify this, by looking at the computation of the Q estimates). This makes the algorithm more amenable to efficient computation _and_ also makes it less sensitive to E's violations of the Markov property. Skip 5.8 and 5.9, and read 5.10.\r\n",
        "> \r\n",
        "> Exercise:\r\n",
        "> \r\n",
        "> Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybFWr-BA-zwf"
      },
      "source": [
        "> (Optional)\r\n",
        "> \r\n",
        "> Explore the OpenAI gyms that are available, and find a discrete environment, and employ your MC control algorithm for it.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1jlxKb-zyn"
      },
      "source": [
        "> The goal of this assignment is to ensure you understand the underlying algorithms of MC control, as distinct from DynProg. Start early and ask questions. The goal is to prepare you for the test: the test will stress the concepts. The assignment will generate working knowledge for these concepts, complementing and reinforcing your theoretical/conceptual knowledge.\r\n",
        "> \r\n",
        "> Submit the code via a file called assignment.py; if you choose to do the optional work, submit it as optional.py.\r\n",
        "> \r\n",
        "> Answer the exercise questions above, and show logs from your code showing how long your solver can keep the pole balanced; the goal is clearly to balance it indefinitely (say for 10,000 steps). This should be submitted as a pdf: assignment.pdf. If you do optional, submit the logs as optional.pdf. For any logs, you must be very clear and explain what you are showing, and what it means. Just submitting a raw log with no comments or explanation is insufficient and can not be properly graded.\r\n",
        "> \r\n",
        "> This assignment can be done within a week, however, I have given you a penalty-free extension as noted. No requests for extensions will be entertained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqwQMwPZ-z05"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbi2xaFo31Sj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e20369-c556-4343-b368-3600f1df8085"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fd30974d278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L4YayzR4FYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6e3733d6-266b-4c3a-d216-a4d04fc3aedb"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "prev_screen = env.render(mode='rgb_array')\n",
        "plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "  screen = env.render(mode='rgb_array')\n",
        "  \n",
        "  plt.imshow(screen)\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterations that were run: 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}