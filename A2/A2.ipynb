{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/A2/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOcmu-k8RS2"
      },
      "source": [
        "# A2: Monte Carlo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnZ2Ue2j8NcF"
      },
      "source": [
        "<!-- Lesson -->\r\n",
        "<!-- ------ -->\r\n",
        "\r\n",
        "### Recap\r\n",
        "\r\n",
        "> Where are we so far in the course? We, in the introduction, mentioned that the RL problem is the problem of designing the intelligence that resides inside an agent (M) enabling it to control an environment (E). The agent is coupled to the environment via: (a) an actuation signal, A\\_t, from M to E, and (b) state, S\\_t, and reward, R\\_T, feedback signals from E to M. The agent must control the environment by selection appropriate actions, A\\_k, based on the feedback it receives, S\\_t and R\\_t. The agent selects its action via a _policy_. The policy may be deterministic or stochastic.\r\n",
        "> \r\n",
        "> This is exactly a classical control theoretic formulation of controller and plant. What, then, is so special that we call this \"reinforcement learning\"? In contrast to classical control theory, we do not assume that we have a full model of E; hence, we must _learn_ something about the environment in order to control it.\r\n",
        "> \r\n",
        "> With dynamic programming, we had perfect knowledge of the environment; i.e., we had access to p(s',r|s,a), the state transition dynamics of E. Using this, we posed the problem of prediction (or policy evaluation, Eval): given a policy pi, and an environment (specified by p), we can iteratively execute a process to obtain, in the limit as n (the number of iterations) goes to infinity, V\\_pi (the state value function of the policy). The foundation of the iterative process was the recursive definition of return and the Bellman relationship. We could do the same of Q\\_pi.\r\n",
        "> \r\n",
        "> We then posed a strategy for policy improvement (Impr), where by if we have a policy pi\\_k, and the associated value function V\\_k, we could propose an improved policy such that in any state, s, we chose the action, a, such that the next state of E, s', would have maximum V\\_pi\\_k(s'). Alternatively, we could simply chose, if we had Q\\_pi\\_k, a=argmax\\_a Q\\_pi\\_k(s, a).\r\n",
        "> \r\n",
        "> Finally, proposed a strategy for control of this environment by an agent. The agent (a) starts with a policy, pi\\_0, (b) executes Eval to obtain the value function, (c) executes Impr to give us pi\\_1, the improved policy, (d) and continues to iterate, alternating Eval and Iter. Since, theoretically, Eval is a process that converges to the value function in the limit (n->\\\\Infty), we illustrated the pragmatic compromise of truncated Eval (Eval\\_trunc) instead of full Eval, and commented that this will also converge to the optimal policy, pi\\_\\*.\r\n",
        "> \r\n",
        "> We note here that for DynProg we could have presented a very straightforward analytical alternative that did not require any of the above; since E is well known (via p), we could have formulated the synthesis of pi\\_\\* via a set of nonlinear equations. However, this is of limited use for cases where we don't have p. The structure of the iterative solution mechanism is generally applicable to a variety of cases, as we will see.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th2YO-Cb-zNo"
      },
      "source": [
        "### Monte Carlo Part A\r\n",
        "\r\n",
        "> The first part of our Monte Carlo presentation showed us how the situation changes when we do not have a model (i.e., p). In this case, there is no chance of an analytical synthesis method, and we must employ an iterative approach that is structurally aligned with what we did for DynProg.\r\n",
        "> \r\n",
        "> Monte Carlo methods, in general, are rooted in the idea that we can sample a phenomenon of interest many times, and average the returns to obtain meaningful data. In the context of Reinforcement Learning, we are sampling E's rewards, and averaging them, to estimate the return, and hence the value function. We presented how to do this to solve the policy evaluation (Eval) problem.\r\n",
        "> \r\n",
        "> Policy improvement (Impr) of course relies on a value function to improve policy pi\\_k to pi\\_{k+1}. Since we don't have access to the transition dynamics, p, of E, V\\_pi\\_k is not of much use in Impr. Rather, we need Q\\_pi\\_k so that we can derive the improved policy such that at state s, it choses action a=argmax\\_a Q\\_pi\\_k(s, a). So here we see how Q and V are distinct.\r\n",
        "> \r\n",
        "> We also see, with Monte Carlo, how the notion of RL being a paradigm for control of unknown environments where the agent \"learns through experience\". Monte Carlo methods are based on random sampling of a phenomenon and computation of averages to learn about the efficacy of the agent's action selection policy (i.e., learn the value functions).\r\n",
        "> \r\n",
        "> We also obtain our motivation for stochastic policies at this point. Since we are learning an action value function (state-action value function, Q(s, a)), we must sample the entire space of the domain, i.e. S x A. That is, our agent must -- to learn effectively -- experience the full domain, S x A. In other words, pi(s, a) > 0 for all s, for all a \\\\in A(s) --- all actions that are possible, must have a non-zero probability of being chosen. This is only possible with a stochastic policy.\r\n",
        "> \r\n",
        "> One can also pose the requirement as Monte Carlo (to this point) expects exploring states in the set of episodes that it has to learn from. This means that the episodes must have sufficient coverage of all (s, a) pairs. This is a very onerous requirement; the stochastic policy is far more reasonable comparatively. This leads us to part B of MC. But for now, regardless of these requirements, we can see that we have the elements of a \"learning-by-experience\" AI, with no \"magical thinking\" (i.e., no appeal to undefined, amorphous concepts) required. Nice!\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejj2A1Ip-zcY"
      },
      "source": [
        "#### Exercises\r\n",
        "\r\n",
        "> 1\\. Explain clearly why V\\_pi is not useful in the MC development above?\r\n",
        "> \r\n",
        "\r\n",
        "$V_\\pi(s)$ is a function that maps each state to a value given a specific policy $\\pi$. In the on-policy MC development, $\\pi$ does not keep track of the optimal policy but rather also includes an $\\epsilon$-soft component to encourage random exploration. Therefore the $V_\\pi(s)$ is not useful since it does not accurate represent the value of the state. In fact, in off-policy MC, the policy to generate exploratory experiences is completely separated from the actual policy, which means $V_\\pi(s)$ does not indicate the actual value of the state.\r\n",
        "\r\n",
        "> 2\\. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q\\_pi\\_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:\r\n",
        "> \r\n",
        "> a. Will we obtain Q\\_pi\\_k from eval?\r\n",
        "> \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "> b. If not why are we able to truncate Eval? Explain clearly.\r\n",
        "> \r\n",
        "\r\n",
        "\r\n",
        "> c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval\\_trunc, is it possible to converge on a sub-optimal policy pi\\_c? Is this a stable fixed point of the GPI for MC? Explain clearly.\r\n",
        "> \r\n",
        "\r\n",
        "\r\n",
        "> 3\\. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).\r\n",
        "\r\n",
        "One possible stochastic policy is to assign at least an $\\epsilon>0$ probability for all state-action pairs such that all $(s, a)\\in S\\times A$ have a finite probability to be sampled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvBTGvrG-zmF"
      },
      "source": [
        "### Monte Carlo Part B-1: Stochastic Policies\r\n",
        "\r\n",
        "> Let's remove the requirement for the exploring starts requirement. The straightforward way to do this is to resort to what we mentioned above -- stochastic policies. Let's make this introduction of \"randomness\" concrete and purposeful. In a state s, we have a set of actions, A(s), that the action can take. We also have the greedy action, a = argmax\\_a(Q(s, a)). We now allow all actions to be selected with some epsilon probability, but bias in favor of selecting the greedy action. We select all actions with some probability epsilon/|A(s)| (where |A(s)| is the size of the set A(s)), while selecting the greedy action with probability 1-epsilon + epsilon/|A(s)|. You can clearly see that this scheme extends to cases when multiple equiprobable greedy actions exist.\r\n",
        "> \r\n",
        "> This scheme of formulating a stochastic policy, the epsilon-soft policy, can be used to find a quasi-optimal policy, without the onerous requirements of exploring starts. A good result, that makes MC more practical.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nARxsm9-zoU"
      },
      "source": [
        "### Monte Carlo Part B-2: Off Policy Methods\r\n",
        "\r\n",
        "#### On Policy Methods\r\n",
        "\r\n",
        "> The final element of this topic is the notion of on and off policy learning. On Policy learning is what we've been doing so far, and is a special case of Off Policy, as we'll see soon. In On Policy learning, we are executing GPI to identify the optimal control policy, pi\\_\\*, and we're using the instantaneous policy, pi\\_k, as the behaviour generation policy.\r\n",
        "> \r\n",
        "> We should realize, however, that there is a conflict in the above. Let's say we start with a stochastic initial policy, pi\\_0 (recall, we know at least one means to obtain this). We can see that as the GPI iteration proceeds through the interleaved iteration of Eval\\_trunc and Impr, we will get better and better policies.\r\n",
        "> \r\n",
        "> Now, on the one hand these better policies -- **to be useful for control** (i.e., our ultimate goal) -- must be progressively more strongly deterministic (barring ties, which introduce the only necessary randomness, if we have no deterministic way to break the tie).\r\n",
        "> \r\n",
        "> On the other hand, however, these policies -- **to be useful for evaluation** (our sub-goal GPI) -- must be effective samplers (\"explorers\") of the S x A space, which suggests randomness (specifically, for all s, pi(s, a) > 0 for all a \\\\in A(s)). Recall: we need evaluation to succeed, because it is the foundation for greedy improvement.\r\n",
        "> \r\n",
        "> This is a very real problem that must be addressed, which we'll do by generalizing to the use of two policies.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04ACdu7-zrE"
      },
      "source": [
        "#### Off Policy Methods\r\n",
        "\r\n",
        "> In the general case of off-policy learning we have two policies of interest, pi and b. Pi is the policy we are trying to solve for in GPI -- the optimal policy that will allow our agent to control the environment optimally. Policy b is a behaviour generation (\"exploratory\") policy: it has, as per the argument so far, freedom to sample the SxA space due to its stochastic nature. Policy b is perfectly suited to its job: allowing M to sample E as required to, in a monte carlo sense, estimate Q\\_b. Policy pi gets refined (improved) using this Q\\_b. But how can we improve policy pi using the Q from another policy b?\r\n",
        "> \r\n",
        "> Clearly we must have some constraints on pi and b; that constraint is that of _coverage_. Policy b must cover pi, which means that in a state s, pi(a|s) > 0 => b(a|s) > 0. That is, actions taken under pi must also be taken under b. Policy b is thus free to be stochastic and exploratory, given that it is guaranteed to take actions of pi.\r\n",
        "> \r\n",
        "> In (5.3) and (5.4) we see that there is a scaling factor that will relate the value functions computed via b and that of pi. This is an excellent result that enables the use of Off Policy methods: we can indeed look at one policies behaviour in order to optimize another (closely related, per _coverage_) policy. This is the fundamental lesson of section 5.5 in the textbook. The textbook shows two strategies of obtaining the required ratio relating the returns from b vs those of pi: ordinary importance sampling (5.3) and weighted importance sampling (5.6). For this course, it is not important to understand the theoretical foundations for these (largely because it is an area of research for MC, which is out of scope): you must understand how to compute these different ratios, operationally, in an algorithms.\r\n",
        "> \r\n",
        "> Further to the goal of computation (our goal in this course), the textbook in section 5.6 covers recursive formulations of the computation of an average. We must remember that with RL (in contrast to supervised learning that you may have encouraged in the past; if you haven't it is fine, it is not necessary for this course) training is _on-line_. That is, we are learning from experience, i.e., learning _while_ in contact with the environment. (One may also consider simulation-oriented training but even with simulations, the agent is still operating as if it is coupled to the problem space, and learning on-line).\r\n",
        "> \r\n",
        "> Hence, we must be able to compute averages on a sample-by-sample basis, versus the bulk computation that we're normally accustomed to. Section 5.6 presents this incremental computation of the average, and p 110 shows the algorithm for MC Off Policy prediction.\r\n",
        "> \r\n",
        "> With this, we can, as we did for all prior cases, obtain the GPI algorithm for MC Off Policy control on p 111 (of section 5.7).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaKcZt4-zts"
      },
      "source": [
        "#### Comments\r\n",
        "\r\n",
        "> MC learning has many open issues that require theoretical justification, that are still an open area of research. It represents the opposite case to Dynamic Programming in many respects. Clearly with DynProg, we had p() while with MC we don't. Beyond this, the solution of DynProg refined old estimates to form new estimates of V or Q; this is called _bootstrapping._ This is _not_ the case with MC. With MC, our evaluation is based on the episodes, and is not based directly on refining a prior estimate of the value function (justify this, by looking at the computation of the Q estimates). This makes the algorithm more amenable to efficient computation _and_ also makes it less sensitive to E's violations of the Markov property. Skip 5.8 and 5.9, and read 5.10.\r\n",
        "> \r\n",
        "> Exercise:\r\n",
        "> \r\n",
        "> Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdWErCtdL9Wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fed0dc-0edf-4fdf-c9be-c9a738621490"
      },
      "source": [
        "#%% Boilerplate setup\r\n",
        "\r\n",
        "import gym, numpy as np, matplotlib.pyplot as plt\r\n",
        "def test_cart_pole(policy_fun, env=gym.make(\"CartPole-v0\"), \r\n",
        "                   max_iter=1000, print_iter=False):\r\n",
        "    \"\"\"\r\n",
        "    Test the policy against the environment\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ------------------\r\n",
        "    policy_fun : function\r\n",
        "        A funtion that takes the observations tuple from the CartPole \r\n",
        "        environment and returns a valid action of the environment\r\n",
        "\r\n",
        "    Yields\r\n",
        "    ------\r\n",
        "    action, obs, reward, done, info\r\n",
        "        Information for the current iteration and its results\r\n",
        "    \"\"\"\r\n",
        "    obs = env.reset()\r\n",
        "    for i in range(max_iter):\r\n",
        "        action = policy_fun(obs)\r\n",
        "        next_obs, reward, done, info = env.step(action)\r\n",
        "        if print_iter:\r\n",
        "            print(f\"State[{i}]={np.array2string(obs, precision=2, suppress_small=True)}, Action[{i}]={action}, Reward[{i+1}]={reward}\")\r\n",
        "        yield action, obs, reward, done, info\r\n",
        "        obs = next_obs\r\n",
        "        if done:\r\n",
        "            if print_iter:\r\n",
        "                print(f\"State[{i+1}]={np.array2string(obs, precision=2, suppress_small=True)}\")\r\n",
        "            break\r\n",
        "    env.close(); \r\n",
        "    if print_iter:\r\n",
        "        print(\"Iterations that were run:\", i)\r\n",
        "\r\n",
        "# Test that the function works\r\n",
        "rng = np.random.RandomState(0)\r\n",
        "_ = list(test_cart_pole(lambda _: rng.randint(0, 2), print_iter=True))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State[0]=[ 0.03 -0.02 -0.04  0.02], Action[0]=0, Reward[1]=1.0\n",
            "State[1]=[ 0.03 -0.22 -0.04  0.3 ], Action[1]=1, Reward[2]=1.0\n",
            "State[2]=[ 0.02 -0.02 -0.03 -0.  ], Action[2]=1, Reward[3]=1.0\n",
            "State[3]=[ 0.02  0.17 -0.03 -0.31], Action[3]=0, Reward[4]=1.0\n",
            "State[4]=[ 0.03 -0.02 -0.04 -0.03], Action[4]=1, Reward[5]=1.0\n",
            "State[5]=[ 0.03  0.17 -0.04 -0.33], Action[5]=1, Reward[6]=1.0\n",
            "State[6]=[ 0.03  0.37 -0.05 -0.64], Action[6]=1, Reward[7]=1.0\n",
            "State[7]=[ 0.04  0.57 -0.06 -0.94], Action[7]=1, Reward[8]=1.0\n",
            "State[8]=[ 0.05  0.76 -0.08 -1.25], Action[8]=1, Reward[9]=1.0\n",
            "State[9]=[ 0.06  0.96 -0.1  -1.57], Action[9]=1, Reward[10]=1.0\n",
            "State[10]=[ 0.08  1.15 -0.14 -1.89], Action[10]=1, Reward[11]=1.0\n",
            "State[11]=[ 0.1   1.35 -0.17 -2.23], Action[11]=0, Reward[12]=1.0\n",
            "State[12]=[ 0.13  1.16 -0.22 -1.99]\n",
            "Iterations that were run: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuYINVvKOq62"
      },
      "source": [
        "#%% Discretization of states\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Observation:\r\n",
        "    Type: Box(4)\r\n",
        "    Num     Observation               Min                     Max\r\n",
        "    0       Cart Position             -4.8                    4.8\r\n",
        "    1       Cart Velocity             -Inf                    Inf\r\n",
        "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\r\n",
        "    3       Pole Angular Velocity     -Inf                    Inf\r\n",
        "The episode ends when the pole is more than 15 degrees from vertical, \r\n",
        "or the cart moves more than 2.4 units from the center.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "bin_divide = [\r\n",
        "        np.linspace(-2.3, +2.3, 5),\r\n",
        "        np.linspace(-1.0, +1.0, 5),\r\n",
        "        np.linspace(-0.20, +0.20, 5),\r\n",
        "        np.linspace(-1.0, +1.0, 5),\r\n",
        "        ]\r\n",
        "multiplier = 6**np.arange(4)\r\n",
        "def obs2state(obs):\r\n",
        "    \"\"\"\r\n",
        "    Discretize observations to states\r\n",
        "    \"\"\"\r\n",
        "    digitized = [np.digitize(o, bins) for o, bins in zip(obs, bin_divide)]\r\n",
        "    return (digitized * multiplier).sum()\r\n",
        "    # return ((obs > 0) * 2**np.arange(len(obs))).sum()\r\n",
        "    # return sum(np.digitize(o, bins) * (2**i) for i, (o, bins) in enumerate(zip(obs, bin_divide)))\r\n",
        "obs2state.S = 6**4 # 8**len(obs)"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vduwwBlIRhP0"
      },
      "source": [
        "def init_random_policy(eps, S, A, rng):\r\n",
        "    \"\"\"\r\n",
        "    Generate a random eps-soft policy\r\n",
        "    \"\"\"\r\n",
        "    pi_random_init = rng.random([S, A])\r\n",
        "    pi_random_mask = np.zeros_like(pi_random_init, dtype=np.float)\r\n",
        "    pi_random_mask[np.arange(S), np.argmax(pi_random_init, axis=1)] = True\r\n",
        "    pi = pi_random_mask * (1 - eps) + eps / A\r\n",
        "    return pi\r\n",
        "\r\n",
        "def get_eps_soft_policy(pi, S, A, eps):\r\n",
        "    assert pi.ndim == 1\r\n",
        "    ret = np.full((S, A), fill_value=1.0*eps/A)\r\n",
        "    ret[np.arange(num_S), pi] += 1 - eps\r\n",
        "    # for i, a in enumerate(pi):\r\n",
        "        # ret[i, a] += 1 - eps\r\n",
        "    return ret"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3knchHBPecK",
        "outputId": "8b479e18-9609-4efd-f79f-ac9495190bbc"
      },
      "source": [
        "#%% Off-policy solution\r\n",
        "# Based on page 111\r\n",
        "\r\n",
        "eps=0.02\r\n",
        "gamma=0.999\r\n",
        "env=gym.make(\"CartPole-v0\")\r\n",
        "rng=np.random.RandomState(0)\r\n",
        "num_A = env.action_space.n\r\n",
        "action_space = np.arange(num_A)\r\n",
        "num_S = obs2state.S\r\n",
        "state_space = np.arange(num_S)\r\n",
        "\r\n",
        "# Initialize\r\n",
        "Q = rng.random([num_S, num_A])\r\n",
        "C = np.zeros([num_S, num_A])\r\n",
        "pi = np.argmax(Q, axis=1)\r\n",
        "\r\n",
        "T_history = []\r\n",
        "for i in range(10000):\r\n",
        "    if i % 100 == 0:\r\n",
        "        iters = len(list(test_cart_pole(lambda obs: pi[obs2state(obs)], print_iter=False)))\r\n",
        "        print(f\"Learning iter {i}, off-policy average {np.mean(T_history[-100:])}, greedy policy lasted {iters} iterations\")\r\n",
        "\r\n",
        "    # b <- any soft policy\r\n",
        "    b = get_eps_soft_policy(pi, num_S, num_A, eps) # rng.random([num_S, num_A]) # init_random_policy(eps, num_S, num_A, rng)\r\n",
        "    # Generate an episode\r\n",
        "    actions, obses, rewards, dones, infos = zip(\r\n",
        "        *test_cart_pole(\r\n",
        "            lambda obs: rng.choice(action_space, p=b[obs2state(obs)])\r\n",
        "        )\r\n",
        "    )\r\n",
        "    # Compatibility with textbook notation\r\n",
        "    R = np.array([None] + list(rewards))\r\n",
        "    S = np.array([obs2state(i) for i in obses])\r\n",
        "    A = np.array(actions)\r\n",
        "\r\n",
        "    G = 0\r\n",
        "    W = 1\r\n",
        "    T = len(actions)\r\n",
        "    T_history.append(T)\r\n",
        "    for t in range(T-1, -1, -1):\r\n",
        "        G = gamma * G + R[t+1]\r\n",
        "        C[S[t], A[t]] = C[S[t], A[t]] + W\r\n",
        "        Q[S[t], A[t]] = (Q[S[t], A[t]] + \r\n",
        "                         W / C[S[t], A[t]] * (G - Q[S[t], A[t]]))\r\n",
        "        pi = np.argmax(Q, axis=1)\r\n",
        "        if A[t] != pi[S[t]]:\r\n",
        "            # then exit inner loop (proceed to next episode)\r\n",
        "            # print(\"exiting inner loop\")\r\n",
        "            break\r\n",
        "        # print(\"continuing inner loop\")\r\n",
        "        W = W * 1.0 / b[S[t], A[t]]\r\n",
        "    # break\r\n",
        "\r\n"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning iter 0, off-policy average nan, greedy policy lasted 38 iterations\n",
            "Learning iter 100, off-policy average 46.44, greedy policy lasted 194 iterations\n",
            "Learning iter 200, off-policy average 49.46, greedy policy lasted 131 iterations\n",
            "Learning iter 300, off-policy average 57.24, greedy policy lasted 79 iterations\n",
            "Learning iter 400, off-policy average 54.31, greedy policy lasted 131 iterations\n",
            "Learning iter 500, off-policy average 69.74, greedy policy lasted 154 iterations\n",
            "Learning iter 600, off-policy average 89.32, greedy policy lasted 45 iterations\n",
            "Learning iter 700, off-policy average 71.89, greedy policy lasted 39 iterations\n",
            "Learning iter 800, off-policy average 61.66, greedy policy lasted 80 iterations\n",
            "Learning iter 900, off-policy average 61.54, greedy policy lasted 64 iterations\n",
            "Learning iter 1000, off-policy average 71.77, greedy policy lasted 200 iterations\n",
            "Learning iter 1100, off-policy average 80.8, greedy policy lasted 37 iterations\n",
            "Learning iter 1200, off-policy average 84.23, greedy policy lasted 16 iterations\n",
            "Learning iter 1300, off-policy average 73.96, greedy policy lasted 26 iterations\n",
            "Learning iter 1400, off-policy average 70.27, greedy policy lasted 169 iterations\n",
            "Learning iter 1500, off-policy average 131.16, greedy policy lasted 49 iterations\n",
            "Learning iter 1600, off-policy average 111.39, greedy policy lasted 16 iterations\n",
            "Learning iter 1700, off-policy average 117.2, greedy policy lasted 200 iterations\n",
            "Learning iter 1800, off-policy average 126.35, greedy policy lasted 16 iterations\n",
            "Learning iter 1900, off-policy average 119.34, greedy policy lasted 46 iterations\n",
            "Learning iter 2000, off-policy average 141.66, greedy policy lasted 53 iterations\n",
            "Learning iter 2100, off-policy average 142.53, greedy policy lasted 84 iterations\n",
            "Learning iter 2200, off-policy average 164.01, greedy policy lasted 57 iterations\n",
            "Learning iter 2300, off-policy average 156.14, greedy policy lasted 158 iterations\n",
            "Learning iter 2400, off-policy average 151.59, greedy policy lasted 200 iterations\n",
            "Learning iter 2500, off-policy average 157.94, greedy policy lasted 200 iterations\n",
            "Learning iter 2600, off-policy average 161.77, greedy policy lasted 42 iterations\n",
            "Learning iter 2700, off-policy average 161.23, greedy policy lasted 188 iterations\n",
            "Learning iter 2800, off-policy average 167.03, greedy policy lasted 200 iterations\n",
            "Learning iter 2900, off-policy average 163.42, greedy policy lasted 200 iterations\n",
            "Learning iter 3000, off-policy average 164.15, greedy policy lasted 200 iterations\n",
            "Learning iter 3100, off-policy average 164.88, greedy policy lasted 200 iterations\n",
            "Learning iter 3200, off-policy average 161.15, greedy policy lasted 59 iterations\n",
            "Learning iter 3300, off-policy average 164.49, greedy policy lasted 20 iterations\n",
            "Learning iter 3400, off-policy average 175.25, greedy policy lasted 200 iterations\n",
            "Learning iter 3500, off-policy average 169.19, greedy policy lasted 186 iterations\n",
            "Learning iter 3600, off-policy average 156.11, greedy policy lasted 200 iterations\n",
            "Learning iter 3700, off-policy average 186.1, greedy policy lasted 200 iterations\n",
            "Learning iter 3800, off-policy average 186.32, greedy policy lasted 200 iterations\n",
            "Learning iter 3900, off-policy average 191.72, greedy policy lasted 200 iterations\n",
            "Learning iter 4000, off-policy average 190.36, greedy policy lasted 200 iterations\n",
            "Learning iter 4100, off-policy average 188.84, greedy policy lasted 200 iterations\n",
            "Learning iter 4200, off-policy average 192.76, greedy policy lasted 169 iterations\n",
            "Learning iter 4300, off-policy average 188.98, greedy policy lasted 200 iterations\n",
            "Learning iter 4400, off-policy average 193.91, greedy policy lasted 200 iterations\n",
            "Learning iter 4500, off-policy average 192.12, greedy policy lasted 190 iterations\n",
            "Learning iter 4600, off-policy average 194.61, greedy policy lasted 200 iterations\n",
            "Learning iter 4700, off-policy average 189.86, greedy policy lasted 200 iterations\n",
            "Learning iter 4800, off-policy average 197.18, greedy policy lasted 186 iterations\n",
            "Learning iter 4900, off-policy average 194.76, greedy policy lasted 200 iterations\n",
            "Learning iter 5000, off-policy average 195.56, greedy policy lasted 157 iterations\n",
            "Learning iter 5100, off-policy average 194.01, greedy policy lasted 184 iterations\n",
            "Learning iter 5200, off-policy average 198.17, greedy policy lasted 200 iterations\n",
            "Learning iter 5300, off-policy average 193.46, greedy policy lasted 200 iterations\n",
            "Learning iter 5400, off-policy average 194.08, greedy policy lasted 200 iterations\n",
            "Learning iter 5500, off-policy average 191.27, greedy policy lasted 200 iterations\n",
            "Learning iter 5600, off-policy average 193.68, greedy policy lasted 200 iterations\n",
            "Learning iter 5700, off-policy average 193.12, greedy policy lasted 200 iterations\n",
            "Learning iter 5800, off-policy average 195.82, greedy policy lasted 200 iterations\n",
            "Learning iter 5900, off-policy average 195.84, greedy policy lasted 200 iterations\n",
            "Learning iter 6000, off-policy average 190.94, greedy policy lasted 200 iterations\n",
            "Learning iter 6100, off-policy average 191.94, greedy policy lasted 200 iterations\n",
            "Learning iter 6200, off-policy average 181.42, greedy policy lasted 200 iterations\n",
            "Learning iter 6300, off-policy average 185.48, greedy policy lasted 200 iterations\n",
            "Learning iter 6400, off-policy average 192.84, greedy policy lasted 200 iterations\n",
            "Learning iter 6500, off-policy average 186.43, greedy policy lasted 200 iterations\n",
            "Learning iter 6600, off-policy average 187.35, greedy policy lasted 200 iterations\n",
            "Learning iter 6700, off-policy average 185.11, greedy policy lasted 200 iterations\n",
            "Learning iter 6800, off-policy average 184.65, greedy policy lasted 161 iterations\n",
            "Learning iter 6900, off-policy average 192.68, greedy policy lasted 200 iterations\n",
            "Learning iter 7000, off-policy average 181.07, greedy policy lasted 200 iterations\n",
            "Learning iter 7100, off-policy average 184.89, greedy policy lasted 200 iterations\n",
            "Learning iter 7200, off-policy average 184.68, greedy policy lasted 200 iterations\n",
            "Learning iter 7300, off-policy average 189.55, greedy policy lasted 200 iterations\n",
            "Learning iter 7400, off-policy average 191.24, greedy policy lasted 200 iterations\n",
            "Learning iter 7500, off-policy average 191.63, greedy policy lasted 200 iterations\n",
            "Learning iter 7600, off-policy average 182.2, greedy policy lasted 199 iterations\n",
            "Learning iter 7700, off-policy average 183.97, greedy policy lasted 200 iterations\n",
            "Learning iter 7800, off-policy average 181.07, greedy policy lasted 200 iterations\n",
            "Learning iter 7900, off-policy average 187.81, greedy policy lasted 200 iterations\n",
            "Learning iter 8000, off-policy average 190.35, greedy policy lasted 200 iterations\n",
            "Learning iter 8100, off-policy average 182.18, greedy policy lasted 200 iterations\n",
            "Learning iter 8200, off-policy average 178.84, greedy policy lasted 200 iterations\n",
            "Learning iter 8300, off-policy average 184.63, greedy policy lasted 200 iterations\n",
            "Learning iter 8400, off-policy average 185.71, greedy policy lasted 200 iterations\n",
            "Learning iter 8500, off-policy average 190.53, greedy policy lasted 200 iterations\n",
            "Learning iter 8600, off-policy average 189.82, greedy policy lasted 200 iterations\n",
            "Learning iter 8700, off-policy average 183.79, greedy policy lasted 133 iterations\n",
            "Learning iter 8800, off-policy average 193.13, greedy policy lasted 200 iterations\n",
            "Learning iter 8900, off-policy average 193.97, greedy policy lasted 174 iterations\n",
            "Learning iter 9000, off-policy average 185.63, greedy policy lasted 200 iterations\n",
            "Learning iter 9100, off-policy average 187.32, greedy policy lasted 200 iterations\n",
            "Learning iter 9200, off-policy average 192.8, greedy policy lasted 200 iterations\n",
            "Learning iter 9300, off-policy average 186.92, greedy policy lasted 127 iterations\n",
            "Learning iter 9400, off-policy average 189.48, greedy policy lasted 188 iterations\n",
            "Learning iter 9500, off-policy average 194.84, greedy policy lasted 200 iterations\n",
            "Learning iter 9600, off-policy average 186.69, greedy policy lasted 200 iterations\n",
            "Learning iter 9700, off-policy average 188.06, greedy policy lasted 57 iterations\n",
            "Learning iter 9800, off-policy average 181.99, greedy policy lasted 200 iterations\n",
            "Learning iter 9900, off-policy average 188.57, greedy policy lasted 197 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ovpgOPMJa2wF",
        "outputId": "3f294c3c-03fd-43b0-e8ff-e2d89947821c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(T_history, '.', markersize=1)\n",
        "plt.title(\"Performance of soft off-policy during training\")\n",
        "plt.xlabel(\"Learning Iteration\")\n",
        "plt.ylabel(\"Training Episode Lasted Iterations\")"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Training Episode Lasted Iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy44miD2WChz",
        "outputId": "d207d27b-d29b-48e7-a2a5-d8ddb7c802eb"
      },
      "source": [
        "for i in range(10):\n",
        "    data = list(test_cart_pole(lambda obs: pi[obs2state(obs)], print_iter=False))\n",
        "    iters = len(data)\n",
        "    print(f\"Testing iter {i}, lasted {iters} iterations\")\n",
        "    if iters < 150:\n",
        "        for i in data:\n",
        "            print(i)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing iter 0, lasted 25 iterations\n",
            "(0, array([-0.00662732,  0.03999087,  0.01368441,  0.00471999]), 1.0, False, {})\n",
            "(1, array([-0.0058275 , -0.15532463,  0.01377881,  0.3016889 ]), 1.0, False, {})\n",
            "(0, array([-0.00893399,  0.03959825,  0.01981259,  0.01338317]), 1.0, False, {})\n",
            "(1, array([-0.00814203, -0.15580215,  0.02008025,  0.31225077]), 1.0, False, {})\n",
            "(0, array([-0.01125807,  0.03902806,  0.02632527,  0.02596757]), 1.0, False, {})\n",
            "(1, array([-0.01047751, -0.15646133,  0.02684462,  0.32683879]), 1.0, False, {})\n",
            "(0, array([-0.01360674,  0.03826834,  0.03338139,  0.04274088]), 1.0, False, {})\n",
            "(1, array([-0.01284137, -0.15731599,  0.03423621,  0.34576625]), 1.0, False, {})\n",
            "(0, array([-0.01598769,  0.03730266,  0.04115154,  0.06407283]), 1.0, False, {})\n",
            "(1, array([-0.01524164, -0.1583844 ,  0.04243299,  0.36945005]), 1.0, False, {})\n",
            "(0, array([-0.01840932,  0.03610977,  0.04982199,  0.09044296]), 1.0, False, {})\n",
            "(1, array([-0.01768713, -0.15968959,  0.05163085,  0.39841917]), 1.0, False, {})\n",
            "(0, array([-0.02088092,  0.03466333,  0.05959924,  0.122451  ]), 1.0, False, {})\n",
            "(1, array([-0.02018765, -0.16125961,  0.06204826,  0.43332542]), 1.0, False, {})\n",
            "(0, array([-0.02341285,  0.03293149,  0.07071476,  0.1608298 ]), 1.0, False, {})\n",
            "(1, array([-0.02275422, -0.16312784,  0.07393136,  0.47495661]), 1.0, False, {})\n",
            "(0, array([-0.02601677,  0.0308765 ,  0.08343049,  0.20646107]), 1.0, False, {})\n",
            "(1, array([-0.02539924, -0.16533326,  0.08755971,  0.5242521 ]), 1.0, False, {})\n",
            "(0, array([-0.02870591,  0.02845438,  0.09804476,  0.26039389]), 1.0, False, {})\n",
            "(0, array([-0.02813682, -0.16792052,  0.10325263,  0.5823206 ]), 1.0, False, {})\n",
            "(0, array([-0.03149523, -0.364326  ,  0.11489905,  0.90566215]), 1.0, False, {})\n",
            "(1, array([-0.03878175, -0.56080061,  0.13301229,  1.23213688]), 1.0, False, {})\n",
            "(0, array([-0.04999776, -0.36761602,  0.15765503,  0.98391002]), 1.0, False, {})\n",
            "(1, array([-0.05735008, -0.56445837,  0.17733323,  1.32167056]), 1.0, False, {})\n",
            "(0, array([-0.06863925, -0.37196433,  0.20376664,  1.08932078]), 1.0, True, {})\n",
            "Testing iter 1, lasted 200 iterations\n",
            "Testing iter 2, lasted 189 iterations\n",
            "Testing iter 3, lasted 200 iterations\n",
            "Testing iter 4, lasted 200 iterations\n",
            "Testing iter 5, lasted 176 iterations\n",
            "Testing iter 6, lasted 200 iterations\n",
            "Testing iter 7, lasted 200 iterations\n",
            "Testing iter 8, lasted 200 iterations\n",
            "Testing iter 9, lasted 17 iterations\n",
            "(0, array([-0.02687706,  0.01456141,  0.04953727,  0.01898832]), 1.0, False, {})\n",
            "(1, array([-0.02658583, -0.18123467,  0.04991703,  0.32688016]), 1.0, False, {})\n",
            "(0, array([-0.03021052,  0.01314238,  0.05645464,  0.05034727]), 1.0, False, {})\n",
            "(1, array([-0.02994768, -0.18274172,  0.05746158,  0.36029417]), 1.0, False, {})\n",
            "(0, array([-0.03360251,  0.01151835,  0.06466747,  0.0862694 ]), 1.0, False, {})\n",
            "(1, array([-0.03337214, -0.18446809,  0.06639286,  0.39863356]), 1.0, False, {})\n",
            "(0, array([-0.03706151,  0.00965225,  0.07436553,  0.12759974]), 1.0, False, {})\n",
            "(1, array([-0.03686846, -0.18645189,  0.07691752,  0.44278628]), 1.0, False, {})\n",
            "(0, array([-0.0405975 ,  0.0075022 ,  0.08577325,  0.1753068 ]), 1.0, False, {})\n",
            "(1, array([-0.04044745, -0.1887359 ,  0.08927938,  0.49376823]), 1.0, False, {})\n",
            "(0, array([-0.04422217,  0.00502095,  0.09915475,  0.23050376]), 1.0, False, {})\n",
            "(0, array([-0.04412175, -0.19136777,  0.10376482,  0.55274367]), 1.0, False, {})\n",
            "(0, array([-0.04794911, -0.38778214,  0.1148197 ,  0.87623267]), 1.0, False, {})\n",
            "(1, array([-0.05570475, -0.58426182,  0.13234435,  1.20269446]), 1.0, False, {})\n",
            "(0, array([-0.06738999, -0.39107572,  0.15639824,  0.95424511]), 1.0, False, {})\n",
            "(1, array([-0.0752115 , -0.58791644,  0.17548314,  1.29169629]), 1.0, False, {})\n",
            "(0, array([-0.08696983, -0.39540498,  0.20131707,  1.05869156]), 1.0, True, {})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybFWr-BA-zwf"
      },
      "source": [
        "> (Optional)\r\n",
        "> \r\n",
        "> Explore the OpenAI gyms that are available, and find a discrete environment, and employ your MC control algorithm for it.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1jlxKb-zyn"
      },
      "source": [
        "> The goal of this assignment is to ensure you understand the underlying algorithms of MC control, as distinct from DynProg. Start early and ask questions. The goal is to prepare you for the test: the test will stress the concepts. The assignment will generate working knowledge for these concepts, complementing and reinforcing your theoretical/conceptual knowledge.\r\n",
        "> \r\n",
        "> Submit the code via a file called assignment.py; if you choose to do the optional work, submit it as optional.py.\r\n",
        "> \r\n",
        "> Answer the exercise questions above, and show logs from your code showing how long your solver can keep the pole balanced; the goal is clearly to balance it indefinitely (say for 10,000 steps). This should be submitted as a pdf: assignment.pdf. If you do optional, submit the logs as optional.pdf. For any logs, you must be very clear and explain what you are showing, and what it means. Just submitting a raw log with no comments or explanation is insufficient and can not be properly graded.\r\n",
        "> \r\n",
        "> This assignment can be done within a week, however, I have given you a penalty-free extension as noted. No requests for extensions will be entertained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqwQMwPZ-z05"
      },
      "source": [
        ""
      ]
    }
  ]
}