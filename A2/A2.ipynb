{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/A2/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOcmu-k8RS2"
      },
      "source": [
        "# A2: Monte Carlo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnZ2Ue2j8NcF"
      },
      "source": [
        "<!-- Lesson -->\r\n",
        "<!-- ------ -->\r\n",
        "\r\n",
        "### Recap\r\n",
        "\r\n",
        "> Where are we so far in the course? We, in the introduction, mentioned that the RL problem is the problem of designing the intelligence that resides inside an agent (M) enabling it to control an environment (E). The agent is coupled to the environment via: (a) an actuation signal, A\\_t, from M to E, and (b) state, S\\_t, and reward, R\\_T, feedback signals from E to M. The agent must control the environment by selection appropriate actions, A\\_k, based on the feedback it receives, S\\_t and R\\_t. The agent selects its action via a _policy_. The policy may be deterministic or stochastic.\r\n",
        "> \r\n",
        "> This is exactly a classical control theoretic formulation of controller and plant. What, then, is so special that we call this \"reinforcement learning\"? In contrast to classical control theory, we do not assume that we have a full model of E; hence, we must _learn_ something about the environment in order to control it.\r\n",
        "> \r\n",
        "> With dynamic programming, we had perfect knowledge of the environment; i.e., we had access to p(s',r|s,a), the state transition dynamics of E. Using this, we posed the problem of prediction (or policy evaluation, Eval): given a policy pi, and an environment (specified by p), we can iteratively execute a process to obtain, in the limit as n (the number of iterations) goes to infinity, V\\_pi (the state value function of the policy). The foundation of the iterative process was the recursive definition of return and the Bellman relationship. We could do the same of Q\\_pi.\r\n",
        "> \r\n",
        "> We then posed a strategy for policy improvement (Impr), where by if we have a policy pi\\_k, and the associated value function V\\_k, we could propose an improved policy such that in any state, s, we chose the action, a, such that the next state of E, s', would have maximum V\\_pi\\_k(s'). Alternatively, we could simply chose, if we had Q\\_pi\\_k, a=argmax\\_a Q\\_pi\\_k(s, a).\r\n",
        "> \r\n",
        "> Finally, proposed a strategy for control of this environment by an agent. The agent (a) starts with a policy, pi\\_0, (b) executes Eval to obtain the value function, (c) executes Impr to give us pi\\_1, the improved policy, (d) and continues to iterate, alternating Eval and Iter. Since, theoretically, Eval is a process that converges to the value function in the limit (n->\\\\Infty), we illustrated the pragmatic compromise of truncated Eval (Eval\\_trunc) instead of full Eval, and commented that this will also converge to the optimal policy, pi\\_\\*.\r\n",
        "> \r\n",
        "> We note here that for DynProg we could have presented a very straightforward analytical alternative that did not require any of the above; since E is well known (via p), we could have formulated the synthesis of pi\\_\\* via a set of nonlinear equations. However, this is of limited use for cases where we don't have p. The structure of the iterative solution mechanism is generally applicable to a variety of cases, as we will see.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th2YO-Cb-zNo"
      },
      "source": [
        "### Monte Carlo Part A\r\n",
        "\r\n",
        "> The first part of our Monte Carlo presentation showed us how the situation changes when we do not have a model (i.e., p). In this case, there is no chance of an analytical synthesis method, and we must employ an iterative approach that is structurally aligned with what we did for DynProg.\r\n",
        "> \r\n",
        "> Monte Carlo methods, in general, are rooted in the idea that we can sample a phenomenon of interest many times, and average the returns to obtain meaningful data. In the context of Reinforcement Learning, we are sampling E's rewards, and averaging them, to estimate the return, and hence the value function. We presented how to do this to solve the policy evaluation (Eval) problem.\r\n",
        "> \r\n",
        "> Policy improvement (Impr) of course relies on a value function to improve policy pi\\_k to pi\\_{k+1}. Since we don't have access to the transition dynamics, p, of E, V\\_pi\\_k is not of much use in Impr. Rather, we need Q\\_pi\\_k so that we can derive the improved policy such that at state s, it choses action a=argmax\\_a Q\\_pi\\_k(s, a). So here we see how Q and V are distinct.\r\n",
        "> \r\n",
        "> We also see, with Monte Carlo, how the notion of RL being a paradigm for control of unknown environments where the agent \"learns through experience\". Monte Carlo methods are based on random sampling of a phenomenon and computation of averages to learn about the efficacy of the agent's action selection policy (i.e., learn the value functions).\r\n",
        "> \r\n",
        "> We also obtain our motivation for stochastic policies at this point. Since we are learning an action value function (state-action value function, Q(s, a)), we must sample the entire space of the domain, i.e. S x A. That is, our agent must -- to learn effectively -- experience the full domain, S x A. In other words, pi(s, a) > 0 for all s, for all a \\\\in A(s) --- all actions that are possible, must have a non-zero probability of being chosen. This is only possible with a stochastic policy.\r\n",
        "> \r\n",
        "> One can also pose the requirement as Monte Carlo (to this point) expects exploring states in the set of episodes that it has to learn from. This means that the episodes must have sufficient coverage of all (s, a) pairs. This is a very onerous requirement; the stochastic policy is far more reasonable comparatively. This leads us to part B of MC. But for now, regardless of these requirements, we can see that we have the elements of a \"learning-by-experience\" AI, with no \"magical thinking\" (i.e., no appeal to undefined, amorphous concepts) required. Nice!\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejj2A1Ip-zcY"
      },
      "source": [
        "#### Exercises\r\n",
        "\r\n",
        "> 1\\. Explain clearly why V\\_pi is not useful in the MC development above?\r\n",
        "> \r\n",
        "\r\n",
        "$V_\\pi(s)$ is a function that maps each state to a value given a specific policy $\\pi$. In the on-policy MC development, $\\pi$ does not keep track of the optimal policy but rather also includes an $\\epsilon$-soft component to encourage random exploration. Therefore the $V_\\pi(s)$ is not useful since it does not accurate represent the value of the state. In fact, in off-policy MC, the policy to generate exploratory experiences is completely separated from the actual policy, which means $V_\\pi(s)$ does not indicate the actual value of the state.\r\n",
        "\r\n",
        "> 2\\. The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q\\_pi\\_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:\r\n",
        "> \r\n",
        "> a. Will we obtain Q\\_pi\\_k from eval?\r\n",
        "> \r\n",
        "\r\n",
        "No. Since the evaluation is truncated, only an approximate solution will be obtained.\r\n",
        "\r\n",
        "> b. If not why are we able to truncate Eval? Explain clearly.\r\n",
        "> \r\n",
        "\r\n",
        "Depending on the environment, with an optimal policy, some of the \"bad\" states will be encounted very rarely. Since the goal is to generate an optimal policy, it is not important to sample and evaluate these states.\r\n",
        "\r\n",
        "> c. Assuming ES (i.e., thorough sampling of the S x A space), and the above truncated Eval\\_trunc, is it possible to converge on a sub-optimal policy pi\\_c? Is this a stable fixed point of the GPI for MC? Explain clearly.\r\n",
        "> \r\n",
        "\r\n",
        "It is not possible to converge on a sub-optimal policy since each iteration produces a non-inferior policy as compared to the previous iteration.\r\n",
        "\r\n",
        "> 3\\. Explain how you can synthesize a stochastic policy given what you know so far (you don't need to read ahead).\r\n",
        "\r\n",
        "One possible stochastic policy is to assign at least an $\\epsilon>0$ probability for all state-action pairs such that all $(s, a)\\in S\\times A$ have a finite probability to be sampled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvBTGvrG-zmF"
      },
      "source": [
        "### Monte Carlo Part B-1: Stochastic Policies\r\n",
        "\r\n",
        "> Let's remove the requirement for the exploring starts requirement. The straightforward way to do this is to resort to what we mentioned above -- stochastic policies. Let's make this introduction of \"randomness\" concrete and purposeful. In a state s, we have a set of actions, A(s), that the action can take. We also have the greedy action, a = argmax\\_a(Q(s, a)). We now allow all actions to be selected with some epsilon probability, but bias in favor of selecting the greedy action. We select all actions with some probability epsilon/|A(s)| (where |A(s)| is the size of the set A(s)), while selecting the greedy action with probability 1-epsilon + epsilon/|A(s)|. You can clearly see that this scheme extends to cases when multiple equiprobable greedy actions exist.\r\n",
        "> \r\n",
        "> This scheme of formulating a stochastic policy, the epsilon-soft policy, can be used to find a quasi-optimal policy, without the onerous requirements of exploring starts. A good result, that makes MC more practical.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nARxsm9-zoU"
      },
      "source": [
        "### Monte Carlo Part B-2: Off Policy Methods\r\n",
        "\r\n",
        "#### On Policy Methods\r\n",
        "\r\n",
        "> The final element of this topic is the notion of on and off policy learning. On Policy learning is what we've been doing so far, and is a special case of Off Policy, as we'll see soon. In On Policy learning, we are executing GPI to identify the optimal control policy, pi\\_\\*, and we're using the instantaneous policy, pi\\_k, as the behaviour generation policy.\r\n",
        "> \r\n",
        "> We should realize, however, that there is a conflict in the above. Let's say we start with a stochastic initial policy, pi\\_0 (recall, we know at least one means to obtain this). We can see that as the GPI iteration proceeds through the interleaved iteration of Eval\\_trunc and Impr, we will get better and better policies.\r\n",
        "> \r\n",
        "> Now, on the one hand these better policies -- **to be useful for control** (i.e., our ultimate goal) -- must be progressively more strongly deterministic (barring ties, which introduce the only necessary randomness, if we have no deterministic way to break the tie).\r\n",
        "> \r\n",
        "> On the other hand, however, these policies -- **to be useful for evaluation** (our sub-goal GPI) -- must be effective samplers (\"explorers\") of the S x A space, which suggests randomness (specifically, for all s, pi(s, a) > 0 for all a \\\\in A(s)). Recall: we need evaluation to succeed, because it is the foundation for greedy improvement.\r\n",
        "> \r\n",
        "> This is a very real problem that must be addressed, which we'll do by generalizing to the use of two policies.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s04ACdu7-zrE"
      },
      "source": [
        "#### Off Policy Methods\r\n",
        "\r\n",
        "> In the general case of off-policy learning we have two policies of interest, pi and b. Pi is the policy we are trying to solve for in GPI -- the optimal policy that will allow our agent to control the environment optimally. Policy b is a behaviour generation (\"exploratory\") policy: it has, as per the argument so far, freedom to sample the SxA space due to its stochastic nature. Policy b is perfectly suited to its job: allowing M to sample E as required to, in a monte carlo sense, estimate Q\\_b. Policy pi gets refined (improved) using this Q\\_b. But how can we improve policy pi using the Q from another policy b?\r\n",
        "> \r\n",
        "> Clearly we must have some constraints on pi and b; that constraint is that of _coverage_. Policy b must cover pi, which means that in a state s, pi(a|s) > 0 => b(a|s) > 0. That is, actions taken under pi must also be taken under b. Policy b is thus free to be stochastic and exploratory, given that it is guaranteed to take actions of pi.\r\n",
        "> \r\n",
        "> In (5.3) and (5.4) we see that there is a scaling factor that will relate the value functions computed via b and that of pi. This is an excellent result that enables the use of Off Policy methods: we can indeed look at one policies behaviour in order to optimize another (closely related, per _coverage_) policy. This is the fundamental lesson of section 5.5 in the textbook. The textbook shows two strategies of obtaining the required ratio relating the returns from b vs those of pi: ordinary importance sampling (5.3) and weighted importance sampling (5.6). For this course, it is not important to understand the theoretical foundations for these (largely because it is an area of research for MC, which is out of scope): you must understand how to compute these different ratios, operationally, in an algorithms.\r\n",
        "> \r\n",
        "> Further to the goal of computation (our goal in this course), the textbook in section 5.6 covers recursive formulations of the computation of an average. We must remember that with RL (in contrast to supervised learning that you may have encouraged in the past; if you haven't it is fine, it is not necessary for this course) training is _on-line_. That is, we are learning from experience, i.e., learning _while_ in contact with the environment. (One may also consider simulation-oriented training but even with simulations, the agent is still operating as if it is coupled to the problem space, and learning on-line).\r\n",
        "> \r\n",
        "> Hence, we must be able to compute averages on a sample-by-sample basis, versus the bulk computation that we're normally accustomed to. Section 5.6 presents this incremental computation of the average, and p 110 shows the algorithm for MC Off Policy prediction.\r\n",
        "> \r\n",
        "> With this, we can, as we did for all prior cases, obtain the GPI algorithm for MC Off Policy control on p 111 (of section 5.7).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKaKcZt4-zts"
      },
      "source": [
        "#### Comments\r\n",
        "\r\n",
        "> MC learning has many open issues that require theoretical justification, that are still an open area of research. It represents the opposite case to Dynamic Programming in many respects. Clearly with DynProg, we had p() while with MC we don't. Beyond this, the solution of DynProg refined old estimates to form new estimates of V or Q; this is called _bootstrapping._ This is _not_ the case with MC. With MC, our evaluation is based on the episodes, and is not based directly on refining a prior estimate of the value function (justify this, by looking at the computation of the Q estimates). This makes the algorithm more amenable to efficient computation _and_ also makes it less sensitive to E's violations of the Markov property. Skip 5.8 and 5.9, and read 5.10.\r\n",
        "> \r\n",
        "> Exercise:\r\n",
        "> \r\n",
        "> Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem. You must discretize the environmental feedback (S) in order to solve this problem properly.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCXGRvYovDCI"
      },
      "source": [
        "---\r\n",
        "##### Exercise solution\r\n",
        "\r\n",
        "Below is the solution to the off-policy Cart Pole problem. Here below sets up a function to generate episode:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdWErCtdL9Wz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22c9080-5596-4a6b-8b14-fd926e8848bb"
      },
      "source": [
        "#%% Boilerplate setup\r\n",
        "\r\n",
        "import gym, numpy as np, matplotlib.pyplot as plt\r\n",
        "def test_cart_pole(policy_fun, env=gym.make(\"CartPole-v0\"), \r\n",
        "                   max_iter=1000, print_iter=False):\r\n",
        "    \"\"\"\r\n",
        "    Test the policy against the environment\r\n",
        "\r\n",
        "    Parameters\r\n",
        "    ------------------\r\n",
        "    policy_fun : function\r\n",
        "        A funtion that takes the observations tuple from the CartPole \r\n",
        "        environment and returns a valid action of the environment\r\n",
        "\r\n",
        "    Yields\r\n",
        "    ------\r\n",
        "    action, obs, reward, done, info\r\n",
        "        Information for the current iteration and its results\r\n",
        "    \"\"\"\r\n",
        "    obs = env.reset()\r\n",
        "    for i in range(max_iter):\r\n",
        "        action = policy_fun(obs)\r\n",
        "        next_obs, reward, done, info = env.step(action)\r\n",
        "        if print_iter:\r\n",
        "            print(f\"State[{i}]={np.array2string(obs, precision=2, suppress_small=True)}, Action[{i}]={action}, Reward[{i+1}]={reward}\")\r\n",
        "        yield action, obs, reward, done, info\r\n",
        "        obs = next_obs\r\n",
        "        if done:\r\n",
        "            if print_iter:\r\n",
        "                print(f\"State[{i+1}]={np.array2string(obs, precision=2, suppress_small=True)}\")\r\n",
        "            break\r\n",
        "    env.close(); \r\n",
        "    if print_iter:\r\n",
        "        print(\"Iterations that were run:\", i)\r\n",
        "\r\n",
        "# Test that the function works\r\n",
        "rng = np.random.RandomState(0)\r\n",
        "_ = list(test_cart_pole(lambda _: rng.randint(0, 2), print_iter=True))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State[0]=[-0.01 -0.01 -0.02  0.  ], Action[0]=0, Reward[1]=1.0\n",
            "State[1]=[-0.01 -0.2  -0.02  0.29], Action[1]=1, Reward[2]=1.0\n",
            "State[2]=[-0.02 -0.01 -0.02 -0.01], Action[2]=1, Reward[3]=1.0\n",
            "State[3]=[-0.02  0.19 -0.02 -0.31], Action[3]=0, Reward[4]=1.0\n",
            "State[4]=[-0.01 -0.01 -0.02 -0.02], Action[4]=1, Reward[5]=1.0\n",
            "State[5]=[-0.01  0.19 -0.02 -0.32], Action[5]=1, Reward[6]=1.0\n",
            "State[6]=[-0.01  0.38 -0.03 -0.62], Action[6]=1, Reward[7]=1.0\n",
            "State[7]=[-0.    0.58 -0.04 -0.92], Action[7]=1, Reward[8]=1.0\n",
            "State[8]=[ 0.01  0.77 -0.06 -1.22], Action[8]=1, Reward[9]=1.0\n",
            "State[9]=[ 0.03  0.97 -0.08 -1.54], Action[9]=1, Reward[10]=1.0\n",
            "State[10]=[ 0.05  1.17 -0.11 -1.85], Action[10]=1, Reward[11]=1.0\n",
            "State[11]=[ 0.07  1.36 -0.15 -2.18], Action[11]=0, Reward[12]=1.0\n",
            "State[12]=[ 0.1   1.17 -0.2  -1.94], Action[12]=0, Reward[13]=1.0\n",
            "State[13]=[ 0.12  0.98 -0.23 -1.71]\n",
            "Iterations that were run: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpjrdXBivYFd"
      },
      "source": [
        "We require a consistnt scheme to discretize the `observation` into discrete indexed states. Below, each of the four observation variables are discretized into 4 states, generating a total of $4^4=16$ possible states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuYINVvKOq62"
      },
      "source": [
        "#%% Discretization of states\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Observation:\r\n",
        "    Type: Box(4)\r\n",
        "    Num     Observation               Min                     Max\r\n",
        "    0       Cart Position             -4.8                    4.8\r\n",
        "    1       Cart Velocity             -Inf                    Inf\r\n",
        "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\r\n",
        "    3       Pole Angular Velocity     -Inf                    Inf\r\n",
        "The episode ends when the pole is more than 15 degrees from vertical, \r\n",
        "or the cart moves more than 2.4 units from the center.\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def obs2state(obs):\r\n",
        "    \"\"\"\r\n",
        "    Discretize observations to states\r\n",
        "    \"\"\"\r\n",
        "    digitized = [np.searchsorted(bins, o, \"right\") \r\n",
        "            for o, bins in zip(obs, obs2state.bin_divide)]\r\n",
        "    return (digitized * obs2state.multiplier).sum()\r\n",
        "\r\n",
        "obs2state.bin_divide = (\r\n",
        "        np.linspace(-2.3, +2.3, 3),\r\n",
        "        np.linspace(-1.0, +1.0, 3),\r\n",
        "        np.linspace(-0.20, +0.20, 3),\r\n",
        "        np.linspace(-1.0, +1.0, 3),\r\n",
        "        )\r\n",
        "obs2state.multiplier = 4**np.arange(4)\r\n",
        "obs2state.S = 4**4 # 8**len(obs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Flm0uhNYvwko"
      },
      "source": [
        "Below, a helper function is created to generate an $\\epsilon$-soft policy based upon an existing policy `pi` to be used as the behaviour policy later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vduwwBlIRhP0"
      },
      "source": [
        "def get_eps_soft_policy(pi, S, A, eps):\r\n",
        "    # assert pi.ndim == 1\r\n",
        "    ret = np.full((S, A), fill_value=1.0*eps/A)\r\n",
        "    ret[np.arange(num_S), pi] += 1 - eps\r\n",
        "    return ret"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d2KGDdvwLtj"
      },
      "source": [
        "The off-policy monte-carlo learning algorithm is implemented below based upon page 11 of textbook:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3knchHBPecK",
        "outputId": "e89406bd-8987-4c96-ffac-877507610470"
      },
      "source": [
        "#%% Off-policy solution\r\n",
        "# Based on page 111\r\n",
        "\r\n",
        "class LazyStates:\r\n",
        "    def __init__(self, observations):\r\n",
        "        self.observations = observations\r\n",
        "        self.cache = {}\r\n",
        "\r\n",
        "    def __getitem__(self, i):\r\n",
        "        if i in self.cache:\r\n",
        "            return self.cache[i]\r\n",
        "        else:\r\n",
        "            ret = obs2state(self.observations[i])\r\n",
        "            self.cache[i] = ret\r\n",
        "            return ret\r\n",
        "\r\n",
        "gamma=0.9999\r\n",
        "env=gym.make(\"CartPole-v0\")\r\n",
        "rng=np.random.RandomState(0)\r\n",
        "\r\n",
        "num_A = env.action_space.n\r\n",
        "action_space = np.arange(num_A)\r\n",
        "num_S = obs2state.S\r\n",
        "state_space = np.arange(num_S)\r\n",
        "\r\n",
        "# Start of algorithm\r\n",
        "# Initialize\r\n",
        "Q = rng.random([num_S, num_A])\r\n",
        "C = np.zeros([num_S, num_A])\r\n",
        "pi = np.argmax(Q, axis=1)\r\n",
        "\r\n",
        "T_history = []\r\n",
        "S_visit_count = np.zeros([num_S])\r\n",
        "for i in range(3000):\r\n",
        "    # Once in a while we test the current greedy policy to see how it performs\r\n",
        "    if i % 100 == 1:\r\n",
        "        iters = len(list(test_cart_pole(lambda obs: pi[obs2state(obs)], print_iter=False)))\r\n",
        "        b_iters = np.mean(T_history[-100:])\r\n",
        "        \r\n",
        "        print(f\"Learning iter {i}, \" +\r\n",
        "        f\"off-policy average {b_iters}, \" +\r\n",
        "        f\"greedy policy lasted {iters} iterations, \" + \r\n",
        "        f\"eps = {eps}\")\r\n",
        "    \r\n",
        "    # Taper the eps value as the training progresses and \r\n",
        "    # the model gets closer to optimum\r\n",
        "    eps = 0.1 - 0.099/3000 * i\r\n",
        "    # b <- any soft policy\r\n",
        "    b = get_eps_soft_policy(pi, num_S, num_A, eps=eps)\r\n",
        "    # Generate an episode\r\n",
        "    actions, obses, rewards, dones, infos = zip(\r\n",
        "        *test_cart_pole(\r\n",
        "            lambda obs: rng.choice(action_space, p=b[obs2state(obs)])\r\n",
        "        )\r\n",
        "    )\r\n",
        "    # Compatibility with textbook notation\r\n",
        "    R = np.array([None] + list(rewards))\r\n",
        "    S = np.array([obs2state(i) for i in obses])\r\n",
        "    for s in S:\r\n",
        "        S_visit_count[s] += 1\r\n",
        "    A = np.array(actions)\r\n",
        "\r\n",
        "    G = 0\r\n",
        "    W = 1\r\n",
        "    T = len(actions)\r\n",
        "    T_history.append(T)\r\n",
        "    for t in range(T-1, -1, -1):\r\n",
        "        G = gamma * G + R[t+1]\r\n",
        "        C[S[t], A[t]] = C[S[t], A[t]] + W\r\n",
        "        Q[S[t], A[t]] = (Q[S[t], A[t]] + \r\n",
        "                        W / C[S[t], A[t]] * (G - Q[S[t], A[t]]))\r\n",
        "        pi = np.argmax(Q, axis=1)\r\n",
        "        if A[t] != pi[S[t]]:\r\n",
        "            # then exit inner loop (proceed to next episode)\r\n",
        "            break\r\n",
        "        W = W * 1.0 / b[S[t], A[t]]\r\n",
        "\r\n",
        "# End of algorithm"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning iter 1, off-policy average 33.0, greedy policy lasted 13 iterations, eps = 0.1\n",
            "Learning iter 101, off-policy average 23.09, greedy policy lasted 12 iterations, eps = 0.09670000000000001\n",
            "Learning iter 201, off-policy average 39.31, greedy policy lasted 15 iterations, eps = 0.09340000000000001\n",
            "Learning iter 301, off-policy average 23.01, greedy policy lasted 16 iterations, eps = 0.0901\n",
            "Learning iter 401, off-policy average 21.88, greedy policy lasted 14 iterations, eps = 0.0868\n",
            "Learning iter 501, off-policy average 23.25, greedy policy lasted 18 iterations, eps = 0.0835\n",
            "Learning iter 601, off-policy average 23.73, greedy policy lasted 53 iterations, eps = 0.08020000000000001\n",
            "Learning iter 701, off-policy average 105.47, greedy policy lasted 200 iterations, eps = 0.0769\n",
            "Learning iter 801, off-policy average 96.97, greedy policy lasted 142 iterations, eps = 0.0736\n",
            "Learning iter 901, off-policy average 39.76, greedy policy lasted 200 iterations, eps = 0.0703\n",
            "Learning iter 1001, off-policy average 35.81, greedy policy lasted 162 iterations, eps = 0.067\n",
            "Learning iter 1101, off-policy average 40.98, greedy policy lasted 25 iterations, eps = 0.0637\n",
            "Learning iter 1201, off-policy average 41.4, greedy policy lasted 14 iterations, eps = 0.0604\n",
            "Learning iter 1301, off-policy average 51.65, greedy policy lasted 12 iterations, eps = 0.057100000000000005\n",
            "Learning iter 1401, off-policy average 40.62, greedy policy lasted 25 iterations, eps = 0.0538\n",
            "Learning iter 1501, off-policy average 88.44, greedy policy lasted 149 iterations, eps = 0.0505\n",
            "Learning iter 1601, off-policy average 187.82, greedy policy lasted 200 iterations, eps = 0.0472\n",
            "Learning iter 1701, off-policy average 189.25, greedy policy lasted 200 iterations, eps = 0.0439\n",
            "Learning iter 1801, off-policy average 195.17, greedy policy lasted 200 iterations, eps = 0.040600000000000004\n",
            "Learning iter 1901, off-policy average 199.77, greedy policy lasted 200 iterations, eps = 0.0373\n",
            "Learning iter 2001, off-policy average 199.89, greedy policy lasted 200 iterations, eps = 0.034\n",
            "Learning iter 2101, off-policy average 199.9, greedy policy lasted 200 iterations, eps = 0.030700000000000005\n",
            "Learning iter 2201, off-policy average 197.78, greedy policy lasted 200 iterations, eps = 0.027399999999999994\n",
            "Learning iter 2301, off-policy average 199.39, greedy policy lasted 200 iterations, eps = 0.024099999999999996\n",
            "Learning iter 2401, off-policy average 200.0, greedy policy lasted 200 iterations, eps = 0.0208\n",
            "Learning iter 2501, off-policy average 200.0, greedy policy lasted 200 iterations, eps = 0.0175\n",
            "Learning iter 2601, off-policy average 200.0, greedy policy lasted 200 iterations, eps = 0.014200000000000004\n",
            "Learning iter 2701, off-policy average 199.97, greedy policy lasted 200 iterations, eps = 0.010899999999999993\n",
            "Learning iter 2801, off-policy average 200.0, greedy policy lasted 200 iterations, eps = 0.007599999999999996\n",
            "Learning iter 2901, off-policy average 200.0, greedy policy lasted 200 iterations, eps = 0.004299999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5uCEHNfwnvl"
      },
      "source": [
        "When using an $\\epsilon$-soft policy as the behaviour policy, we also visualize how the behaviour policy performed during the training - it should have been improving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ovpgOPMJa2wF",
        "outputId": "29a421b7-c39e-434d-8ce6-dc188d9609fe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(T_history, '.', markersize=1, alpha=0.5)\n",
        "plt.title(\"Performance of soft off-policy during training\")\n",
        "plt.xlabel(\"Learning Iteration\")\n",
        "plt.ylabel(\"Training Episode Lasted Iterations\")"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Training Episode Lasted Iterations')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9d5gcV5Ww/56e0eRRmKgcrDSSjSXbkgFLgCQwwdhmCQ6bsA0fXrAJuwtL2t3fCjYAu8C3ATCYxYnlMw7A2jLBQcFCso0t2XKQZmRJVpZmeoImJ830+f1RVa2aVofqNB3mvs9TT3dXuPdUdfc595577rmiqhgMBoPBAODLtAAGg8FgyB6MUTAYDAZDEGMUDAaDwRDEGAWDwWAwBDFGwWAwGAxBjFEwGAwGQxBjFLIMEakXke0i0iMi38m0PJlGREpFZJOIdInIQyksd6mI7LGf82dTUN4aETkgIr0i8kfp/B5F5IiIvMt+/1UR+e9Ulh+mvptFZEcS1/+piDyRSplSiYj8UET+PtXn5iqFmRYgHxCRI0A9MAr0Ab8FPq2qvQkUdyvQBkxWM4kE4CNYz7ZaVUdSWO4Xga2quhKC3+H/UdWnEizv68D3VPU/7PL+nnH4HlX1X9JVdqpQ1Z8BP0tH2Sn43lDVT6bj3FzF9BRSxzWqWgFcCqwC/i6ei8XCB8wD9iWiSEQkH438POD1FBsEp9y9aSwv4e8xn8j0bzLT9eckqmq2JDfgCPAu1+d/Ax6z378FeAboBF4G1rnO2wb8M7ATGAD+BzgLDAO9wLuAYuDfgVP29u9AsX39OuAE8CWgGfgpsBF4yC6rB3gVWAJ8BfADx4F3u2S4BWi0z30D+AvXMaf8z9vXngZucR0vBb4DHAW6gB1Aaaz7DvP8ltnPohNLsV5r7/+a/SzO2s/j42GuvRzYBXQDLcB3XceutcvrtMtfZu/fgtWrG7TLvR8I2N9BL/DFCHJ+AjgIdACPAjPt/YdCrr8/9HsMU9Y24BvA87bsjwBVsWQP/b3Z3/f/uI6tdT3348DNwGr72RS4zvsQ8HKE+6y276/blu8fgR32sfmAAoUh9/J/7Pc3Y/2e/y/QDvyTvW+H63wFPgkcsOX8PiD2sQKs31QbcBj4dGh9rnJ+Gvq9ueT7OHAM2G6f+xDWf6QL2A5c6CrnHuCfPP7m4zm3GthkP8cX7GexI9wzz6Yt4wLkwxbyJ51j/5n/EZhl/zGuwuqVXWl/rrXP3Wb/cC/EcuVNcv/o7HO+DjwH1AG1WH/4f3T9KEeAb2EZj1IsJTEIvMcu8z77z/W3dvmfAA67yn8/sBAQ4B1AP3BpSPlft6+9yj4+zT7+ffseZtl/5itsOaLed8izm4SlaL8KFAEbsAzUUvv4RlxKL8z1zwJ/br+vAN5iv1+C5cq70q7ji3Y9Ra5n/3/CfYcR6tmApagute/xv7AVTrjrQ7/HMOVtA04CFwHlwC+c+/Qge7Au9/PB6p30AH9sX1cNrLSP7QPe56r/V8DnI8j2c+BBW66LbDnjMQojwGewfn+lhDcKjwFTgblAK/Be+9gnbVlnA9OAp0Lri/TfC5HvPlt+p5HyMaCSc42sPeG+K2L/5uM59+f2VgYsxzLSxihMhM3+YfZitXqOAj+w/wxfAn4acu7jwE32+23A10OOB3909udDwFWuz+8Bjtjv12G1RktcxzcCT7o+X2PLVmB/rrT/NFMj3Mv/Ap9zlT/AWAXgx+oF+OxjK8KUEfW+Q/a/DasF53Ptux/Y6LqfaEZhO1aPoiZk/98DD7o++7CU2zrXs4/HKPwE+FfX5wqs3sD8cNeHfo9hytsGfNP1ebn9XRZ4kD1YF2ONwleAX0Wo70vAz+z3VVjKa0aY8wrs+2pw7fsX4jMKx0LKvJnzjcJa1+cHgS/b77cwtrf6rtD6wvz3whmFC6I8+6n2OVNCvyui/ObjOdf1HJe6juVET8GMKaSOP1LVqao6T1VvU9UBrJbbdSLS6WxY3fsZruuOxyh3JpahcThq73NoVdXBkGtaXO8HgDZVHXV9BkupISLvE5HnRKTDlu8qoMZ1fbuO9ef329fWACVYRisUL/ftvr/jqhoIucdZYc4Nx8exWtZNIvKCiFztKjf43Ozyj8dRbjg53eX1YvV+YpZnR6z02ttXXYfc3/1RrNZmTRKyzyH89wGWO/EaESkHrgd+r6qnw5xXi9XCD5UtHmL9psFqCDg4vymwfw9xlhVVBhEpEJFvisghEenGMiQw9nfuJtJvPp5zwz3HRO9lXDFGIb0cx2oxT3Vt5ar6Tdc5GqOMU1hK1mGuvc/r9RERkWIst8W3gXpVnQr8BsuVFIs2LDfVwjDHvNy3wylgjj3I7jAXq2UcE1U9oKp/jOVe+xbwsK34xjw3EREspRmp3Li+B7uOai9yquonVbXC3tzRQnNc7+ditSzbEpDd4Tjhvw9U9SSWq+1DwJ9j+ePD0YrlEgmVzaHPfi1z7ZseWl0MOaNxGst15DAn0okx6nLv/xPgA1i9jilYvQnw9jtPFOc5xnMvWYExCunFaZ29x26tlIjIOhGZHfPKc9wP/J2I1IpIDfD/2eWmgiIsH2srMCIi7wPe7eVCu/V6F/BdEZlp399bbUMTz33/Aat19UURmSQi67BcXj/3IoeI/JmI1NrydNq7A1guifeLyDtFZBLWYOAQ1phMOFqAC6JUdT9wi4istO/xX4A/qOoRL3JG4M9EZLmIlGH5pR+2e3Txyu7wM+BdInK9iBSKSLWIrHQdvw9rfOJNwC/DFWDX/0tgo4iUichy4CbX8VYs4/Rn9nf7MSIYogR5EPiciMwSkalYbq9oxPrewHKZDmH17Mqwvru0EuY5NgAfTXe9qcAYhTSiqsexWihfxVK8x4G/Ib7n/k9Y0TWvYEUSvWjvS4V8PcBnsf6IZ7BaVI/GUcQXbJlewIrI+RbW2IDn+1bVYSwj8D6sVvIPgI+qapNHGd4L7BWRXuA/gBtVdUBV9wN/hjUg3GbXcY1dXzi+gWV8O0XkC2HkfArL1/8LrNbsQuBGjzJG4qdYPupmLFfcZ+264pXdkfEYlvvv81jfxx5gheuUX2H1QH6lqv1Rivo0lguk2Zbv7pDjn8D6PtuxgiRiGat4+DHwBNbv/SWsnusIVrRYOKJ+bzb3YbnATmINYj+XQnmj8WmsnokTGXg/lnHKapwwMIPBMI6IyDasAeK0zkYOU+8hrIHchCd7jSd27/WHqjov5slZjoh8C5iuqjfFPDmDmJ6CwTBBEJEPY/nat2RalkjYaU2ust1fs4B/wOrh5Bwi0iAiF9sTUy/HCorI+nsxs/0MhgmA3TNZjjWnIxDj9EwiWCHGD2BFyv0aaxwtF6nEchnNxBr7+A7WJMWsxriPDAaDwRDEuI8MBoPBECSn3Uc1NTU6f/78TIthMBgMOcXu3bvbVLU23LGcNgrz589n165dmRbDYDAYcgoRiThL3biPDAaDwRDEGAWDwWAwBDFGwWAwGAxBjFEwGAwGQxBjFAwGg8EQJG1GQUTmiMhWEdknIntF5HP2/ioReVJEDtiv0+z9IiL/KSIHReQVEbk0XbIZDAaDITzp7CmMYC33txxrJaLb7TS8XwY2q+piYLP9GawsmYvt7VbgjjTKZjAYDIYwpG2egr2q02n7fY+INGKtHPUBrGXsAO7FWsrvS/b++9TKu/GciEwVkRkRVofKCkZHR3n2cAdvXVBFQUFBpsXxRCAQoLG5mylFPra93sby+nL2nu5lTlUpNZUlTCubxIvHu7hk9mTeaB+gqmwSVeVFHGztY1ppIT6fj+qKYnw+H7WVxVjrv4wPqkpr7xA15UW09Q0HX2srEpMjEAjQ1NJDQ30lPt+59pFTj5dyVZVTZ3rZ2tTKsroynjt8hvrJRfQNjlJeXMDxtl7eaO8FrKUOB86OgkJhYSELa8s42TnEtLICOvpGKC3yISKUFArtvWdRDTA4EqCksADQ894XF/gYGrVeB0dGEYTiQuu9e4kZEaFkknWdU7/7WHHh2HKc416OhdYVeqzAV8D0yUWc7BzEJ8LUskkMDI+cJ4fP52NKaSFn+ofPuw+vdaX7WOh+9+dIzz5dclSUFnPJgmr+5PL5TJo0iVQyLpPXRGQ+cAnWgir1LkXfDNTb72cxdrm6E/a+MUZBRG7F6kkwd657Qajx59nDHfztL1/lnz/0JtYuCjs5MOtoaunh64/uQ3yw68gZppRO4kzfWSpKCpgxpYT6KSXsPtrJm2ZN5pC/j6ryScytLmfPsU4mlxRQWlzI8hlTqC4v4pa1C6irLBk32Vt7h3joheOsX1bH1kZ/8PW61XMSkqOppYdv/LqRr7x/GctnTDmvnkjluo1Ga+8QGzc1snV/K5XFhZwZGKEAa5UfH5EXAQDg9a64ZTYYLHp59LV2JhVO4k/ePD+lJac9IZ6IVABPA/+sqr8UkU572Ufn+BlVnSYij2EtZL7D3r8Z+JKqRpyyvGrVKs3kjOZ87SnsPtbJvGkldA2OUlVeNKanICKICDWVJdRNLpmQPQV/z2DQaNRWFJueQpQW7khAaOsZYmpZIWVFhaankCU9BRHZraqrwh5Lp1GwlxJ8DHhcVb9r79sPrFPV0yIyA9imqktF5Ef2+/tDz4tUfqaNQr7iVnqhLeVox3KFeNxD6bh+IpHPzyqX7y2aUUhn9JEAPwEaHYNg8yjn1ny9iXP5xR8FPmpHIb0F6Mrm8YR8praiONgKjudYruC4h1p7E1sZUUSoqxzfXlKuks/PKtnfUbaStp6CiKwFfo+1hq+zqMdXscYVHgTmYq2ber2qdthG5HtYa+72A7dEcx2B6SkYEiOZFl4utw4NqSWXfwvRegrpjD7agbWKUjjeGeZ8BW5PlzwGg4PTek2EWIPQholDMr+jbMbMaDYY4iAf3GcGQzSMUTAY4iDVPnJVxd8ziFkW15AtGKMwwTFKKbPk62Clg/l95R7GKCRAPv3Q810pZTv57o5y/77y6X+TzxijkAD5pEgTVUrp/oMnUn4uKp1cCNlM5rm6f1/R/je5+N3lK8YoJEA+te5iKaVIf9Z0G8ZEys8nYz3eRFPKyTxX9+8r2v/GfHfZQ9rTXKQTM08h/USawZzuWcGJlJ/LceOZJtpM9fF4rua7G18yMqPZkB9Eat0l6/aI1TJMpPxccMVkK9Fa8ePxXM13lz0Yo2CISrx/Vq++4XxywYUj13zkRikbHIxRMKQUr77hfFdC8frIc82IGPKXmEZBRMpFxGe/XyIi19rZTw15RKqUUr73ALwS6zmEPm8z0GrIFrz0FLYDJSIyC3gC+HPgnnQKZRh/UqWU8r0H4JVYzyH0eRtjasgWvBgFUdV+4EPAD1T1OuDC9IplGG8mmlLKtLsm9HkbY2rIFjwZBRF5K/CnwK/tfbmxzJjBMxNNKWXaXTPRnrchd/BiFD4HfAX4laruFZELgK3pFcswHmS6tZxJnJZ6TXlRWp7BRH62htwmplFQ1e2qeq2qfsv+/Iaqfjb9ohnSTWvvEA8+f4zG5u68Vl7hFLTTUm/rG05Lj8HfM8jdOw7j7xlMabkGQ7rxEn20RETuFJEnRGSLs42HcIb0UltRzIbl9Wxp9EdVirne6o3mKkpkLMXL8xB7fSmJuM6UwZCdxExzISIvAz8EdgOjzn5V3R3juruAqwG/ql5k73sAWGqfMhXoVNWVIjIfaAT228eeU9VPxhLepLlIHi/pBaKlQMgFUp1CwcvzyKa0DdkkiyE7iJbmwotR2K2qlyVQ6duBXuA+xyiEHP8O0KWqX7eNwmPhzouGMQrjg1EqY8m155HrRt2QepLNfbRJRG4TkRkiUuVssS5S1e1ARwSBBLgeuN9D/YYMYyJlxpJrzyNbwo1z3Q05UfBiFG4C/gZ4BsuFtBtItnn+NqBFVQ+49i0QkZdE5GkReVukC0XkVhHZJSK7WltbkxTDkCy58kfPZjnTLVu2GLFMhwEbvOEl+mhBmO2CJOv9Y8b2Ek4Dc1X1EuCvgf8nIpMjyHOnqq5S1VW1tbVJimFIlmT+6OOpqLNZIWWzbKkkW3oshuh4iT6aJCKfFZGH7e3TyeQ+EpFCrNnRDzj7VHVIVdvt97uBQ8CSROswjB/J/NHHUxlms0LKZtlSSbb0WAzR8eI+ugO4DPiBvV1m70uUdwFNqnrC2SEitSJSYL+/AFgMvJFEHYZxwllRy1mDNx7SqQxDeyHZrJCyWTbDxMOLUVitqjep6hZ7uwVYHesiEbkfeBZYKiInROTj9qEbOX+A+e3AKyKyB3gY+KSqhh2kNmQfibb4U6EMk1kuNJvHGQyGTOElJPVF4DpVPWR/vgB4WFUvHQf5omJCUrODTIZousMtnR6L0/PI9PyLXAtdNUwckg1J/Rtgq4hsE5GngS3A51MpoCG3yaT7w+2CcvcOvMiUbl/+RBlANuQXMXsKACJSzLmZyPtVNSt+5aanYHCTbS3zbJPHYHCI1lMojHLRBlXdIiIfCjm0SERQ1V+mVErDuJDPisrpHYSS7D0nen0keQyGbCaa++gd9us1Ybar0yyXIU3kyryCVNafrBvHuIEMEwkvA80LVPVwrH2ZwLiP4ifRVq+q0tjczZZGP9dnKIdOogPD8dxzuHPzuXdlmJgkO9D8izD7Hk5OJEOmSHRQuLV3iC37WtiwrC5jk6wSHRiO557D9QrMPALDRCLamEID1lrMU0LGFSYDxlE6waitKOb6y+dmtLU8Hj768Z5dbHohhmwjWk9hKdbYwVTGjidcCnwi/aIZsol0t5a9jBdEOicVYx1OGcC49grMeIUh24hoFFT1EXv28tWqeotr+6yqPjOOMhrSQKYHjUPxohwjnZMKxZop5TxR8h4ZcgcvA80lwMexXEnBvruqfiy9osXGDDQnTrRB20y4NLzUGemcVMhr3DiGiUSyA80/BaYD7wGeBmYDPakTz5AJorVQk201J9IL8eKeinSOe3+iPaDQsqOVk4yry2DIdrwYhUWq+vdAn6reC7wfeHN6xTKkm2hKOFmXRib95KmqO1o5ybi6DIZsx4v76HlVvVxEtgO3Ac3A8ylYaCdpjPsoOwh1vWTSFZPMPAyv95CMq8tgyAaSdR/dKSLTgL8DHgX2Ad9KoXyGLCNe10doqziTcf3JzMPweg/JuLoMhmwnqlEQER/QrapnVHW7ql6gqnWq+qNxks+QAeJ1fURzN+WKb91EARkMFlGNgqoGgC+OkyyGDOJW3rEUZDyrmo23bz1VA83jXb/BkC14cR89JSJfEJE5IlLlbGmXzJB23AosnrUIwin6SMow3hZ4sko10wO8ma7fYEgWL0bhBuB2YDuw295iju6KyF0i4heR11z7NorISRHZY29XuY59RUQOish+EXlP/LdiiBe3AotHeYc7N5IyjLcFnqxSzbQbKNP1GwzJ4mmRnYQKFnk70Avcp6oX2fs2Ar2q+u2Qc5djrdt8OTATeApYoqqj0eow0UfJkcoImVSVleqonUxlPc2miCyDIZSkoo9EpExE/k5E7rQ/LxaRmOspqOp2oMOjjB8Afq6qQ3ZK7oNYBsKQYtzumVRGyKSqrETKieZyCtfzGA8XT2gdxq1kyBW8uI/uBoaBK+zPJ4F/SqLOT4vIK7Z7aZq9bxZw3HXOCXvfeYjIrSKyS0R2tba2JiHGxCSXlJPX8YVo9xTOnZOIiyfesY7QOoxbyTtmsD6zeDEKC1X1X4GzAKraDyTaHLwDWAisBE4D34m3AFW9U1VXqeqq2traBMWYuKRz4DfVf2avBizaPYXreSTSG4nXmIbWYeYteCeXGi75iBejMCwipYACiMhCIKFvS1VbVHXUDnX9MedcRCeBOa5TZ9v7DCkmnQO/qf4zezVg8dxTooYrVBbTmk0fpleVWbwYhY3A74A5IvIzYDPwpUQqE5EZro8fBJzIpEeBG0WkWEQWAIuB5xOpw5Bako1KSoZ0tK5DDZdX5R4qi2nNpg/Tq8osEVdec1DVJ0RkN/AWLLfR51S1LdZ1InI/sA6oEZETwD8A60RkJVav4wjwF3Yde0XkQawUGiPA7bEijwzjQ+hqZ9GiaMZjZbRkqSkvYv2yOmrKi4Bzyj3edZ9Na9aQr3hJiLdZVd8Za18mMCGp40+0dRhygVD5TaioYSISLSQ12hrNJUAZVkt/GucGlycTITLIkP8k2kJ2K18gY4o4tKeQC70bg2E8iTam8BdYs5cbgBc5N5v5EeB76RfNkI2kIgtpJv3xbX3DbG3009Y3PO51Gwy5gBf30WdU9b/GSZ64MO6j9JKuGc+qSlNLDw31lfh8vojnpWNmtHEXGQwJzmgWkQ+JyIeAk85795Y2aQ1pJxWTwuLF3cOI1lqPVWeyspvIFoMhOhF7CiJyd5TrVFU/lh6RvJPJnkIutzi9Dhan6x6TWdUs07IbDPlAtJ5C2hLijQeZNAq5HIWTywozl2U3GLKFpBLiGcKTy3HquexCyQXZHRdXIBAws54NOYcxCgmSC8ppvDApH8IvWNTU0mNmPRtyDmMUDEmTyykfUmXQwi1Y1FBfOa6rzhkMqSDa5LWoEUaq+svUi2PIBMlOLBtPV1oiYwrRrkk0zUUo7mfgnhAXT5mpksVgSIZoPYVr7O3jwE+AP7W3/wYyHnlkSB2RJpYlmixuvGRNxTWpMmipeAa1FcV8ZNVsUExvwZAxIhoFVb1FVW8BJgHLVfXDqvph4EJ7nyEHCafo3YrR/T7RjKLpkDGcrF5line9hUwhIogID+3KTVecIT/wMqYwR1VPuz63AHPTJI8hzYRrNbsVo4gEDUJNeRHXrZ5DTXkR/p5BWntSN3YQ7xKa4WSNdW6kaxKRabzI5ag2Q37gxShsFpHHReRmEbkZ+DXwVHrFMqQLL0rHUbRtfcPUVZbQ1jfMQy8cR9GUKaxUuXRSqURTMWCerGHJpp6LYWLiafKaiHwQeLv9cbuq/iqtUnnE5D6KHy8DtYFAYExuonRMGMvGSWipkCmXJzUaJg6pmLz2IvBrVf0r4HERqUyZdIZxxUtrODQ3UTpar5loEcdqxSciU2iZxv1jyHViGgUR+QTwMPAje9cs4H/TKZQhfXhRWqlQbOEUcKZ99umYTxFapnH/GHIdLz2F24E1QDeAqh4A6mJdJCJ3iYhfRF5z7fs3EWkSkVdE5FciMtXeP19EBkRkj739MLHbMcTCi9Lyck4sBR9OAXtVyskaj0jXu41dpHPirdv0DAz5hhejMKSqwRzHIlKItcZyLO4B3huy70ngIlW9GHgd+Irr2CFVXWlvn/RQviGDxFLw4ZSlVwWabIveS9rsSOfEW7fpGRjyDS9G4WkR+SpQKiJXAg8Bm2JdpKrbgY6QfU+o6oj98TlgdpzyGrKEWAo+nLL0qkCTbX1Hut7dC4h0jmn5GyY6XozCl4FW4FWsJTp/o6p/m4K6Pwb81vV5gYi8JCJPi8jbIl0kIreKyC4R2dXa2poCMQyRiOZKcc9nSPUYQbKt70jXu3sBkc4xLX/DRMeLUfiMqv5YVa9T1Y+o6o9F5HPJVCoifwuMAD+zd50G5qrqJcBfA/9PRCaHu1ZV71TVVaq6qra2NhkxDCGEGoFYrpRcS4RnegEGQ2y8GIWbwuy7OdEK7QlwVwN/qrb2UdUhVW233+8GDgFLEq3DkBiOkvf3DOLvGTxvRnO0gdtsxjF2QNaFwRoM2Ua0NZr/WEQ2Ybl1HnVt2wgZK/CKiLwX+CJwrar2u/bXikiB/f4CYDHwRiJ1eMX8Wc/HUfKChJ3RnKvrHcfbo0lVZFIidRsMmSbaGs3zgAXAN7DGFRx6gFdcA8aRrr8fWAfUYOVL+gesaKNioN0+7TlV/aSIfBj4OnAWCAD/oKoxB7OTmdFsZp5GJnRmb6ZmH6eq3njLifTbSOQ3k40ztw2GpNZoFpFyYEBVAyKyBGgAfquqZ1MvanwkYxTMnzX7yZThjvTbML8ZQ76QbJqL7UCJiMwCngD+HGsOQk6TK66PXCEd7rhMjVmYyCTDRMaLURDb//8h4Aeqeh3WmgqGPCVbfOdGCRsM448noyAib8Vade3X9r6C9IlkyDSJKPhsjERK5YCxwTBR8GIUPoc1QPwrVd1rRwdtTa9Yhkyhqqgq162KT8FnY6s+VaksDIaJhKf1FLKVTK2nkM8DjvkUlWUGjA2G8CQ10GzPIfg3EfmNiGxxttSLmTvkc0vTixsoV9wvyQ4Y58p9GgypxIv76GdAE9acha8BR4AX0ihT1pON/vN4iJXTKJbCDJ35nOoU19lCPht/gyESXoxCtar+BDirqk+r6seADWmWK6vJRv95PCSr7EJnPqc6xXW2EK/xz3YjZzB4wYtRcCapnRaR94vIJUBVGmUaFybyHzjZno5jFGsr05PiOlsIZ/yj/W6y3cgZDF7wYhT+SUSmAJ8HvgD8N/CXaZVqHJjIf+B4lV085cRTXi72uPw9g9y943AwwZ6bVBq5idxoMWSWmEZBVR9T1S5VfU1V16vqZcDCcZAtrWR7K3W8iWYks2UyWzYgyJjXMcdSaOTy9fkZsp+EQlJF5Jiqzk2DPHGRqZDUfCRamKY7TNVZWCdWOGessM9cDQsdL7lz9fkYcoNkcx+FLTMJeQxZSLRWrrtX5bUFG6vVnKstYfd9pdPFk4uuNUN+kKhRMI7OCYRbQaXK7ZYP7rtcNWwGQzSiLbLTIyLdYbYeYOY4ymhIMe4WbqYGNBNpCWfb4KvbsGWbbAZDokQ0CqpaqaqTw2yVqlo4nkIaUou7hRtvazeTreNM1O11op/pNRjyBZP7aALiHsQEwr6P1ILP5ADoeNbt1IXCQ7ti54IyA8OGXCIdA81eK75LRPwi8pprX5WIPCkiB+zXafZ+EZH/FJGDIvKKiFyaTtkcJmK3393CjdXaDX0+mRwAHc+6nWehqKexDzMwbMgX0moUsFZoe2/Ivi8Dm1V1MbCZc+s/vw9YbG+3AnekWTZgYg8Whir8cIO/0SZr5TPOs6irLMn5sQ+DIR48GQURmSci77Lfl4pIpZfrVHU70BGy+wPAvfb7e4E/cu2/Ty2eA6aKyAwv9SRDPkTBJEqoQQzX2o02WSufSablP5EbGobcx0vq7E8AD/37msYAACAASURBVAM/snfNBv43iTrrVfW0/b4ZqLffzwKOu847Ye8LledWEdklIrtaW1uTECNY3oTt9nsxiLWVxdyydgG1lZk1muFa39naIp/IDQ1D7uOlp3A7sAboBlDVA0BdKipX698c1z9aVe9U1VWquqq2tjYVYhiikC1GM1zrO1tb5M4zA7LSaBkM0fBiFIZUddj5ICKFJDd5rcVxC9mvfnv/SWCO67zZ9j5DmnCvi9DSPYC/O3sVWLjWd7a2yJ0eTGtPdhotgyEaXozC0yLyVaBURK4EHgI2JVHno8BN9vubgEdc+z9qRyG9BehyuZkMacC9LsI9O49w987DWavAwo53ZEkvJpR4I5fcZKtLzDBxiDlPQUR8wMeBd2PlPHoc+G/18KsVkfuBdUAN0AL8A9Z4xIPAXOAocL2qdoj1z/4eVrRSP3CLqkadhJDpeQr5EpvuKCLUUrS1lcXB3D75cH/jhfO8asqLaOsbTui55dMa2YbsJdo8BTN5LQny7Q8cej/5dn/pJhXPyxhiw3iQkFEQkVeJMnagqhenRrzEybRRyLc/cOj9BAIBmlp6aKivxOeL7mnMt2eRCOYZGHKFRGc0Xw1cA/zO3v7U3n4L/CbVQuYi2erTjkUkv7WTBbW1dwhVpa13mE17TtHWOxyhpHMkGwmUD770XP09GAxuoiXEO6qqR4ErVfWLqvqqvX0Ja3zBkKO4o45CFbFbuavdUVQPwWbJRgKleuW3bCLX5TdMLLxEH4mIrHF9uMLjdXlNLv/R3VFHoYrYrdzrKku4Ze0CT/7xZFvJ0YxKts5H8Equy2+YWHiJProMuAuYghV9dAb4mKq+mH7xopPJMYVIg4q55Ff2ImvoOZm4v1x6ppAdz8xgiEZSWVJVdbeqrgBWABer6spsMAiZJlLLNpdahV5a96H3k4n7S7WvPt29PC85pcZDDoMhEbzkPpoiIt/Fymi6WUS+IyJT0i9adhPpj56ts2wTJfR+8uH+0m3YvD6jXGpAGCYOXtxHvwBe41xm0z8HVqjqh9IsW0wyHZJqyE2yxZ2TLXIYJh7R3EdeltVcqKofdn3+mojsSY1oBsP4405YZ+QwGMbiJYpoQETWOh/sSKSB9IlkMBgMhkzhpafwKeBeexxBsBbNuTmdQhkMBoMhM8Q0Cqq6B1ghIpPtz91plyrLmQi+YBNWmV7M8zRkK16ijz5nG4Qe4Lsi8qKITOgZzemOGsmGUMVsCEXNZ8zzNGQrXsYUPmb3Dt4NVGNFH30zrVJlOekOy8wGhZGPoajZhHmehmzFU5oL+/Uq4D5V3evaNyFJd+KzbFAYofeYqnvOhl5QNmCS5xmyFS9GYbeIPIFlFB4XkUogkF6xJjb5rDCyoRdkMBgi48UofBz4MrBaVfuBIuCWtEplyDpS1cLPhl5QNhDP8zS9K8N4EtEoiEiD/Xal/XqBiFwKzMNbKGukcpeKyB7X1i0ifykiG0XkpGv/VYnWYUg9qWrh53MvKB68Pk9VpbG5mwdN78owTkRbee1OVb1VRLaGOayquiHpykUKgJPAm7F6H72q+m2v12dzmot8CTlMxbrDEwmv37vX8/w9gzz4/DE2LK9n2fTJ5tkbUkJCWVJV9Vb7dX2YLWmDYPNO4JC9mE9eMR6+8/FwKzj30dY3bFr4HvD6vXvtMdVWFHP95XONQTCMG17mKZSIyF+LyC9F5Be2qydVCVtuBO53ff60iLwiIneJyLQI8twqIrtEZFdra2uKxEg94+E7T7XhCWdk3PdhfNuxSfX3btxthvHGy0DzfcCFwH8B37Pf/zTZikWkCLgWeMjedQewEGsM4zTwnXDXqeqdqrpKVVfV1tYmJUMsJZeMEhyPP3OiCsh9X+734YyMO2mb8W1HxnmOgFHihpzGi1G4SFU/rqpb7e0TWIYhWd4HvKiqLQCq2qKqo6oaAH4MXJ6COqISq6Wd7eGTiRoe932538daEnPLvhY2LKub8JFD4cj234rB4BUv6yn8D/A9VX3O/vxm4HZV/WhSFYv8HHhcVe+2P89Q1dP2+78C3qyqN0YrI9mB5liDffkyWByK+76AsPeYjtxH+fo8Ib/vzZB/JLUcJ3AZ8IyIHBGRI8CzwGoReVVEXklQoHLgSuCXrt3/6ipzPfBXiZQdpxxRW9r56s9131eke/S6pGQ8pKM1nS3jHPn6WzFMPLz0FOZFO57JyKFsDknNddLR8k1Hmf6eQR564TjXrZ5jFqwxGDySUE9BRDZAUOn7VPWoswGXud4b8pDQlm8qWuTpaE2bGdIGQ2qJ5j5yTyL7Rcixv0uDLIYsJlsHUvPFbZMtbjCDIZpRkAjvw33OafLxD5nIPUW6xgldvW6VaZGni2w1uoaJRzSjoBHeh/uc0+TTH9JR7K098d9TpOfQ2jvEw7tOgJDzLfJsxbjBDNlCtNxHncB2rF7B2+z32J/XqmrYGcfjSaoGmlOdryaTOAOvH1k1GxGJS9ZI95cL920wGLwTbaA5mlF4R7RCVfXpFMiWFMkYhXCKLpbyc0e61FYUZ2WiuHQrcGMgDIbcJ9GEeE9H29In7vgQ6irxkqLY3cV3rm9q6ckq15M7LUWkMYVkcvnnk6vNYDCcj5fJa3lJqA/XSxoHd6SLc31DfWVW+oKjKe9ox0LzIoUaypryItYvq6OmvCjt9xCLfAwQMBgyzYQ1CqGhjOFSFEdTOtkeChlt4DLcsXAD1OEMZVvfMFsb/bT1DY/bvUTC9FoMhtQzYY1CKOGUvBelk62KKZrRinavigYNRjhDmU1RMuMti+mZGCYCXtJcbOL8ENQuYBfwI1UdTJNsMUllmotEBp6TuS4bCE2M5+8ZRBBqK7Nb7nhI5XdhUmoY8oVkE+K9AfRipbP+MdAN9ABL7M95QbS1BOIxCJHKikQmWp/hXEVOcryHdmVfrycZUtmTy6ZeksGQLrz0FF5Q1dXh9onIXlVNxdoKCZHunkIsIrUc4ykr1a1PL3VHmssQCARoaumhob4Sny8/PIu50mvzQj7diyGzJNtTqBCRua7C5gIV9sfMjzamiEQGjmsrivnIqtmg51JBxLv6Vqpbn15axk6ddZUlY+TMpkHkZHD3vrI9ICAesnX8ypBfeDEKnwd2iMhWEdkG/B74gr0mwr3pFC4XaOsZ5PtbD+DvHkzoT5sqpeUowpryophGJlKdsQxUvK6uTA3M5qvyNO4rw3gQ0yio6m+AxcBfAp8Dlqrqr1W1T1X/Pd0CZjOtvUM88MJxXjnRRXtf5OUsU6EcY5XhKMK2vuGwCj/W9V5cE/Eq20wp52SUZzZHGOVTr8eQvXh1HF+GtS7zCuB6EUlqKc58obaimNvWL+KfP/gmGuywTS+rmCWCv2eQu3ccDrqnwskSTRHGux51OOXoVdk611aXTRq3iW6pchkl+11ls1ExGLwQ0yiIyE+x1lZYC6y2t7ADFPFgL+/5qojsEZFd9r4qEXlSRA7YrxlPuhcNEWH6lDKWz5wSdWDWi1umpXsAf3f4tNX+nsFgULBEyFoeSRF6dSuFm+EdbzSWg3Ptfn/vuI1RpKpXkqyLxt89yPe3HsTfnbFIbYMhKbz0FFYBa1T1NlX9jL19NkX1r1fVla5R8C8Dm1V1MbDZ/pw1hKaA8PcMEggEYrYMwylTd1mtvUPcs/MId+88HDZt9UMvHAeBW9YuoLay+LzroxHJrRTr+tBB9HjIRAqQVPnbk3XRtPUN0XSqm7a+/BrPMEwcvBiF14Dp6RbE5gOcG7y+F/ijcarXE+7WqOPOaWruTmhOgrus2opibl4zn1vWLDhPqUWKFPLaMnaurykvGpvYrmeIu3ccprVnKGyOo2TmLTiK1efzpdwHHsmYZYu/fdn0yfzDBy5k2fTJGZXDYEgUL0ahBtgnIo+LyKPOloK6FXhCRHaLyK32vnpVPW2/bwbqQy8SkVtFZJeI7GptbU2BGN5xt0YdN051eeQWarQMo2PKEqF+cim1lVb2VbfCi6TsasqLWNdQiwZ0zPmRXFFtvcNjxwxsf5SiEZMBpjraJRl/ezKLB6WCcL3EcPfh8/lYPiO6O9FgyGa8TF4Lu65CsumzRWSWqp4UkTrgSeAzwKOqOtV1zhmNsphPKievRSPRVBYt3QPcs/MIN6+ZT/3k0rjWa4g2kc1p2W/acwqw3ErudNl37zgc3A+MmajmrP/gXgcCGJdJUclM1Etm8aBU4JYdMOkuDDlNQovsjCcishErlcYngHWqelpEZgDbVHVppOvGyyiEU2aeZg53D3LXjje4duUsGqZXxlyMx+uMVX/PIA8+f4x1DbX4xEfD9HMzkJ1WrJPDCMYq/HgMT6oNRTJlZno2b2ieKDOz2JDLJDSjWUR22K89ItLt2npEpDtJgcpFpNJ5D7wba+ziUeAm+7SbgEeSqSdVhHOjeFmToKaiiGsvmcWWJj9Nzd1BH36k8yH6TGh3qOeG5fXUVpSwtWlsdI/jiqqbXBIcF3CX6XU9hHD3l6z7JxlFmukxA3f9mZbFYEgn0VZeW2u/VqrqZNdWqarJjqLVY82Sfhl4Hvi1qv4O+CZwpYgcAN5lf8444ZRANH+7O+Jn2fTJXL96DtXldtTQeQlnvQ8ah4Z6IsTt8/eayiJeQxiLfJ1lbDDkG57cRyJSgKXIC519qnosjXJ5ItVrNHs5lkjZoS6dROtzzgtdFzoeeb2Mj0QqL5fdP+kgH+/JMDFIKiGeiHwGaMEaDP61vT2WUgkzgNflKqPNR/AaHtnaO8TDu06A4HnRGzeh7qXQUM94WuHh6gq930hrVcfrNsnXxHQOpvdjyEe8xM05+Y4uVNU32dvF6RYs3XhdrtL54ze19AQVwOjoKDsOttLSNcBDLxzH3zMY1WC4y0vELx9L+SQbOuoeZ4gUnpoI6VKaXp9hulNOJPu9GgzZiBejcBxrpbW8IlrL1TkGlmK5btW52bk15UX8dm8zf/vLVznQ2st1q+cgyHnKz60Q3XUloihjKf3kZ+GeG2cItwRnoqQrq2e8YzDRggGSUeLJfq/hMMbFkGm8zFP4CbAUy20U/MWr6nfTK1ps0h2SGi58098zyM+fO8KCugpWz6uifkopcH6IYrx++VT5pxMpJ53jK+kg3jGYcOdlYnEjL5glPw3jQVJjCsAxrPGEIqDSteU94Vq6tRXF3PiW+bx5QQ2/ePHkeT0Bh0it90j7U9XS9JLtNHRfOJkyPYM4Gl57RtHOS3UvJlVjJmbNBEOmyYrJa4kyXpPX3ESKAEq0nHgjiGKdF3o8Um8nVms0UzOIU9FrSfTZGgwThUQnr/27/bpJXDmPJHW5j3KSSFlHEy3HnYDOU1pqVyK7cISWE6m3E6s1GikRX7xE8pFH2u81Kiwa0aKpsmWQ2mDIVqK5j35qv34b+E6YbULhnk2cioVjvKSmjpaELdwkuHA4rXx/z2AwSZ4XAxR6TjQlGe1YJEUeab/XqLBwhPuO3NFUNeVFEcNtvcodDWNIDPlAtBnNu+3Xp8Nt4ydiduC00Pe3pGbhGCddQqTU1KEtXEdJOesqxDMI2dobeb2GeMoIpySjzWuAyIo8dL+XVB+xjFnojO/QaKq2vmHP4baJ+PbNvAVDPuAl+mgx8A1gORDURKp6QXpFi814jik4GU9vumIePp8vJf7qWNExDz5/jA3L64O5+ZOpJ9qMai9yRZLV3zPIA384ysp501hSVxnMuRTvPUca50hktnak8Z5IZQUCAZpaemior4w75bW7TDCJ8gy5QbLRR3cDdwAjwHrgPuB/UideblBXWcItaxdYyeYizCaO133gbvmGXhs6XyCe6JZw0UXuJHmxCNfijRShpKq8c/l0njnYzj3PHPHklnnw+WM0NnePeU6RWuaJzNaOtLhPpGfY1NLDN37dSFNLz/luuiguvFD58nHWtmHi4cUolKrqZqxexVFV3Qi8P71ipZ94FHggEKCxuTs4juC+LnQ2cLhw0FhLdoZzwURTMKrR13R2ynJmWntZMtSNV9eJk76juqIo7Mpx4Z5xbUUxG5bXs6XRH9aQhjOOobLEUtTx0lBfyVfev4yG+srzvkP353AGKtKzMuMLhlzFi1EYEhEfcEBEPi0iHwQq0ixX2onUYnXj/LGbms+1JEMVQ+hsYLeCcOp45o32qIObXlNLuOcORFvT2SnLmWntTtHhhVgGKTR9R11lCfWTS6mpsAZyA4HAmPt3P2MRoaG+MjjwG0wjYqcWb2ruiRmVFUtRR3t24b5rn88XHHOoKS8aEwDg/k7DGQBnIN8xcOFkTCfG+BhSjtPairQBq7GMwGwsV9IvgLfEum48tssuu0wTJRAI6N5Tnfpfm1/Xlu6BsOe0dA/o9za/rqc7+3TvqU4dHR3VQCCgLd0DGggEguW4P7v3jY6O6t5TnfqfT+3Xvac6x5wTKotzfqSyAoGAtnQN6Dd/s0+bO/u1uatfW7rOv8Z9frRyE8V5JuGe2d5TnfpnP35W957qDMoS7hm7y3BkbO7s12/8eq++dqIj7H2Fe17uewx9Zs1d/drcaT0j55zQ7zLSfTV3WbLsPXnG0zNzvpeWrnP3GE6udBDt+zAYIgHs0gh6NWpPQayU2Teoaq+qnlDVW1T1w6r6XJptVdoRkeBaB5Fa545rqK6yJNiSDAQCtLlahdFass56CjdEyCWkIRE3bX3D57UunbJaugdoau6if+gsigbXdG5q6YnodorlYw/Fkcf5cYRrgUZzLbndMI4s7l5BuDIcGesml3DtJbPYur8NhLDPYoyMAaW1Z+g8+Zxnds/OI/xg28Fgb8qps6PvbNjxgzEyIQycHWXTy6cjzgwf86xc6107x8ZrwNnMgDakmmiT1wpVdRRYO47yjCuxBgbdriFHOT97uCOoVBxCFYZ7DoLzubV3KOjbd1793dZ6yv7uwfPi650ynX0dvcN8b/NB+kcC+MT62kJj8MMpLfd4QiRF7+DFLRNp/kIgEKCtb5iGemvZUacO5xm29g6FTaOt9vhIa88QDfWVQSNdU17EuoZaNKBj5A1V+E3N3dz1+zfYd6oLf7clRyAQ4OqLZ/CpdywMjnM4bp6q8kl85SrLcPm7B/nelgPsO9lJa885JV5bWcxt6xdx8xXzQc+NKT0YYazBCUJwoqbGMzTVDG4bUk3EkFQReVFVLxWRO4BZwENAn3NcVX85PiJGJp0hqY6yc8I4VZV9p7sgoPh8PhpmTA6GL4aGj4amlwBrofd1DbVs2nOKtYtreOlYJyvnTmXngbbg50vmTWPPsU6ud12zflkdWxv9fPiyWbT3DVNTXhyMInK3SB1F5LQaG5u72dLoZ4N9vZcF593lgbckf06o7jUrZrCl0c+KuVN55mA7N10xD4D23mGqy4vo6D/LliY/19t1O8+3vXeYR/ecZGAkwG3vWAgCKLT3DbPp5VOoKtesnEltRUlwzWl/zyAowWfwg20Hg8bmhtVz2PTyacCaz+E8m3DPaMfBVr7z+Ossrq+gpqJ4jGIPBAI880Y7e46eYX1DLQ/sOsGNq+awbOaUiKHIzr5kU6B4YTx7I4b8I1pIamG4nSGUAO3ABqy2r/23JSGjICJzsMJa6+1y7lTV/xCRjcAngFb71K+q6m8SqSMV+LsH+cHTh7jtHQuDSv7enUc44O/lC+9ZOuaP6ETUbN7XAsCy6ZPP69Zft3oOGlAGzo6y40Aba5fU8uKRDi6aPYWXjp7hkvlVvHT0DBuW1aEBy1Bft2oONRVFVJcXIQjLZ0wJm3TP6QVct2pOcPbyoy+d5JoVM6guK7LKsXsS6xvO9URClYpTnkOo4XAr1eAxhf7hEarKitiwvJ5HX7IUfEffWR544Rj7TnVz05r5HG7t453L64PPw98zyPe3HCAQCLBsxhSOnxmgtWeAe585CkBpUQE3rJ4DCA++cJyyokJuumIeHf1ng+6pppYeltZVcNv6RbT3DPHInpO09w5z01utuSQ15UXsO9XJAy8c58bVc2mYMXnMOhkvHeng42+bz6q50ygoKKCmvIiW7gEEoa13iB9uPcgn1y8ChKZT3bT1DeHvHgw+K7eRCjU66c5wOp51GSYW0YxCnYj8NfAa54yBQzKhDiPA51X1RRGpBHaLyJP2sf+rqt9OouyU0dY3RNOpblp7BxGfgELJJB9zqsrZebCdJdMnj/kzVpVN4pK509i8t9n2UZdQU1EUVLyO8r5t/SIEoaaiiKryIjbva2HD8npqyotZUlcZbPmWFRVyy9oFiAjtfcNsafQHXSuhrXkUHt51go+smh38bI0p+Hh49wmuWz2Htr7hoMKtqVwARO81OATHPexv3G1gHINSVlSIiDC1pIDlMyq5oKacJXXlXLdqNp19Z3np2JngPZ4rGPqHR+nqP0tT80luXrsAQTjg72X2tFLKiidRU1FCe98QJYU+1iyqoaN3mG/8tomvvH8ZqsrXHtnLZ961mLWLaqmrLKFj4Cw/3HaIr75/GctnTMHfPcg9O4/Q2NyDiHD7hsXBe60pL+LSBdXsOXqGN19QQ11lCf4e63yAm66Yx1evXs7Sugr8PYN89Ir5PHOwnaf2tTB4NkBVeRHXrJyJKDz68imuXTmLhumV4+bfN2MJhnQRzX10GmvSWri+qarq11MigMgjwPeANUBvPEYh1e4jd+tZVWlq6aGqdBL3PGO5R6rLi2nrHcIncp776O4dh1FV1iyu4al9LZQVFXLNyplB142jzJ0wTMcwtPUNg8JDuywFrarcveMw16yYQW2ltVbDgy+cc025W4jAmEymTjkfunQmh9r6ecv8aXQMjATr1IBaLqiKYmoriz25OJx76x8eCRoqp173faHw7ceb2H6gjTWLqljfUM+R1r7gPR1o6eWZQ+2Wj7+ymH2nu3jg+WNjegpOT6C6zBqUPtjax0tHO4K9qHVLa+jsH2FxnRUR/d2nDlBdXhR0+4TOTG7pHuCu37/B2sU1LK6z9jmt+8bmbp7a28yl86u44oJqfD7fGAPo9ARae630JoFAgDWLaxCFHQfbeNPsqew92UXv0Fk6B0aYNa2Uj629IKlWu3EJGcaLaO6jmGMKaRZsPrAduAj4a+BmoBvYhdWbOBPmmluBWwHmzp172dGjR1MmT7ixgZauAb7z5OtUlxdx9cXTuffZY1SVTeLaS2YFB1Xdil5VuXvn4WDL0dkPlnJfOW8aOw+0Aed83s7YRXX5JJpaeoIt6nueOZdWw/FTu/3VMNbv7yi1Ay29/PBpV4vZvq/Qur0osNCegogl535/75i0EKpKc1c/B/19EAjwn1sP8bl3LWHNohoam7vZ+L+vMb+2gs+/ewmCcNeON7hmxUxUlXt2HkbExxfeszS4aNG+013886a9/PFb5vHuhloefukUx9r76R8eAeD6VbMR8QUNnGOY3HI7Bnzp9Eqamrt5cNcJblu3CPFJ8Hk4YzjulfYaT3fx6B6r9b+0voLG5m7O9A7zVGMLpUWFrF1SG3T1uXsKy2bEv1pdpHEh4xIypJNExxTS2lQRkQqsOQ9/qard9oD2P2L9tf8RKxPrx0KvU9U7gTvB6imkUqbaimLWL6tj055TVK8pon5yKYpSUihcffF0QDjS2svbrpjPlkY/AFsb/VakkU1NRRHXXjKLpXUVtPUNWxPIdlmteWfc4f0XT7cMiB1W+fAuy8Wz39/LN39juUccw+AT35iB4+tDFIb7vROG+tLRDj65bmHQ9+4e87h6xQxr0DZkhnCkAWZnnMFRug/tsgbMH3v5NNVr5lM/uZTR0VGePdzBWxdUMWNqOc1d/SybUcm0MuvntbSugts3LEQUa7xEHHlBEA629rGoroI2O1VEbWUxDfWVfGrDYvYcPcMTwB1bD/LJdQtZOXsqD+w6zn3PHqe6wuoltPYO8YOtB8d8lwPDo7ze0svFc6ZwpT3WcbC1j/a+IRqmT2bD8nqW1JZTVV4UHGNp6R7goL+PHa/76eg/y6N7TrJ2SS07D7TROzjM4Ihyw+qZLJ1eybSySdSUWwapZnJJwq370MFv4xIyZJpoRuGd6apURCZhGYSfOVFMqtriOv5j4LF01R+KWynWVlhK1mnd+8RHefEkaiutGbsb/+giltZV0N5/lpryImoqikHh7h1v0D8S4IZVc9jaNNZgOH/0usoSqsuLaO+1ImsAbl4zP3i8pryIL1/VwNTiAvY3d3PTW+cFW8Fb7LGH2oriMW4SERmjxGsrirnhzfPO61FUlU1iQ0NdUJGFJqO7Z+cRbl4z38rcGtJadRSXcy/OQLjzjJ493MHf/vJV/umDF7HUHmu58c3z2NLop6ayhLbeIX79SjOvt/SyYu5Ublu3kGsvmcXmfS2sb6jlnz/4JgSrxT1wdpRPrVuIz+fjiguqWVxXQWA0wJfet4zDbX3UTC7hhsvnsmnPKctQKwRGA/QPjXDTW+dRU1lCe98wVWWTaO8bprP/LDteb6V4ko+PrZlPVVkRbb1WttSOedN46ajVGa0uK+I7j+/ngL+Xz797CYtqK9h19Azbm1p425JaqsqK2PTKaRTY39LLY64Ip2Ra9eHmbBgMmSRa6uyOdFQoljb6CdCornWeRWSG67QPYg1wpxWndbjvVJc1X6BnkNpKKzTR8T27P/t8PpbPmEJBQcGYSWG1lcVcvXImZZMKqCqfxHWr59BQX3neIjUigiA8uuckV18847xcQZbPu4R/33yQL/3iFXYdsxSWkxyvob4Sf88gzxxq51/suRL+nkE7Tr+Tli4rHxJ6LtW3k//ojm2HeHDX8bBJ6xzlLoinRXmqyydxxaJqRkdH8XcP8uZ5U/mb9y5lYXWZtQBQ71Bw0hoKm146SUCVz1+5mE+9/QIEYWldBZfMm8bWRj/is1w8VyyqBqCjdzg4+Q+FO7a/wYLqMq5fNQfBmhB37SWzqC4v5u6dhznY2svRjgF8BQX4Cnxs299KQWEBdVNKeenYGa5YXMN7CfGh5QAAGdNJREFUL5zBvlPd3PvsUQIaYOW8abx0pINL5k1j874WXvf3UFwozKkqY3FdJWcGzvLfv3+DN9r6eKrRT3VlMddeMovHXjnNo3tO8v6Lp3P1xTMYOTvCjgOtjI6OJvQbzPd5BrHmxhiyDy8hqalmDfDnwKsissfe91Xgj0VkJZb76AjwF+kWxJkI1Td0FvH5EOS81pqX1puIFS5aWznWjRB2LgBWPH1NpTXfIDRldG1FMX915WLevqSWw619wclR7uiYQCAQdA+19Q4zcHaUB3edCNZRVlTINSssG+so/NJJBVyzYmZQRjdBwxehtersc2RdMXcq39t8kHk15VTbUThH2/q5oKY8WKczae0jq2Zz7SWz2PTyaZbOmAJybv7FHjsU13HFPXuwHUSodhmhfae7ePV4FwA3Xj6XrY3+4NyN9Q21ACyuq2TjBy4M9pzcBuydF05n056x8x0A9hw9wzsvnE5DfWUwCuw9b5rJMwfb8fl8VJcXs6T+3FLkPrHyI1WvKQo+07t3HubEmX5ePNbJtz5yMWsX1Ub9nUxEzDhJ7jGh12h2egrtvVZETqTU0olEhUS6JnR/JH+++33o7GH3ugjutA+O2+TMwAhL68+5uNxhqk4PyOt6CW653QPi+0530d49SM9QgPdeVM+ZwdFgqGpH39mI9U8rLeDxRj/vWVZHx8BIcL8TvuvcmyNjVWkhzx7uYFFN+Zi1DiJd43yf1eVF1gB9yDnhJp45n6vLrAH0JbXl7Pf3Ul1WNGYOR+jkOYDR0VFePN7Fey+sp7AwE22s7MZEVGUn0Qaa41tRJM9wwjjvefZIcEZsOGKti+zGUZ6tPd7SRLg/e8nN71aGjsz1k0vxFfjY2uTnUHs/W5r8tPefpbbCyo300K4TtPcPB1d5c7KX7jvdFXQ5tfZEzxrrpMlGoKCggIKCAr795EH+5XdN/OFoZ9Cd1tF/lm/8ppH9/t5gPqeHXfU/3ujn3363nz8c7UREeHi3VabP5xuz5oMTCvu6v5dXjndxZnCEh3db5bivEZEx9/WDrQf54sOv8J0nX+funYdp6xs+by2JcN9BbUUx+/29bGn089yRM3zzN010DJylfkop4htbh5Ohtr1/mMLCQo6299Pef9a4ScKQ7+6xfGTCN20OtPby7MF2DqzoZfrUsrDnhCY8C4cz+FtdVsTDu62JZB+6dCb7T3czOjJKQUFB1FXPnFnJH7nsXNrmcKksHvjDUebXVvDKsQ7m11SwfmktBQUFAGxYVs/mxhbeubzeNZv3BDesmk1V6aTgbGYRYcPyejbtORWcf3DzmvnBdQ5q7EFxN6HjCg31lXzjwxfR1T/CWxdUBc9bWlfBJ96+AB0NMDo6GpxpXVNhDcpXlRYytayIty6owufzRYy2cVw0VWVWUsKldVYqCmdw37nGkcvpkdywypoFfcOq2dROLg27zkG4lmtr7xCb9zZzyfwq3jJ/GlWuxH6hddx8xXza+4atiYfL6rhuVewUIgZDrjBh3UdOkrNpJYW80T7AWxZYE73CuXHCLWUZ6vbZeaiNH247xFeuaghGJL1wpINv/raJS+ZNZfbUsjGRKqEuC8eYrF9WF4w0aqivpKmlh837LEXfUF/JM2+08x9Pvk5BgfDqyW6uWFjFzCml1piBK0dQa+8Q//XUfl481sWX3ruEzU2twcln7txDsdxKkYjkgkHh+1sP0Hi6h8++azEvH+sMTq4LV24sN5sTBusuI5ycznhHtLrc54Vb9nPf6S4ee/k0N9uhtuGudeaxON9NuJnmplVsyHaSzX2UlzS19PD1R/fRMHMyt69fRMfASFBZOLOKr105k5qKkuA8gtCWZXBmscLv9/v55Duswd+m5m7u2XmU4kL4wpWL8ImPy+ZNpa3H8pE3NndzpLWfvae7Wbu4hh89/QZfet/SYItYAwEeeP441106kwd2HWf1/Co27TlFzZoFvHVBFfquxcyfWsz2gx2sW1JDYWEh7X3DPLXPmqFbU2HF3l+5vJ6BswFQa6D56ounowHF321FWYVTfF5bua09Q9y98zC3rFlA3eSS4Oebr5jPp9YtDPr1l9RZre1ICjs0Tj/crO+PrJpNe88Qm145zS1rFgQHq51rHOPm9Eja+obPk9edrM7dO3Hvd75dIbxSrykvYuW8c6lMasqLuX6VCSc15BcT1ig01Ffyd1c30NU/QnXZJHw+37l013aSt58/f4wbVs8JJppz43an+LsHGRwJsLCmjGcPd/DEq6d47WQnC+sq6B0c5fvbD3D7Oy7gkZdP84EVM7n7mcN0D46wev5UOnor+eK7F3Omd5gdB1p529I6ppVOovFUF7uqitm2v5WTHf1UVRbT2jOAqrLzQBu6sJrX/b1WAj2UQCDABTUV/HDbIaaWFnKm/yxP7mthcHiER14+yQcvmUN1efGYvErJpmToHRxm/+kuAoEA7bYidqKrRISHXzzJ9bbbZb2d6O+unW+wdnEtS+org9lnnefrjHVcPGcKm/acorS4gNvXL0ZEgnMYAhpAEK5bNYfqsknsPNTGjv1++kcC3Lh6LnAu1cfB1j4W11ZQP6U0WLZ7tjrYiQ+3HeSGVXPY0uTnajtqy+lBu7OeovDSkQ7m11bwwPPHAOHGy+cG78P0FAz5QMHGjRszLUPC3HnnnRtvvfXWhK8/0NrHHdsOMX1qKbOnlrDnRCePvHSCkkJhks/H8TP9HGjp4/IF1ZQVFdDY3G1lLLWVXllRgb0uwgAvHO6gb2iYO7Yd4pqLZ/Lu5fW80drH4dZeugeHWVBVzsDZUY6199LeO8ytb1vAvGnl/N/NB5lWVsRdzxxheDTA715rZsXsSmZNK2Pbfj+Np3sZHg1QXlzI0bY+Ahrg5WNn2Huqi/3N3Zw9O8pdO4/wvy+d4HhHH+9ZXs/0ySX82+/2U1hg5fPZd6qb5u4BplcWs/dUD9evms38moqklFff0AhP7Gvhd3tbeN3fy/GOfq69ZBblRZO4Z8dhXjx2hjULa5hWWsgrJ7t55kArhQXC06+38dS+Zvb7e5k9tZQHd53g4tmTOXamn2If9J8d5XevnOKl411Ul02ibnIpi2rKEJ+wet40yosn8fDuEyyfOZmXT3bxgy0HWLWgirbuAXYdOcOSugpWza9i19EzbHxkLwfbelk1r4raymLKinzcu/MI0ydPor3vLNXlRRw/08+DLxxn1tRiqspLWFBVyh1PH6J+cjGq2DmtAjzwwnEumzeVGVNLeWJvM/1DI3QPnmXvqS4unj2FgbMBHnrhOPNryykvnrBtLUOO8LWvfe30xo0b7wx3bML+ev09g2xvauF9F9bz+Kun6OgZ5Cc7DlNZUsjv9jbTNzRCw4xKZvp8BNQaRP6Xx/bxF+sWsqSuMqhQ79l5hNbuAV463snuox2cGRjhiUY/N14+l5IiHwEt4D0XzuDePxxjUa0VVrly9lSmlhXz9Ot+ls+o5LWTnQQCAZbWT+Z0l5//98JJ/N2DlBf7mDmlmFvWzOO1U710Do5w5+8PMzwyypL6yVz1ppncuf0NegZHWFBTykvHuzjROcDtJYv49PqFVNmrnf1o+2EOt/fx0IsnOdFpZX1NxiAEAgFe9/cwc3Ix111mTSKrsVdPA7h6xQzufeYoTzW2sOnlUQ639XHTmvlsbvTT0j3AVRfNoHNgBNUAfUNn2XX4DD/ZedgaC+kZorpiEqVFBVw2r5ofPn2IzoE5/PSZozTMnMyn3nEB6xpqae0a4PFXT/Enl8/jcFsvqy+w3HAP7DrBjW+eyxv+Hm5bv5DjHQPBAIGj7QP8/mA7nQNn6RoYYeMHLmRpfSV/dMks/nPLIfpHAnxm/SJeOdGNyEluWD2bvqGz3PfsUdr7znKmf4TqimLKigq5cnk9j7x0ksbTvXT0DrNs5hSTosKQF0xYo+Asubhtv5+jHf2c6OijUJTOnn4GBs/y3gvr6OgbtVYU6x5EgRtWzeHJvc08afuU333hdD76lrkc8PdSUihcNm8avYOjvHyyiyf3NfPuC6fzVKOfzv5hlkyvQBRe9/eydvF87nrmCENnR7l2xUxOnOnjAytm81RjM3/65nkca+/j6jdN5/evt/EXb1sIwKsnuvnQinp6huqYWlbIs4c7uXbFDOonT+J0xyAH/v/2zjW4qus6wN8S6P2+elrCCElICJEahRAXMCSyC9gmIU4au3bbSUni1HFoi+tOp3XqmZpO/zTJJJ2x69d04tb1JMGPtCmhEzdgIH5gG4wtYd6SAFsiICEJCUkISeju/tj7Hg63V0IIYd0rrW/mzN1nn3P3Wevuc886+7VWew+z+ocZClpDdWNuKsWZyaytKeLRL8yno2+Q7GQ7qD7PtyhrPBxu7eGZHY384ZISKgsyeP7tj1i3rMQbOO88P0QgJZ4vLrwBY6Dr/BBz89NYPDubhrZe3mo4461IFhF2n+igODuZ5JlxfG3JbCry070yFs7OIpAST8kXkunuv4gJGl7a08z5gYs0tPWyqrqA2+YXEBweZn5hOiur8ggkx3PvzTbIjzGGjl7b9XOsrYe/WzOP0oCdZdbZM8CZ5HiOnD5H4kzh/uXl3LOoiNk5KVTkpZGTGk9RVhL1zd18a0UpOakJ5KUl8s0VZeSkxBM0hrh9p8lOifcmIyhKrDNtjUJuWgKfLc3hmZ2N5KclcqS1l64LQwza5wdbD3WQkhBHeV46z75+nOaz55kVSCaQksiq+fm8tPckT2xvYsPKCva1dHP7TcVsO9hKMBjkwtAwInFU5KdTWZhBR88Am/Z8TDBouPfmElbPz6O6KIPuviGef/sE2WmJSFwfZQUZ1Fbm8sHJBEqykvjp7hbODQb58RvH6B8aZiAotJ67wGNfquYby7PBwNG2fvoGhkhPSeL3P1PAmw1n+FRxJvtbulhQlMGWejs4W12UyaHT56hr7qKyMJ389Bnj/u1Czuo+ONFJZnI8AJ19Q+w4bCOvPbuziW/XliMSx8t7mzHGsO1wG9+pLScnLQHjzp9XkM76W+digoaGNutae15hJhIn5KXPYNO7J8jNSKKjd5BPl2Tz7OvHeODzZSTNjOPWykJ+2zXArqZO+ofs1FeJE7Z8eJqc42dZW1PE9oOtzMlLZdPuZv72jkoWleYwNzeVV/a2kJ2awJM7GvnunVWsr51LU/t5lpYF6Dg/RP3HXeSkJdJ4po8X3v6YBbOyKM9L47k3j7GiMo9l5bm09w3yTlMnyYkzOdt/0UaPGxpm/a1zIw7gK0qsMG2NQnvfIAdPdrOgOIvlZQGe2N5IZV4qi0sCiAhbDpymOCuJrKR4gibI7dUFdPUP8aWaIkAoykzk3s/OYmlpgMqCdEzQ8ObRM3yqOMCeEx2IxFnfSBnWRcX6W+d6K6e7LgTZ19xNbVUeD62s5K3Gds8FxdHWHn7w6hH+8csLeOyuBVTmpVKam4IYCKQmcLb/IlWF6Z7//6/fMsebVpqTGk9OeiLz8tMoL0i3UdCGhjGYy+bh57pupfESFxfHLeW5BFIT2Lr/FAuKM6jISwHJt55Hv1hNIDmep3Y2AsKq6kJ2NXV4jgB/Z1YWT+9sBJnLLeW5nOkdoL65i7U1RYDtx//6sjmUFaTz/V8d5m/urGJpaYDslHhyUhLISU30orptWFnhzXDq6B1gc/1vWbuwyFtjsO3Aab79+TK6z1/k6d808WBtObfNL+AX7zezoDiT/Se7yUpNYFdjhzf4HVqv8dVFRXzvnoVU5Nnxl7Pnh3j8tUYCaYnML8zwAgPlpiVATRG/rD+lrQUl5pm26xT8bhuCwSBP/aaJ1fMLqGvp5m4XDzknJYHOvkH+fdcJjrf38dCqSirz0y8LeuNft3Do9DlvQVNovYB/Hv2/vXkcsJ5RQ9HU7gmbpul3Qx1alDZW/HPw/XEa/IFlIrnfHi/GGG99xoO15dR/3OXN/2891+9N660qzKC9bxATNDZGxNISGtv7qPvoLH9w8+zL5vj7PbbmpiZc9lv41yIYY+jsG/IMZEiekdZOvLj7I0rz07zAP/YLNhZ0+Fv+SPGXW8/1/79r+n8LnX2kxArjCrITC0xU5LXRAq6HPwzCXVVHKmekY+EP6Yl+iIzFf9FEXzO0kjvkTjyST6eRFvyNxTfURMh/pfqNtDhRUaYyahQURVEUD3WIpyiKoowJNQqKoiiKhxoFRVEUxUONgqIoiuKhRkFRFEXxUKOgKIqieKhRUBRFUTxiep2CiJwBPrqGInKB9gkSZzKZKnqA6hKtTBVdpooecG26lBhj8iIdiGmjcK2IyHsjLeCIJaaKHqC6RCtTRZepogdcP120+0hRFEXxUKOgKIqieEx3oxAxHF0MMlX0ANUlWpkqukwVPeA66TKtxxQURVGUy5nuLQVFURTFhxoFRVEUxWNaGgURuUNEjohIo4g8MtnyjAUROSEiH4pInYi85/ICIrJVRBrcZ7bLFxF53Om3T0QWTbLsz4lIm4js9+Vdtewiss6d3yAi66JEj40ictLVS52IrPEd+67T44iI3O7Ln/T7T0RuFJEdInJQRA6IyEMuP6bqZRQ9Yq5eRCRJRHaLSL3T5R9cfqmIvOvkelFEElx+ottvdMfnXEnHMWGMmVYbMANoAsqABKAeqJ5sucYg9wkgNyzv+8AjLv0I8D2XXgP8ChBgCfDuJMv+OWARsH+8sgMB4Jj7zHbp7CjQYyPw1xHOrXb3ViJQ6u65GdFy/wE3AItcOh046mSOqXoZRY+Yqxf326a5dDzwrvutXwLuc/nPAN9x6fXAMy59H/DiaDqOVY7p2FK4GWg0xhwzxgwCm4C7Jlmm8XIX8LxLPw982Zf/H8byDpAlIjdMhoAAxpjXgc6w7KuV/XZgqzGm0xhzFtgK3HH9pb/ECHqMxF3AJmPMgDHmONCIvfei4v4zxpwyxrzv0j3AIaCYGKuXUfQYiaitF/fb9rrdeLcZ4DbgFZcfXiehunoF+D0REUbWcUxMR6NQDDT79lsY/SaKFgzwaxHZKyIPuLwCY8wplz4NFLh0LOh4tbJHs05/7rpUngt1txBDerhuh09j30xjtl7C9IAYrBcRmSEidUAb1sA2AV3GmIsR5PJkdse7gRyuUZfpaBRileXGmEXAncCficjn/AeNbTfG5PziWJYdeBooB2qAU8APJ1ecq0NE0oCfA39pjDnnPxZL9RJBj5isF2PMsDGmBpiFfbuv+qRlmI5G4SRwo29/lsuLaowxJ91nG/Bf2BumNdQt5D7b3OmxoOPVyh6VOhljWt0fOQj8K5ea6VGvh4jEYx+kPzHG/KfLjrl6iaRHLNcLgDGmC9gBLMV21c2MIJcnszueCXRwjbpMR6OwB6hwI/oJ2AGazZMs06iISKqIpIfSwGpgP1bu0GyPdcB/u/Rm4E/cjJElQLevSyBauFrZ/xdYLSLZritgtcubVMLGar6CrRewetznZoiUAhXAbqLk/nN9zz8GDhljfuQ7FFP1MpIesVgvIpInIlkunQyswo6R7ADudqeF10moru4GtrvW3Ug6jo1PcnQ9WjbsTIqj2P66RydbnjHIW4adTVAPHAjJjO0/fA1oALYBAXNpFsOTTr8PgcWTLP/PsE34IWz/5v3jkR34JnbQrBH4RpTo8YKTc5/7M97gO/9Rp8cR4M5ouv+A5diuoX1AndvWxFq9jKJHzNULcBPwgZN5P/D3Lr8M+1BvBF4GEl1+kttvdMfLrqTjWDZ1c6EoiqJ4TMfuI0VRFGUE1CgoiqIoHmoUFEVRFA81CoqiKIqHGgVFURTFQ42CEjOISO+Vz5rQ6+2aoHJqRWSLL71sIsp15c0RkT/y7S8Wkccnqnxl+qFGQZm2+FaJRsQYM2EPbx+1wFWVewU55wCeUTDGvGeM2TAuyRQFNQpKjCMi5SLyqnMU+IaIVLn8tc7H/Acisk1EClz+RhF5QUTeAl5w+8+JyE4ROSYiG3xl97rPWnf8FRE5LCI/cStpEZE1Lm+v2HgDW0aRdQ7wIPCwWB//K9wq1p+LyB633TKCnHOcfu+7LWRY/glY4cp7OKxVEhCRX4h1CveOiNzkKzuizoryia+k1E238W5Ab4S814AKl/5d7FJ/sL79Q4szvwX80KU3AnuBZN/+Lqzv+Vys75h4//Wwb/fdWB8yccDb2JW0SVhvlKXuvJ8BWyLIWBvKJ8zPP/BTrLNDgNlYdw2R5EwBkly6AngvvOwI13oCeMylbwPqrqSzbrqN2nxWlGhGrGfMZcDL7sUd7IMO7AP8RecDJwE47vvqZmNMv2//f4wxA8CAiLRh3UW3hF1utzGmxV23Dttt0wscM9ZnPVij8ABXx0qg2id/htMrXM544F9EpAYYBirHUPZy4KsAxpjtIpIjIhnu2Fh0VqYhahSUWCYO62u+JsKxJ4AfGWM2i0gt9u04RF/YuQO+9DCR/xdjOWc8xAFLjDEX/JnOSPjlfBhoBRa671x2/ji4XvooMY6OKSgxi7F+84+LyD3gxRFe6A5ncsld8PWKG3wEKJNLsXHvHcN3erBhI0P8GviL0I5rCUQiEzhlrCvor2HDR0Yqz88bwB+7cmuBdhMWM0FRwlGjoMQSKSLS4tv+CvvQu19EQh5kQyEUN2K7lfYC7ddDGNe1sx541V2nBzv2MBq/BL4SGmgGNgCL3WDwQexAdCSeAtY5Pau41IrYBwyLDfb+cNh3NgKfEZF92AHp62UclSmEeklVlGtARNKMMb1uNtKTQIMx5p8nWy5FGS/aUlCUa+NP3cDzAWwXz7OTLI+iXBPaUlAURVE8tKWgKIqieKhRUBRFUTzUKCiKoigeahQURVEUDzUKiqIoisf/ASEkfoN+/kDxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxxOlfWVwnCV"
      },
      "source": [
        "Finally we test how well the learned greedy policy performs. In this case, we can see that it consistenly lasted 200 iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy44miD2WChz",
        "outputId": "c51aedf3-729b-4ce9-c038-c2a9a293f51d"
      },
      "source": [
        "for i in range(10):\n",
        "    data = list(test_cart_pole(lambda obs: pi[obs2state(obs)], print_iter=False))\n",
        "    iters = len(data)\n",
        "    print(f\"Testing iter {i}, lasted {iters} iterations\")\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing iter 0, lasted 200 iterations\n",
            "Testing iter 1, lasted 200 iterations\n",
            "Testing iter 2, lasted 200 iterations\n",
            "Testing iter 3, lasted 200 iterations\n",
            "Testing iter 4, lasted 200 iterations\n",
            "Testing iter 5, lasted 200 iterations\n",
            "Testing iter 6, lasted 200 iterations\n",
            "Testing iter 7, lasted 200 iterations\n",
            "Testing iter 8, lasted 200 iterations\n",
            "Testing iter 9, lasted 200 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J85yyULOPkT"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBZNztDQOPxN"
      },
      "source": [
        "import pickle\r\n",
        "\r\n",
        "with open(\"model.pickle\", \"wb\") as f:\r\n",
        "    pickle.dump({\r\n",
        "        \"obs2state_fun\": obs2state,\r\n",
        "        \"Q\": Q,\r\n",
        "        \"pi\": pi\r\n",
        "    }, f)\r\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7LSYykNxBPf"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "---\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybFWr-BA-zwf"
      },
      "source": [
        "> (Optional)\r\n",
        "> \r\n",
        "> Explore the OpenAI gyms that are available, and find a discrete environment, and employ your MC control algorithm for it.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1jlxKb-zyn"
      },
      "source": [
        "> The goal of this assignment is to ensure you understand the underlying algorithms of MC control, as distinct from DynProg. Start early and ask questions. The goal is to prepare you for the test: the test will stress the concepts. The assignment will generate working knowledge for these concepts, complementing and reinforcing your theoretical/conceptual knowledge.\r\n",
        "> \r\n",
        "> Submit the code via a file called assignment.py; if you choose to do the optional work, submit it as optional.py.\r\n",
        "> \r\n",
        "> Answer the exercise questions above, and show logs from your code showing how long your solver can keep the pole balanced; the goal is clearly to balance it indefinitely (say for 10,000 steps). This should be submitted as a pdf: assignment.pdf. If you do optional, submit the logs as optional.pdf. For any logs, you must be very clear and explain what you are showing, and what it means. Just submitting a raw log with no comments or explanation is insufficient and can not be properly graded.\r\n",
        "> \r\n",
        "> This assignment can be done within a week, however, I have given you a penalty-free extension as noted. No requests for extensions will be entertained."
      ]
    }
  ]
}