{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HstDan5Doa3z",
        "YSXetAO3obIY",
        "CJD93SyEobKm",
        "TlMXw31xobNL",
        "M6uVosP3obPj",
        "9n_p4i88obRz"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOcb7rsw7yfuqA9n51fUV4I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/E1/E1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcvdkKS8n7Bl"
      },
      "source": [
        "# Exercise I: Introductory Lectures "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YP0hhFQdNbb"
      },
      "source": [
        "\r\n",
        "## A. Read Chapter 3 and do the following exercises: 3.7-3.9, 3.12, 3.18, 3.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HstDan5Doa3z"
      },
      "source": [
        "### 3.7\r\n",
        "\r\n",
        "Noâ€”The agent initially does not have the knowledge of what actions to take to escape the maze. In this situation, the agenent has a constant reward of zero as long as the maze is not escpated. There is no difference learned about different states by the agent. Instead a negative reward needs to be apply when the maze is not escaped to encourage escaping the maze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSXetAO3obIY"
      },
      "source": [
        "### 3.8\r\n",
        "\r\n",
        "Based on equation 3.8:\r\n",
        "\r\n",
        "$$ \\begin{align*}\r\n",
        "G_5 &= 0 \\\\\r\n",
        "G_4 &= R_5 + \\gamma G_5 = 2 \\\\\r\n",
        "G_3 &= R_4 + \\gamma G_4 = 4 \\\\\r\n",
        "G_2 &= R_3 + \\gamma G_3 = 8 \\\\\r\n",
        "G_1 &= R_2 + \\gamma G_2 = 6 \\\\\r\n",
        "G_0 &= R_1 + \\gamma G_1 = 2 \r\n",
        "\\end{align*}$$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJD93SyEobKm"
      },
      "source": [
        "### 3.9\r\n",
        "\r\n",
        "$$ \r\n",
        "\\begin{align*}\r\n",
        "G_1 &\\doteq \\sum_{k=0}^\\infty \\gamma^k R_{1+k+1} \\\\\r\n",
        "&= \\sum_{k=0}^\\infty 0.9^k 2 \\\\\r\n",
        "&= 10\r\n",
        "\\\\\r\n",
        "\\\\\r\n",
        "G_0 &= R_1 + \\gamma G_1 = 2 + 0.9\\times10 = 11 \r\n",
        "\\end{align*}\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlMXw31xobNL"
      },
      "source": [
        "### 3.12\r\n",
        "\r\n",
        "<!-- v_\\pi(q_\\pi,\\pi) \r\n",
        "&= \\mathbb{E}_{(s|q_\\pi,\\pi)}\r\n",
        "\\left[ v_\\pi(s) \\right]\r\n",
        "\\\\ -->\r\n",
        "\r\n",
        "$$ \\begin{align*}\r\n",
        "v_{\\pi}(s) &\\doteq \\mathbb{E}_{\\pi}\r\n",
        "\\left[ G_t | S_t=s \\right]\r\n",
        "\\\\\r\n",
        "&= \\mathbb{E}_{\\pi}\r\n",
        "\\left[ q_{\\pi} (s,a) | S_t=s \\right]\r\n",
        "\\\\\r\n",
        "&= \\sum_{a\\in \\mathcal{A}} \\left[ \\pi(a|s) q_{\\pi} (s,a) \\right]\r\n",
        "\\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6uVosP3obPj"
      },
      "source": [
        "### 3.18\r\n",
        "\r\n",
        "$$ \\begin{align*}\r\n",
        "v_{\\pi}(s) &= \\mathbb{E}_{\\pi}\r\n",
        "\\left[ q_{\\pi}(s,a) | S_t=s \\right]\r\n",
        "\\\\\r\n",
        "&= \\sum_{a\\in \\mathcal{A}} \\left[ \\pi(a|s) q_{\\pi} (s,a) \\right]\r\n",
        "\\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n_p4i88obRz"
      },
      "source": [
        "### 3.19\r\n",
        "\r\n",
        "$$ \\begin{align*}\r\n",
        "q_{\\pi}(s,a) &= \\mathbb{E}_{p(s',r|s,a)}\\left[\r\n",
        "r+v_{\\pi}(s')\r\n",
        "| S_t=s,A_t=a\r\n",
        "\\right]\r\n",
        "\\\\\r\n",
        "&= \\sum_{s'\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{R}}\\left[\r\n",
        "    p(s',r|s,a)(r+v_{\\pi}(s'))\r\n",
        "\\right]\r\n",
        "\\end{align*} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inj1X63C7y5i"
      },
      "source": [
        "## B. Practical\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFFj4xMHdWXQ"
      },
      "source": [
        "### 1. Anatomy\r\n",
        "\r\n",
        "> Read the code and observe how the concepts of the course thus far relate to it. Comment on these correspondences. What does an RL agent solve? What should a RL agent have inside of it? Do you observe these in the program?\r\n",
        "\r\n",
        "The RL agent tries to solve for the state value function for each state. The agent plays against another player (i.e. `player1` vs `player2`); based on the result for each episode (i.e. game), the final result is used to `backup()` to iteratively solve for value for each state. The agent also makes moves either greedily exploit or randomly explore based on `if np.random.rand() < self.epsilon`.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZVRSvXddT2M"
      },
      "source": [
        "### 2. Instrumentation and Play\r\n",
        "\r\n",
        "> Modify the code so it trains against you interactively. Instrument the code to print the value function as it learns, and also when it takes an exploit vs explore action. Observe how the agent takes exploitative actions and explorative actions, and how the value function changes.\r\n",
        "> \r\n",
        "> Let the agent train against you for N games. Comment on how the agent's competence increases as N increases.\r\n",
        "\r\n",
        "With only a handful of games (small N) the agent's overall improvement is minimal, especially the agent is not able to generalize past experienced states to novel states. However, the agent has learned to recognize the same states that have occured before to avoid a loss, or obtain a win in these states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuiVuh5l8C69",
        "outputId": "8a6cfca8-0d70-4743-f041-3b6d0666b04f"
      },
      "source": [
        "#@title\n",
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2016 - 2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)           #\n",
        "# 2016 Jan Hakenberg(jan.hakenberg@gmail.com)                         #\n",
        "# 2016 Tian Jun(tianjun.cpp@gmail.com)                                #\n",
        "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "BOARD_ROWS = 3\n",
        "BOARD_COLS = 3\n",
        "BOARD_SIZE = BOARD_ROWS * BOARD_COLS\n",
        "\n",
        "\n",
        "class State:\n",
        "    def __init__(self):\n",
        "        # the board is represented by an n * n array,\n",
        "        # 1 represents a chessman of the player who moves first,\n",
        "        # -1 represents a chessman of another player\n",
        "        # 0 represents an empty position\n",
        "        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n",
        "        self.winner = None\n",
        "        self.hash_val = None\n",
        "        self.end = None\n",
        "\n",
        "    # compute the hash value for one state, it's unique\n",
        "    def hash(self):\n",
        "        if self.hash_val is None:\n",
        "            self.hash_val = 0\n",
        "            for i in np.nditer(self.data):\n",
        "                self.hash_val = self.hash_val * 3 + i + 1\n",
        "        return self.hash_val\n",
        "\n",
        "    # check whether a player has won the game, or it's a tie\n",
        "    def is_end(self):\n",
        "        if self.end is not None:\n",
        "            return self.end\n",
        "        results = []\n",
        "        # check row\n",
        "        for i in range(BOARD_ROWS):\n",
        "            results.append(np.sum(self.data[i, :]))\n",
        "        # check columns\n",
        "        for i in range(BOARD_COLS):\n",
        "            results.append(np.sum(self.data[:, i]))\n",
        "\n",
        "        # check diagonals\n",
        "        trace = 0\n",
        "        reverse_trace = 0\n",
        "        for i in range(BOARD_ROWS):\n",
        "            trace += self.data[i, i]\n",
        "            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n",
        "        results.append(trace)\n",
        "        results.append(reverse_trace)\n",
        "\n",
        "        for result in results:\n",
        "            if result == 3:\n",
        "                self.winner = 1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "            if result == -3:\n",
        "                self.winner = -1\n",
        "                self.end = True\n",
        "                return self.end\n",
        "\n",
        "        # whether it's a tie\n",
        "        sum_values = np.sum(np.abs(self.data))\n",
        "        if sum_values == BOARD_SIZE:\n",
        "            self.winner = 0\n",
        "            self.end = True\n",
        "            return self.end\n",
        "\n",
        "        # game is still going on\n",
        "        self.end = False\n",
        "        return self.end\n",
        "\n",
        "    # @symbol: 1 or -1\n",
        "    # put chessman symbol in position (i, j)\n",
        "    def next_state(self, i, j, symbol):\n",
        "        new_state = State()\n",
        "        new_state.data = np.copy(self.data)\n",
        "        new_state.data[i, j] = symbol\n",
        "        return new_state\n",
        "\n",
        "    # print the board\n",
        "    def print_state(self):\n",
        "        for i in range(BOARD_ROWS):\n",
        "            print('-------------')\n",
        "            out = '| '\n",
        "            for j in range(BOARD_COLS):\n",
        "                if self.data[i, j] == 1:\n",
        "                    token = '*'\n",
        "                elif self.data[i, j] == -1:\n",
        "                    token = 'x'\n",
        "                else:\n",
        "                    token = '0'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print('-------------')\n",
        "\n",
        "\n",
        "def get_all_states_impl(current_state, current_symbol, all_states):\n",
        "    for i in range(BOARD_ROWS):\n",
        "        for j in range(BOARD_COLS):\n",
        "            if current_state.data[i][j] == 0:\n",
        "                new_state = current_state.next_state(i, j, current_symbol)\n",
        "                new_hash = new_state.hash()\n",
        "                if new_hash not in all_states:\n",
        "                    is_end = new_state.is_end()\n",
        "                    all_states[new_hash] = (new_state, is_end)\n",
        "                    if not is_end:\n",
        "                        get_all_states_impl(new_state, -current_symbol, all_states)\n",
        "\n",
        "\n",
        "def get_all_states():\n",
        "    current_symbol = 1\n",
        "    current_state = State()\n",
        "    all_states = dict()\n",
        "    all_states[current_state.hash()] = (current_state, current_state.is_end())\n",
        "    get_all_states_impl(current_state, current_symbol, all_states)\n",
        "    return all_states\n",
        "\n",
        "\n",
        "# all possible board configurations\n",
        "all_states = get_all_states()\n",
        "\n",
        "\n",
        "class Judger:\n",
        "    # @player1: the player who will move first, its chessman will be 1\n",
        "    # @player2: another player with a chessman -1\n",
        "    def __init__(self, player1, player2):\n",
        "        self.p1 = player1\n",
        "        self.p2 = player2\n",
        "        self.current_player = None\n",
        "        self.p1_symbol = 1\n",
        "        self.p2_symbol = -1\n",
        "        self.p1.set_symbol(self.p1_symbol)\n",
        "        self.p2.set_symbol(self.p2_symbol)\n",
        "        self.current_state = State()\n",
        "\n",
        "    def reset(self):\n",
        "        self.p1.reset()\n",
        "        self.p2.reset()\n",
        "\n",
        "    def alternate(self):\n",
        "        while True:\n",
        "            yield self.p1\n",
        "            yield self.p2\n",
        "\n",
        "    # @print_state: if True, print each board during the game\n",
        "    def play(self, print_state=False):\n",
        "        alternator = self.alternate()\n",
        "        self.reset()\n",
        "        current_state = State()\n",
        "        self.p1.set_state(current_state)\n",
        "        self.p2.set_state(current_state)\n",
        "        if print_state:\n",
        "            current_state.print_state()\n",
        "        while True:\n",
        "            player = next(alternator)\n",
        "            i, j, symbol = player.act()\n",
        "            next_state_hash = current_state.next_state(i, j, symbol).hash()\n",
        "            current_state, is_end = all_states[next_state_hash]\n",
        "            self.p1.set_state(current_state)\n",
        "            self.p2.set_state(current_state)\n",
        "            if print_state:\n",
        "                current_state.print_state()\n",
        "            if is_end:\n",
        "                return current_state.winner\n",
        "\n",
        "\n",
        "# AI player\n",
        "class Player:\n",
        "    # @step_size: the step size to update estimations\n",
        "    # @epsilon: the probability to explore\n",
        "    def __init__(self, step_size=0.1, epsilon=0.1):\n",
        "        self.estimations = dict()\n",
        "        self.step_size = step_size\n",
        "        self.epsilon = epsilon\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "        self.symbol = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.greedy = []\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.states.append(state)\n",
        "        self.greedy.append(True)\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        for hash_val in all_states:\n",
        "            state, is_end = all_states[hash_val]\n",
        "            if is_end:\n",
        "                if state.winner == self.symbol:\n",
        "                    self.estimations[hash_val] = 1.0\n",
        "                elif state.winner == 0:\n",
        "                    # we need to distinguish between a tie and a lose\n",
        "                    self.estimations[hash_val] = 0.5\n",
        "                else:\n",
        "                    self.estimations[hash_val] = 0\n",
        "            else:\n",
        "                self.estimations[hash_val] = 0.5\n",
        "\n",
        "    # update value estimation\n",
        "    def backup(self):\n",
        "        states = [state.hash() for state in self.states]\n",
        "\n",
        "        for i in reversed(range(len(states) - 1)):\n",
        "            state = states[i]\n",
        "            td_error = self.greedy[i] * (\n",
        "                self.estimations[states[i + 1]] - self.estimations[state]\n",
        "            )\n",
        "            self.estimations[state] += self.step_size * td_error\n",
        "\n",
        "    # choose an action based on the state\n",
        "    def act(self):\n",
        "        state = self.states[-1]\n",
        "        next_states = []\n",
        "        next_positions = []\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                if state.data[i, j] == 0:\n",
        "                    next_positions.append([i, j])\n",
        "                    next_states.append(state.next_state(\n",
        "                        i, j, self.symbol).hash())\n",
        "\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            action = next_positions[np.random.randint(len(next_positions))]\n",
        "            action.append(self.symbol)\n",
        "            self.greedy[-1] = False\n",
        "            return action\n",
        "\n",
        "        values = []\n",
        "        for hash_val, pos in zip(next_states, next_positions):\n",
        "            values.append((self.estimations[hash_val], pos))\n",
        "        # to select one of the actions of equal value at random due to Python's sort is stable\n",
        "        np.random.shuffle(values)\n",
        "        values.sort(key=lambda x: x[0], reverse=True)\n",
        "        action = values[0][1]\n",
        "        action.append(self.symbol)\n",
        "        return action\n",
        "\n",
        "    def save_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n",
        "            pickle.dump(self.estimations, f)\n",
        "\n",
        "    def load_policy(self):\n",
        "        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n",
        "            self.estimations = pickle.load(f)\n",
        "\n",
        "\n",
        "# human interface\n",
        "# input a number to put a chessman\n",
        "# | q | w | e |\n",
        "# | a | s | d |\n",
        "# | z | x | c |\n",
        "class HumanPlayer:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.symbol = None\n",
        "        self.keys = ['q', 'w', 'e', 'a', 's', 'd', 'z', 'x', 'c']\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def set_state(self, state):\n",
        "        self.state = state\n",
        "\n",
        "    def set_symbol(self, symbol):\n",
        "        self.symbol = symbol\n",
        "\n",
        "    def act(self):\n",
        "        self.state.print_state()\n",
        "        key = input(\"Input your position:\")\n",
        "        data = self.keys.index(key)\n",
        "        i = data // BOARD_COLS\n",
        "        j = data % BOARD_COLS\n",
        "        return i, j, self.symbol\n",
        "\n",
        "\n",
        "def train(epochs, print_every_n=500):\n",
        "    player1 = Player(epsilon=0.01)\n",
        "    player2 = Player(epsilon=0.01)\n",
        "    judger = Judger(player1, player2)\n",
        "    player1_win = 0.0\n",
        "    player2_win = 0.0\n",
        "    for i in range(1, epochs + 1):\n",
        "        winner = judger.play(print_state=False)\n",
        "        if winner == 1:\n",
        "            player1_win += 1\n",
        "        if winner == -1:\n",
        "            player2_win += 1\n",
        "        if i % print_every_n == 0:\n",
        "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win / i, player2_win / i))\n",
        "        player1.backup()\n",
        "        player2.backup()\n",
        "        judger.reset()\n",
        "    player1.save_policy()\n",
        "    player2.save_policy()\n",
        "\n",
        "\n",
        "def compete(turns):\n",
        "    player1 = Player(epsilon=0)\n",
        "    player2 = Player(epsilon=0)\n",
        "    judger = Judger(player1, player2)\n",
        "    player1.load_policy()\n",
        "    player2.load_policy()\n",
        "    player1_win = 0.0\n",
        "    player2_win = 0.0\n",
        "    for _ in range(turns):\n",
        "        winner = judger.play()\n",
        "        if winner == 1:\n",
        "            player1_win += 1\n",
        "        if winner == -1:\n",
        "            player2_win += 1\n",
        "        judger.reset()\n",
        "    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win / turns, player2_win / turns))\n",
        "\n",
        "\n",
        "# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n",
        "# So we test whether the AI can guarantee at least a tie if it goes second.\n",
        "def play():\n",
        "    while True:\n",
        "        player1 = HumanPlayer()\n",
        "        player2 = Player(epsilon=0)\n",
        "        judger = Judger(player1, player2)\n",
        "        player2.load_policy()\n",
        "        winner = judger.play()\n",
        "        if winner == player2.symbol:\n",
        "            print(\"You lose!\")\n",
        "        elif winner == player1.symbol:\n",
        "            print(\"You win!\")\n",
        "        else:\n",
        "            print(\"It is a tie!\")\n",
        "\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path(\"policy_first.bin\").exists() or not Path(\"policy_second.bin\").exists():\n",
        "    train(int(1e5))\n",
        "compete(int(1e3))\n",
        "# play()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000 turns, player 1 win 0.00, player 2 win 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5RUlBL88pvr",
        "outputId": "b7e8f4d9-5889-4344-92fb-cf599b526641"
      },
      "source": [
        "#%% Agent training against human\r\n",
        "def train_against_human(epochs, print_every_n=1):\r\n",
        "    player1 = HumanPlayer()\r\n",
        "    player2 = Player(epsilon=0.01)\r\n",
        "    # Continue from last training\r\n",
        "    if Path(\"policy_human_train.bin\").exists():\r\n",
        "        with open(\"policy_human_train.bin\", 'rb') as f:\r\n",
        "            player2.estimations = pickle.load(f)\r\n",
        "    judger = Judger(player1, player2)\r\n",
        "    player1_win = 0.0\r\n",
        "    player2_win = 0.0\r\n",
        "    for i in range(1, epochs + 1):\r\n",
        "        winner = judger.play(print_state=False)\r\n",
        "        if winner == 1:\r\n",
        "            player1_win += 1\r\n",
        "        if winner == -1:\r\n",
        "            player2_win += 1\r\n",
        "        if i % print_every_n == 0:\r\n",
        "            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win / i, player2_win / i))\r\n",
        "        # player1.backup()\r\n",
        "        player2.backup()\r\n",
        "        judger.reset()\r\n",
        "        # Save training every epoch\r\n",
        "        with open('policy_human_train.bin', 'wb') as f:\r\n",
        "            pickle.dump(player2.estimations, f)\r\n",
        "\r\n",
        "train(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Input your position:d\n",
            "Epoch 1, player 1 winrate: 0.00, player 2 winrate: 1.00\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Input your position:x\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "Input your position:a\n",
            "-------------\n",
            "| x | 0 | * | \n",
            "-------------\n",
            "| * | * | x | \n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "Input your position:w\n",
            "Epoch 2, player 1 winrate: 0.50, player 2 winrate: 0.50\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:e\n",
            "-------------\n",
            "| 0 | 0 | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Input your position:x\n",
            "-------------\n",
            "| 0 | x | * | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "Input your position:a\n",
            "-------------\n",
            "| 0 | x | * | \n",
            "-------------\n",
            "| * | * | x | \n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "Input your position:q\n",
            "Epoch 3, player 1 winrate: 0.33, player 2 winrate: 0.33\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:q\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| x | 0 | x | \n",
            "-------------\n",
            "Input your position:x\n",
            "-------------\n",
            "| * | 0 | 0 | \n",
            "-------------\n",
            "| x | * | 0 | \n",
            "-------------\n",
            "| x | * | x | \n",
            "-------------\n",
            "Input your position:w\n",
            "Epoch 4, player 1 winrate: 0.50, player 2 winrate: 0.25\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "Input your position:s\n",
            "-------------\n",
            "| 0 | 0 | 0 | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:q\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| 0 | * | 0 | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:d\n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| 0 | 0 | x | \n",
            "-------------\n",
            "Input your position:z\n",
            "-------------\n",
            "| * | x | x | \n",
            "-------------\n",
            "| x | * | * | \n",
            "-------------\n",
            "| * | 0 | x | \n",
            "-------------\n",
            "Input your position:x\n",
            "Epoch 5, player 1 winrate: 0.40, player 2 winrate: 0.20\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}