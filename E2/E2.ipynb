{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GymRendering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/constructor-s/aps1080_winter_2021/blob/main/E2/E2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1jMMu5wpPP8"
      },
      "source": [
        "# Exercise II: Cart Pole preliminaries and Monte Carlo\r\n",
        "\r\n",
        "Open Files/A2_... and study it in Colab by running it.\r\n",
        "\r\n",
        "Observe how all the facets of a reinforcement learning coupled machine/environment system are present.\r\n",
        "\r\n",
        "The notebook includes some code to show how the behaviour of the agent can be rendered, using a random policy that exploits the .sample() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZnOglQppupE"
      },
      "source": [
        "## A2_CartPoleWithRendering.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XDvssQd64Pf"
      },
      "source": [
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5esgX013vPe"
      },
      "source": [
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbi2xaFo31Sj"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGqXqJxoAsHG"
      },
      "source": [
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(400, 300))\n",
        "# display.start()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L4YayzR4FYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ddbf7e-b3d0-445e-f315-0a4bff2d92d3"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "# prev_screen = env.render(mode='rgb_array')\n",
        "# plt.imshow(prev_screen)\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\n",
        "#   screen = env.render(mode='rgb_array')\n",
        "  \n",
        "#   plt.imshow(screen)\n",
        "#   ipythondisplay.clear_output(wait=True)\n",
        "#   ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done:\n",
        "    break\n",
        "    \n",
        "# ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "print(\"Iterations that were run:\",i)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step i 0 action= 0\n",
            "obs= [-0.01269733 -0.16805774  0.01115469  0.34068777] reward= 1.0 done= False info= {}\n",
            "step i 1 action= 1\n",
            "obs= [-0.01605849  0.02690374  0.01796844  0.05154316] reward= 1.0 done= False info= {}\n",
            "step i 2 action= 1\n",
            "obs= [-0.01552041  0.2217635   0.01899931 -0.23541685] reward= 1.0 done= False info= {}\n",
            "step i 3 action= 1\n",
            "obs= [-0.01108514  0.41660892  0.01429097 -0.52204677] reward= 1.0 done= False info= {}\n",
            "step i 4 action= 1\n",
            "obs= [-0.00275296  0.61152683  0.00385003 -0.81019239] reward= 1.0 done= False info= {}\n",
            "step i 5 action= 0\n",
            "obs= [ 0.00947757  0.41635234 -0.01235381 -0.51630092] reward= 1.0 done= False info= {}\n",
            "step i 6 action= 1\n",
            "obs= [ 0.01780462  0.61164605 -0.02267983 -0.81285106] reward= 1.0 done= False info= {}\n",
            "step i 7 action= 1\n",
            "obs= [ 0.03003754  0.80707118 -0.03893685 -1.11258066] reward= 1.0 done= False info= {}\n",
            "step i 8 action= 1\n",
            "obs= [ 0.04617896  1.00268228 -0.06118847 -1.41721946] reward= 1.0 done= False info= {}\n",
            "step i 9 action= 1\n",
            "obs= [ 0.06623261  1.19850617 -0.08953286 -1.72838378] reward= 1.0 done= False info= {}\n",
            "step i 10 action= 0\n",
            "obs= [ 0.09020273  1.00451429 -0.12410053 -1.46484899] reward= 1.0 done= False info= {}\n",
            "step i 11 action= 0\n",
            "obs= [ 0.11029302  0.81111161 -0.15339751 -1.213368  ] reward= 1.0 done= False info= {}\n",
            "step i 12 action= 1\n",
            "obs= [ 0.12651525  1.0078436  -0.17766487 -1.54992307] reward= 1.0 done= False info= {}\n",
            "step i 13 action= 1\n",
            "obs= [ 0.14667212  1.20459639 -0.20866333 -1.89236575] reward= 1.0 done= False info= {}\n",
            "step i 14 action= 1\n",
            "obs= [ 0.17076405  1.40128346 -0.24651065 -2.24189956] reward= 1.0 done= True info= {}\n",
            "Iterations that were run: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h756zQwp4Ps"
      },
      "source": [
        "## Exercise 1:\r\n",
        "\r\n",
        "> Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcJzjV6CqgyR"
      },
      "source": [
        "No. Designing a dynamic programming based policy requires explicit knowledge of the environment transition probabilities $p(s',r|s,a)$. For the `CartPole` environment, the behaviors are dicated by laws of physics that can be easily simulated but is extremely difficult to translate into the form of $p(s',r|s,a)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF-gbG2BrWLN"
      },
      "source": [
        "## Exercise 2:\r\n",
        "\r\n",
        "> Can you design a Monte Carlo based policy for the agent? What ingredients do you require? Explain the design flow, and execute it. Show that it works, or indicate why you can't proceed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rzxo9o_raHV"
      },
      "source": [
        "Yes. To design a Monte Carlo based policy we require the ability to generate and record episodes of experience in this environment. Below is an implementation of *on-policy first-visit MC control for $\\epsilon$-soft policies* where the agent has learned to maintain balance for more than 100 iterations. One challenge is that the state observation from the environment is expressed as four continous variables (0 Cart Position, 1 Cart Velocity, 2 Pole Angle, 3 Pole Angular Velocity); for simplicity we will for now discretize this into a finite number of states."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNub0pMosEAS"
      },
      "source": [
        "from collections import defaultdict\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "Observation:\r\n",
        "    Type: Box(4)\r\n",
        "    Num     Observation               Min                     Max\r\n",
        "    0       Cart Position             -4.8                    4.8\r\n",
        "    1       Cart Velocity             -Inf                    Inf\r\n",
        "    2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\r\n",
        "    3       Pole Angular Velocity     -Inf                    Inf\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def obs2state(obs):\r\n",
        "    \"\"\"\r\n",
        "    Discretize observations to states\r\n",
        "    \"\"\"\r\n",
        "    digitized = [np.digitize(obs[i], bins) for i, bins in enumerate([\r\n",
        "        np.linspace(-4.8, +4.8, 9)[1:-1],\r\n",
        "        np.linspace(-0.6, +0.6, 7),\r\n",
        "        np.linspace(-0.418, +0.418, 9)[1:-1],\r\n",
        "        np.linspace(-0.6, +0.6, 7),\r\n",
        "        ])]\r\n",
        "    return ((obs > 0) * 8**np.arange(len(obs))).sum()\r\n",
        "obs2state.S = 8**len(obs)\r\n",
        "\r\n",
        "def init_random_policy(eps, S, A, rng):\r\n",
        "    pi_random_init = rng.random([S, A])\r\n",
        "    pi_random_mask = np.zeros_like(pi_random_init, dtype=np.float)\r\n",
        "    pi_random_mask[np.arange(S), np.argmax(pi_random_init, axis=1)] = True\r\n",
        "    pi = pi_random_mask * (1 - eps) + eps / A\r\n",
        "    return pi"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-gEsMFOQ-eW",
        "outputId": "35950978-73ab-4968-e671-b9a908c3bc3b"
      },
      "source": [
        "#%% On-policy first-visit MC control\r\n",
        "eps=0.05\r\n",
        "gamma=0.99\r\n",
        "env=env\r\n",
        "rng=np.random.RandomState(0)\r\n",
        "# Initialize\r\n",
        "A = env.action_space.n\r\n",
        "actions = np.arange(A)\r\n",
        "S = obs2state.S\r\n",
        "states = np.arange(S)\r\n",
        "\r\n",
        "# pi <- an arbitrary epsilon soft policy\r\n",
        "pi = init_random_policy(eps, S, A, rng)\r\n",
        "# Arbitrary Q initialization\r\n",
        "Q = rng.random([S, A])\r\n",
        "# Empty initialization for returns\r\n",
        "Returns = defaultdict(lambda : defaultdict(list))\r\n",
        "\r\n",
        "iterations_lasted = []\r\n",
        "# Repeat forever for each episode\r\n",
        "for episode in range(1000):\r\n",
        "    # Generate an episode following pi\r\n",
        "    done = False\r\n",
        "    obs = env.reset()\r\n",
        "    R = []\r\n",
        "    SA = [] # S, A pairs history\r\n",
        "    for i in range(50000):\r\n",
        "        state = obs2state(obs)\r\n",
        "        action = rng.choice(np.arange(A), p=pi[state])\r\n",
        "        SA.append((state, action))\r\n",
        "\r\n",
        "        obs, reward, done, info = env.step(action)\r\n",
        "        R.append(reward)\r\n",
        "\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "    \r\n",
        "    iterations_lasted.append(i)\r\n",
        "\r\n",
        "    G = 0\r\n",
        "    # Loop backward for each step of episode\r\n",
        "    # i is where the loop above finished, i.e. T\r\n",
        "    for t in range(i, -1, -1):\r\n",
        "        G = gamma * G + R[t]\r\n",
        "        St, At = SA[t]\r\n",
        "        if not (St, At) in SA[0:t]:\r\n",
        "            Returns[St][At].append(G)\r\n",
        "            Q[St, At] = np.mean(Returns[St][At])\r\n",
        "            A_star = np.argmax(Q[St])\r\n",
        "            for a in range(A):\r\n",
        "                if a == A_star:\r\n",
        "                    pi[St, a] = 1 - eps + eps / A\r\n",
        "                else:\r\n",
        "                    pi[St, a] = eps / A\r\n",
        "    print(\".\", end=\"\")\r\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "Dff1kjqYbLbs",
        "outputId": "c4b03d0a-55da-4ee1-9be8-5c88a2c63314"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(11, 8.5))\r\n",
        "ax.plot(iterations_lasted, '.', markersize=1)\r\n",
        "ax.set(xlabel=\"Episode number\", ylabel=\"Iterations lasted\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Text(0, 0.5, 'Iterations lasted'), Text(0.5, 0, 'Episode number')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 792x612 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWaCBRsos3cT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c4defc-814e-4478-fd22-f78b71ef629d"
      },
      "source": [
        "# Convert epsilon-soft policy to a greedy policy\r\n",
        "pi_greedy = np.zeros_like(pi, dtype=np.float)\r\n",
        "pi_greedy[np.arange(S), np.argmax(pi, axis=1)] = 1\r\n",
        "\r\n",
        "# Test\r\n",
        "done = False\r\n",
        "obs = env.reset()\r\n",
        "for i in range(50000):\r\n",
        "    state = obs2state(obs)\r\n",
        "    action = rng.choice(np.arange(A), p=pi[state])\r\n",
        "\r\n",
        "    print(\"step i\",i,\"action=\",action)\r\n",
        "    obs, reward, done, info = env.step(action)\r\n",
        "    print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"info=\",info)\r\n",
        "\r\n",
        "    if done:\r\n",
        "        break"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step i 0 action= 1\n",
            "obs= [-0.03863433  0.19923228  0.01313335 -0.26377668] reward= 1.0 done= False info= {}\n",
            "step i 1 action= 0\n",
            "obs= [-0.03464969  0.00392536  0.00785782  0.03301954] reward= 1.0 done= False info= {}\n",
            "step i 2 action= 1\n",
            "obs= [-0.03457118  0.19893375  0.00851821 -0.25717384] reward= 1.0 done= False info= {}\n",
            "step i 3 action= 0\n",
            "obs= [-0.0305925   0.00369123  0.00337473  0.03818364] reward= 1.0 done= False info= {}\n",
            "step i 4 action= 1\n",
            "obs= [-0.03051868  0.19876462  0.00413841 -0.25343261] reward= 1.0 done= False info= {}\n",
            "step i 5 action= 0\n",
            "obs= [-0.02654339  0.00358383 -0.00093025  0.04055276] reward= 1.0 done= False info= {}\n",
            "step i 6 action= 0\n",
            "obs= [-2.64717106e-02 -1.91524774e-01 -1.19190111e-04  3.32942043e-01] reward= 1.0 done= False info= {}\n",
            "step i 7 action= 0\n",
            "obs= [-0.03030221 -0.38664503  0.00653965  0.62558738] reward= 1.0 done= False info= {}\n",
            "step i 8 action= 1\n",
            "obs= [-0.03803511 -0.19161498  0.0190514   0.3349712 ] reward= 1.0 done= False info= {}\n",
            "step i 9 action= 1\n",
            "obs= [-0.04186741  0.00323073  0.02575082  0.04835646] reward= 1.0 done= False info= {}\n",
            "step i 10 action= 1\n",
            "obs= [-0.04180279  0.19797415  0.02671795 -0.23609193] reward= 1.0 done= False info= {}\n",
            "step i 11 action= 0\n",
            "obs= [-0.03784331  0.00248087  0.02199611  0.06489748] reward= 1.0 done= False info= {}\n",
            "step i 12 action= 1\n",
            "obs= [-0.03779369  0.19728066  0.02329406 -0.22076518] reward= 1.0 done= False info= {}\n",
            "step i 13 action= 0\n",
            "obs= [-0.03384808  0.00183362  0.01887876  0.07917367] reward= 1.0 done= False info= {}\n",
            "step i 14 action= 1\n",
            "obs= [-0.03381141  0.19667993  0.02046223 -0.20749368] reward= 1.0 done= False info= {}\n",
            "step i 15 action= 0\n",
            "obs= [-0.02987781  0.00127144  0.01631236  0.09157316] reward= 1.0 done= False info= {}\n",
            "step i 16 action= 1\n",
            "obs= [-0.02985238  0.19615583  0.01814382 -0.19591891] reward= 1.0 done= False info= {}\n",
            "step i 17 action= 0\n",
            "obs= [-0.02592926  0.00077911  0.01422544  0.10243192] reward= 1.0 done= False info= {}\n",
            "step i 18 action= 1\n",
            "obs= [-0.02591368  0.19569434  0.01627408 -0.18572919] reward= 1.0 done= False info= {}\n",
            "step i 19 action= 0\n",
            "obs= [-0.02199979  0.00034337  0.0125595   0.11204283] reward= 1.0 done= False info= {}\n",
            "step i 20 action= 1\n",
            "obs= [-0.02199293  0.19528312  0.01480036 -0.17665134] reward= 1.0 done= False info= {}\n",
            "step i 21 action= 0\n",
            "obs= [-1.80872628e-02 -4.74777532e-05  1.12673283e-02  1.20663612e-01] reward= 1.0 done= False info= {}\n",
            "step i 22 action= 1\n",
            "obs= [-0.01808821  0.19491125  0.0136806  -0.16844339] reward= 1.0 done= False info= {}\n",
            "step i 23 action= 0\n",
            "obs= [-0.01418999 -0.00040383  0.01031173  0.12852378] reward= 1.0 done= False info= {}\n",
            "step i 24 action= 1\n",
            "obs= [-0.01419806  0.1945689   0.01288221 -0.16088817] reward= 1.0 done= False info= {}\n",
            "step i 25 action= 0\n",
            "obs= [-0.01030669 -0.00073509  0.00966445  0.13583077] reward= 1.0 done= False info= {}\n",
            "step i 26 action= 1\n",
            "obs= [-0.01032139  0.19424711  0.01238106 -0.15378757] reward= 1.0 done= False info= {}\n",
            "step i 27 action= 0\n",
            "obs= [-0.00643645 -0.00104991  0.00930531  0.14277544] reward= 1.0 done= False info= {}\n",
            "step i 28 action= 1\n",
            "obs= [-0.00645744  0.19393755  0.01216082 -0.14695736] reward= 1.0 done= False info= {}\n",
            "step i 29 action= 0\n",
            "obs= [-0.00257869 -0.00135642  0.00922167  0.14953713] reward= 1.0 done= False info= {}\n",
            "step i 30 action= 1\n",
            "obs= [-0.00260582  0.19363228  0.01221241 -0.14022234] reward= 1.0 done= False info= {}\n",
            "step i 31 action= 0\n",
            "obs= [ 0.00126682 -0.00166244  0.00940797  0.15628824] reward= 1.0 done= False info= {}\n",
            "step i 32 action= 1\n",
            "obs= [ 0.00123358  0.19332356  0.01253373 -0.13341191] reward= 1.0 done= False info= {}\n",
            "step i 33 action= 0\n",
            "obs= [ 0.00510005 -0.00197566  0.00986549  0.16319873] reward= 1.0 done= False info= {}\n",
            "step i 34 action= 1\n",
            "obs= [ 0.00506053  0.19300368  0.01312947 -0.12635564] reward= 1.0 done= False info= {}\n",
            "step i 35 action= 0\n",
            "obs= [ 0.00892061 -0.00230388  0.01060235  0.1704404 ] reward= 1.0 done= False info= {}\n",
            "step i 36 action= 1\n",
            "obs= [ 0.00887453  0.19266473  0.01401116 -0.11887904] reward= 1.0 done= False info= {}\n",
            "step i 37 action= 0\n",
            "obs= [ 0.01272782 -0.00265514  0.01163358  0.17819115] reward= 1.0 done= False info= {}\n",
            "step i 38 action= 1\n",
            "obs= [ 0.01267472  0.19229842  0.0151974  -0.1107992 ] reward= 1.0 done= False info= {}\n",
            "step i 39 action= 0\n",
            "obs= [ 0.01652069 -0.00303797  0.01298142  0.18663941] reward= 1.0 done= False info= {}\n",
            "step i 40 action= 1\n",
            "obs= [ 0.01645993  0.19189587  0.01671421 -0.10192027] reward= 1.0 done= False info= {}\n",
            "step i 41 action= 0\n",
            "obs= [ 0.02029785 -0.00346158  0.0146758   0.19598872] reward= 1.0 done= False info= {}\n",
            "step i 42 action= 1\n",
            "obs= [ 0.02022862  0.1914474   0.01859558 -0.09202875] reward= 1.0 done= False info= {}\n",
            "step i 43 action= 0\n",
            "obs= [ 0.02405756 -0.00393608  0.016755    0.20646259] reward= 1.0 done= False info= {}\n",
            "step i 44 action= 1\n",
            "obs= [ 0.02397884  0.19094232  0.02088426 -0.08088824] reward= 1.0 done= False info= {}\n",
            "step i 45 action= 0\n",
            "obs= [ 0.02779769 -0.00447268  0.01926649  0.21830987] reward= 1.0 done= False info= {}\n",
            "step i 46 action= 1\n",
            "obs= [ 0.02770824  0.19036863  0.02363269 -0.06823386] reward= 1.0 done= False info= {}\n",
            "step i 47 action= 0\n",
            "obs= [ 0.03151561 -0.00508403  0.02226801  0.23181063] reward= 1.0 done= False info= {}\n",
            "step i 48 action= 1\n",
            "obs= [ 0.03141393  0.18971277  0.02690422 -0.05376588] reward= 1.0 done= False info= {}\n",
            "step i 49 action= 0\n",
            "obs= [ 0.03520818 -0.00578441  0.02582891  0.24728266] reward= 1.0 done= False info= {}\n",
            "step i 50 action= 1\n",
            "obs= [ 0.0350925   0.18895932  0.03077456 -0.03714264] reward= 1.0 done= False info= {}\n",
            "step i 51 action= 0\n",
            "obs= [ 0.03887168 -0.00659012  0.03003171  0.26508891] reward= 1.0 done= False info= {}\n",
            "step i 52 action= 1\n",
            "obs= [ 0.03873988  0.18809061  0.03533348 -0.0179725 ] reward= 1.0 done= False info= {}\n",
            "step i 53 action= 0\n",
            "obs= [ 0.04250169 -0.00751978  0.03497403  0.28564583] reward= 1.0 done= False info= {}\n",
            "step i 54 action= 1\n",
            "obs= [0.0423513  0.18708637 0.04068695 0.00419537] reward= 1.0 done= False info= {}\n",
            "step i 55 action= 1\n",
            "obs= [ 0.04609302  0.38160191  0.04077086 -0.2753778 ] reward= 1.0 done= False info= {}\n",
            "step i 56 action= 0\n",
            "obs= [0.05372506 0.18592269 0.0352633  0.02988042] reward= 1.0 done= False info= {}\n",
            "step i 57 action= 1\n",
            "obs= [ 0.05744352  0.38052167  0.03586091 -0.25147131] reward= 1.0 done= False info= {}\n",
            "step i 58 action= 0\n",
            "obs= [0.06505395 0.18490647 0.03083148 0.05230368] reward= 1.0 done= False info= {}\n",
            "step i 59 action= 1\n",
            "obs= [ 0.06875208  0.37957308  0.03187756 -0.23049444] reward= 1.0 done= False info= {}\n",
            "step i 60 action= 0\n",
            "obs= [0.07634354 0.18401046 0.02726767 0.07207088] reward= 1.0 done= False info= {}\n",
            "step i 61 action= 1\n",
            "obs= [ 0.08002375  0.37873109  0.02870909 -0.21188579] reward= 1.0 done= False info= {}\n",
            "step i 62 action= 0\n",
            "obs= [0.08759837 0.18321068 0.02447137 0.08971328] reward= 1.0 done= False info= {}\n",
            "step i 63 action= 1\n",
            "obs= [ 0.09126258  0.37797348  0.02626564 -0.19514958] reward= 1.0 done= False info= {}\n",
            "step i 64 action= 0\n",
            "obs= [0.09882205 0.18248585 0.02236264 0.10570193] reward= 1.0 done= False info= {}\n",
            "step i 65 action= 1\n",
            "obs= [ 0.10247177  0.37728031  0.02447668 -0.17984264] reward= 1.0 done= False info= {}\n",
            "step i 66 action= 0\n",
            "obs= [0.11001738 0.18181681 0.02087983 0.12046022] reward= 1.0 done= False info= {}\n",
            "step i 67 action= 1\n",
            "obs= [ 0.11365371  0.37663348  0.02328904 -0.16556287] reward= 1.0 done= False info= {}\n",
            "step i 68 action= 0\n",
            "obs= [0.12118638 0.18118603 0.01997778 0.13437516] reward= 1.0 done= False info= {}\n",
            "step i 69 action= 1\n",
            "obs= [ 0.1248101   0.37601621  0.02266528 -0.15193871] reward= 1.0 done= False info= {}\n",
            "step i 70 action= 0\n",
            "obs= [0.13233043 0.18057717 0.01962651 0.14780757] reward= 1.0 done= False info= {}\n",
            "step i 71 action= 1\n",
            "obs= [ 0.13594197  0.37541265  0.02258266 -0.13861954] reward= 1.0 done= False info= {}\n",
            "step i 72 action= 0\n",
            "obs= [0.14345022 0.17997466 0.01981027 0.16110144] reward= 1.0 done= False info= {}\n",
            "step i 73 action= 1\n",
            "obs= [ 0.14704972  0.37480748  0.0230323  -0.12526661] reward= 1.0 done= False info= {}\n",
            "step i 74 action= 0\n",
            "obs= [0.15454587 0.17936327 0.02052696 0.17459285] reward= 1.0 done= False info= {}\n",
            "step i 75 action= 1\n",
            "obs= [ 0.15813313  0.37418552  0.02401882 -0.11154446] reward= 1.0 done= False info= {}\n",
            "step i 76 action= 0\n",
            "obs= [0.16561684 0.17872778 0.02178793 0.18861844] reward= 1.0 done= False info= {}\n",
            "step i 77 action= 1\n",
            "obs= [ 0.1691914   0.37353135  0.0255603  -0.09711242] reward= 1.0 done= False info= {}\n",
            "step i 78 action= 0\n",
            "obs= [0.17666203 0.17805256 0.02361805 0.20352389] reward= 1.0 done= False info= {}\n",
            "step i 79 action= 1\n",
            "obs= [ 0.18022308  0.37282893  0.02768853 -0.08161611] reward= 1.0 done= False info= {}\n",
            "step i 80 action= 0\n",
            "obs= [0.18767965 0.17732123 0.02605621 0.21967242] reward= 1.0 done= False info= {}\n",
            "step i 81 action= 1\n",
            "obs= [ 0.19122608  0.37206122  0.03044966 -0.06467876] reward= 1.0 done= False info= {}\n",
            "step i 82 action= 0\n",
            "obs= [0.1986673  0.17651623 0.02915608 0.23745357] reward= 1.0 done= False info= {}\n",
            "step i 83 action= 1\n",
            "obs= [ 0.20219763  0.37120978  0.03390515 -0.04589195] reward= 1.0 done= False info= {}\n",
            "step i 84 action= 0\n",
            "obs= [0.20962182 0.17561846 0.03298731 0.25729261] reward= 1.0 done= False info= {}\n",
            "step i 85 action= 1\n",
            "obs= [ 0.21313419  0.37025431  0.03813317 -0.02480582] reward= 1.0 done= False info= {}\n",
            "step i 86 action= 0\n",
            "obs= [0.22053928 0.17460683 0.03763705 0.27966048] reward= 1.0 done= False info= {}\n",
            "step i 87 action= 1\n",
            "obs= [ 0.22403142  0.36917224  0.04323026 -0.00091826] reward= 1.0 done= False info= {}\n",
            "step i 88 action= 0\n",
            "obs= [0.23141486 0.17345782 0.04321189 0.30508482] reward= 1.0 done= False info= {}\n",
            "step i 89 action= 1\n",
            "obs= [0.23488402 0.36793819 0.04931359 0.02633693] reward= 1.0 done= False info= {}\n",
            "step i 90 action= 1\n",
            "obs= [ 0.24224278  0.56231953  0.04984033 -0.2503883 ] reward= 1.0 done= False info= {}\n",
            "step i 91 action= 0\n",
            "obs= [0.25348917 0.36652258 0.04483256 0.0575894 ] reward= 1.0 done= False info= {}\n",
            "step i 92 action= 1\n",
            "obs= [ 0.26081962  0.560974    0.04598435 -0.22061828] reward= 1.0 done= False info= {}\n",
            "step i 93 action= 1\n",
            "obs= [ 0.2720391   0.75540952  0.04157199 -0.49844862] reward= 1.0 done= False info= {}\n",
            "step i 94 action= 0\n",
            "obs= [ 0.28714729  0.55972685  0.03160301 -0.19295958] reward= 1.0 done= False info= {}\n",
            "step i 95 action= 0\n",
            "obs= [0.29834183 0.36416741 0.02774382 0.10952286] reward= 1.0 done= False info= {}\n",
            "step i 96 action= 1\n",
            "obs= [ 0.30562518  0.55888103  0.02993428 -0.17427955] reward= 1.0 done= False info= {}\n",
            "step i 97 action= 0\n",
            "obs= [0.3168028  0.36334374 0.02644869 0.12769435] reward= 1.0 done= False info= {}\n",
            "step i 98 action= 1\n",
            "obs= [ 0.32406967  0.55807701  0.02900257 -0.15652839] reward= 1.0 done= False info= {}\n",
            "step i 99 action= 0\n",
            "obs= [0.33523121 0.36255208 0.02587201 0.14516123] reward= 1.0 done= False info= {}\n",
            "step i 100 action= 1\n",
            "obs= [ 0.34248226  0.55729415  0.02877523 -0.13924859] reward= 1.0 done= False info= {}\n",
            "step i 101 action= 0\n",
            "obs= [0.35362814 0.36177213 0.02599026 0.16237177] reward= 1.0 done= False info= {}\n",
            "step i 102 action= 1\n",
            "obs= [ 0.36086358  0.55651256  0.02923769 -0.12199994] reward= 1.0 done= False info= {}\n",
            "step i 103 action= 0\n",
            "obs= [0.37199383 0.36098419 0.0267977  0.17976193] reward= 1.0 done= False info= {}\n",
            "step i 104 action= 1\n",
            "obs= [ 0.37921352  0.55571262  0.03039293 -0.10434827] reward= 1.0 done= False info= {}\n",
            "step i 105 action= 0\n",
            "obs= [0.39032777 0.36016859 0.02830597 0.19776646] reward= 1.0 done= False info= {}\n",
            "step i 106 action= 1\n",
            "obs= [ 0.39753114  0.55487448  0.0322613  -0.08585454] reward= 1.0 done= False info= {}\n",
            "step i 107 action= 0\n",
            "obs= [0.40862863 0.3593053  0.03054421 0.21682977] reward= 1.0 done= False info= {}\n",
            "step i 108 action= 1\n",
            "obs= [ 0.41581474  0.55397759  0.0348808  -0.06606386] reward= 1.0 done= False info= {}\n",
            "step i 109 action= 0\n",
            "obs= [0.42689429 0.35837335 0.03355953 0.23741689] reward= 1.0 done= False info= {}\n",
            "step i 110 action= 1\n",
            "obs= [ 0.43406176  0.55300019  0.03830786 -0.04449432] reward= 1.0 done= False info= {}\n",
            "step i 111 action= 0\n",
            "obs= [0.44512176 0.35735045 0.03741798 0.26002472] reward= 1.0 done= False info= {}\n",
            "step i 112 action= 1\n",
            "obs= [ 0.45226877  0.55191882  0.04261847 -0.02062522] reward= 1.0 done= False info= {}\n",
            "step i 113 action= 0\n",
            "obs= [0.46330714 0.35621241 0.04220597 0.28519387] reward= 1.0 done= False info= {}\n",
            "step i 114 action= 1\n",
            "obs= [0.47043139 0.5507078  0.04790984 0.00611547] reward= 1.0 done= False info= {}\n",
            "step i 115 action= 1\n",
            "obs= [ 0.48144555  0.74511108  0.04803215 -0.27107474] reward= 1.0 done= False info= {}\n",
            "step i 116 action= 0\n",
            "obs= [0.49634777 0.5493378  0.04261066 0.03636252] reward= 1.0 done= False info= {}\n",
            "step i 117 action= 1\n",
            "obs= [ 0.50733453  0.74382362  0.04333791 -0.24257768] reward= 1.0 done= False info= {}\n",
            "step i 118 action= 0\n",
            "obs= [0.522211   0.54811029 0.03848636 0.06345404] reward= 1.0 done= False info= {}\n",
            "step i 119 action= 1\n",
            "obs= [ 0.5331732   0.74265991  0.03975544 -0.2168421 ] reward= 1.0 done= False info= {}\n",
            "step i 120 action= 0\n",
            "obs= [0.5480264  0.54699285 0.03541859 0.08811161] reward= 1.0 done= False info= {}\n",
            "step i 121 action= 1\n",
            "obs= [ 0.55896626  0.74158968  0.03718083 -0.19318969] reward= 1.0 done= False info= {}\n",
            "step i 122 action= 0\n",
            "obs= [0.57379805 0.54595613 0.03331703 0.11098648] reward= 1.0 done= False info= {}\n",
            "step i 123 action= 0\n",
            "obs= [0.58471718 0.350373   0.03553676 0.41399176] reward= 1.0 done= False info= {}\n",
            "step i 124 action= 1\n",
            "obs= [0.59172464 0.54497369 0.0438166  0.13272062] reward= 1.0 done= False info= {}\n",
            "step i 125 action= 1\n",
            "obs= [ 0.60262411  0.73944151  0.04647101 -0.14582316] reward= 1.0 done= False info= {}\n",
            "step i 126 action= 0\n",
            "obs= [0.61741294 0.54368593 0.04355455 0.16115076] reward= 1.0 done= False info= {}\n",
            "step i 127 action= 1\n",
            "obs= [ 0.62828666  0.73815816  0.04677756 -0.11747995] reward= 1.0 done= False info= {}\n",
            "step i 128 action= 0\n",
            "obs= [0.64304982 0.54239827 0.04442796 0.18958626] reward= 1.0 done= False info= {}\n",
            "step i 129 action= 1\n",
            "obs= [ 0.65389779  0.73685739  0.04821969 -0.08875707] reward= 1.0 done= False info= {}\n",
            "step i 130 action= 0\n",
            "obs= [0.66863494 0.54107862 0.04644455 0.21874083] reward= 1.0 done= False info= {}\n",
            "step i 131 action= 1\n",
            "obs= [ 0.67945651  0.73550695  0.05081936 -0.05893738] reward= 1.0 done= False info= {}\n",
            "step i 132 action= 0\n",
            "obs= [0.69416665 0.53969457 0.04964062 0.24933646] reward= 1.0 done= False info= {}\n",
            "step i 133 action= 1\n",
            "obs= [ 0.70496054  0.73407377  0.05462735 -0.02728482] reward= 1.0 done= False info= {}\n",
            "step i 134 action= 0\n",
            "obs= [0.71964201 0.5382127  0.05408165 0.28212099] reward= 1.0 done= False info= {}\n",
            "step i 135 action= 1\n",
            "obs= [0.73040627 0.73252321 0.05972407 0.00697362] reward= 1.0 done= False info= {}\n",
            "step i 136 action= 1\n",
            "obs= [ 0.74505673  0.92674003  0.05986354 -0.26628376] reward= 1.0 done= False info= {}\n",
            "step i 137 action= 0\n",
            "obs= [0.76359153 0.73081705 0.05453787 0.04466365] reward= 1.0 done= False info= {}\n",
            "step i 138 action= 1\n",
            "obs= [ 0.77820787  0.92511626  0.05543114 -0.23032565] reward= 1.0 done= False info= {}\n",
            "step i 139 action= 0\n",
            "obs= [0.7967102  0.72924784 0.05082463 0.07931415] reward= 1.0 done= False info= {}\n",
            "step i 140 action= 1\n",
            "obs= [ 0.81129516  0.92360575  0.05241091 -0.19691026] reward= 1.0 done= False info= {}\n",
            "step i 141 action= 0\n",
            "obs= [0.82976727 0.72777483 0.0484727  0.11183452] reward= 1.0 done= False info= {}\n",
            "step i 142 action= 1\n",
            "obs= [ 0.84432277  0.9221699   0.05070939 -0.16517019] reward= 1.0 done= False info= {}\n",
            "step i 143 action= 0\n",
            "obs= [0.86276616 0.72636011 0.04740599 0.14306912] reward= 1.0 done= False info= {}\n",
            "step i 144 action= 1\n",
            "obs= [ 0.87729337  0.9207722   0.05026737 -0.13428927] reward= 1.0 done= False info= {}\n",
            "step i 145 action= 0\n",
            "obs= [0.89570881 0.72496762 0.04758159 0.17381901] reward= 1.0 done= False info= {}\n",
            "step i 146 action= 1\n",
            "obs= [ 0.91020816  0.91937743  0.05105797 -0.10348195] reward= 1.0 done= False info= {}\n",
            "step i 147 action= 0\n",
            "obs= [0.92859571 0.72356235 0.04898833 0.2048624 ] reward= 1.0 done= False info= {}\n",
            "step i 148 action= 1\n",
            "obs= [ 0.94306696  0.91795076  0.05308558 -0.07197359] reward= 1.0 done= False info= {}\n",
            "step i 149 action= 0\n",
            "obs= [0.96142597 0.72210953 0.0516461  0.23697427] reward= 1.0 done= False info= {}\n",
            "step i 150 action= 1\n",
            "obs= [ 0.97586816  0.91645705  0.05638559 -0.0389811 ] reward= 1.0 done= False info= {}\n",
            "step i 151 action= 0\n",
            "obs= [0.99419731 0.72057376 0.05560597 0.27094545] reward= 1.0 done= False info= {}\n",
            "step i 152 action= 1\n",
            "obs= [ 1.00860878  0.91485995  0.06102488 -0.00369367] reward= 1.0 done= False info= {}\n",
            "step i 153 action= 0\n",
            "obs= [1.02690598 0.71891831 0.060951   0.30760186] reward= 1.0 done= False info= {}\n",
            "step i 154 action= 1\n",
            "obs= [1.04128435 0.9131212  0.06710304 0.03474697] reward= 1.0 done= False info= {}\n",
            "step i 155 action= 0\n",
            "obs= [1.05954677 0.71710434 0.06779798 0.34782402] reward= 1.0 done= False info= {}\n",
            "step i 156 action= 1\n",
            "obs= [1.07388886 0.91119976 0.07475446 0.07726711] reward= 1.0 done= False info= {}\n",
            "step i 157 action= 1\n",
            "obs= [ 1.09211285  1.10517488  0.0762998  -0.19092562] reward= 1.0 done= False info= {}\n",
            "step i 158 action= 0\n",
            "obs= [1.11421635 0.90904902 0.07248129 0.12481764] reward= 1.0 done= False info= {}\n",
            "step i 159 action= 1\n",
            "obs= [ 1.13239733  1.10306177  0.07497764 -0.14414653] reward= 1.0 done= False info= {}\n",
            "step i 160 action= 0\n",
            "obs= [1.15445857 0.90695059 0.07209471 0.17121655] reward= 1.0 done= False info= {}\n",
            "step i 161 action= 1\n",
            "obs= [ 1.17259758  1.10097058  0.07551904 -0.09787993] reward= 1.0 done= False info= {}\n",
            "step i 162 action= 0\n",
            "obs= [1.19461699 0.90485208 0.07356145 0.21764085] reward= 1.0 done= False info= {}\n",
            "step i 163 action= 1\n",
            "obs= [ 1.21271403  1.09884957  0.07791426 -0.05096085] reward= 1.0 done= False info= {}\n",
            "step i 164 action= 0\n",
            "obs= [1.23469102 0.90270188 0.07689505 0.26525171] reward= 1.0 done= False info= {}\n",
            "step i 165 action= 1\n",
            "obs= [ 1.25274506  1.09664691  0.08220008 -0.00222132] reward= 1.0 done= False info= {}\n",
            "step i 166 action= 0\n",
            "obs= [1.274678   0.90044818 0.08215565 0.31522269] reward= 1.0 done= False info= {}\n",
            "step i 167 action= 1\n",
            "obs= [1.29268696 1.09430962 0.08846011 0.04953794] reward= 1.0 done= False info= {}\n",
            "step i 168 action= 1\n",
            "obs= [ 1.31457315  1.2880591   0.08945087 -0.21397656] reward= 1.0 done= False info= {}\n",
            "step i 169 action= 0\n",
            "obs= [1.34033434 1.09177972 0.08517133 0.1055289 ] reward= 1.0 done= False info= {}\n",
            "step i 170 action= 1\n",
            "obs= [ 1.36216993  1.2855844   0.08728191 -0.15911422] reward= 1.0 done= False info= {}\n",
            "step i 171 action= 0\n",
            "obs= [1.38788162 1.08932831 0.08409963 0.1597776 ] reward= 1.0 done= False info= {}\n",
            "step i 172 action= 1\n",
            "obs= [ 1.40966818  1.28315185  0.08729518 -0.10523402] reward= 1.0 done= False info= {}\n",
            "step i 173 action= 0\n",
            "obs= [1.43533122 1.08689441 0.0851905  0.21366339] reward= 1.0 done= False info= {}\n",
            "step i 174 action= 1\n",
            "obs= [ 1.45706911  1.28070163  0.08946377 -0.05097745] reward= 1.0 done= False info= {}\n",
            "step i 175 action= 0\n",
            "obs= [1.48268314 1.08441834 0.08844422 0.26853729] reward= 1.0 done= False info= {}\n",
            "step i 176 action= 1\n",
            "obs= [1.50437151 1.27817408 0.09381496 0.00500837] reward= 1.0 done= False info= {}\n",
            "step i 177 action= 1\n",
            "obs= [ 1.52993499  1.47183412  0.09391513 -0.25666313] reward= 1.0 done= False info= {}\n",
            "step i 178 action= 0\n",
            "obs= [1.55937167 1.27550555 0.08878187 0.06410243] reward= 1.0 done= False info= {}\n",
            "step i 179 action= 1\n",
            "obs= [ 1.58488178  1.46924983  0.09006392 -0.19930179] reward= 1.0 done= False info= {}\n",
            "step i 180 action= 0\n",
            "obs= [1.61426678 1.27296288 0.08607788 0.12037832] reward= 1.0 done= False info= {}\n",
            "step i 181 action= 1\n",
            "obs= [ 1.63972604  1.46675285  0.08848545 -0.14395474] reward= 1.0 done= False info= {}\n",
            "step i 182 action= 0\n",
            "obs= [1.6690611  1.27048243 0.08560635 0.17527986] reward= 1.0 done= False info= {}\n",
            "step i 183 action= 1\n",
            "obs= [ 1.69447074  1.46428155  0.08911195 -0.08921674] reward= 1.0 done= False info= {}\n",
            "step i 184 action= 0\n",
            "obs= [1.72375638 1.26800279 0.08732762 0.23019745] reward= 1.0 done= False info= {}\n",
            "step i 185 action= 1\n",
            "obs= [ 1.74911643  1.46177535  0.09193156 -0.0337121 ] reward= 1.0 done= False info= {}\n",
            "step i 186 action= 0\n",
            "obs= [1.77835194 1.26546354 0.09125732 0.28650198] reward= 1.0 done= False info= {}\n",
            "step i 187 action= 1\n",
            "obs= [1.80366121 1.45917357 0.09698736 0.02393843] reward= 1.0 done= False info= {}\n",
            "step i 188 action= 1\n",
            "obs= [ 1.83284468  1.6527805   0.09746613 -0.23663756] reward= 1.0 done= False info= {}\n",
            "step i 189 action= 0\n",
            "obs= [1.86590029 1.45641096 0.09273338 0.08512848] reward= 1.0 done= False info= {}\n",
            "step i 190 action= 1\n",
            "obs= [ 1.89502851  1.65008973  0.09443595 -0.17691686] reward= 1.0 done= False info= {}\n",
            "step i 191 action= 0\n",
            "obs= [1.9280303  1.45375208 0.09089761 0.14400028] reward= 1.0 done= False info= {}\n",
            "step i 192 action= 1\n",
            "obs= [ 1.95710535  1.64746265  0.09377762 -0.1186789 ] reward= 1.0 done= False info= {}\n",
            "step i 193 action= 0\n",
            "obs= [1.9900546  1.45113091 0.09140404 0.20205494] reward= 1.0 done= False info= {}\n",
            "step i 194 action= 1\n",
            "obs= [ 2.01907722  1.64483483  0.09544514 -0.06045265] reward= 1.0 done= False info= {}\n",
            "step i 195 action= 0\n",
            "obs= [2.05197391 1.44848318 0.09423609 0.26075259] reward= 1.0 done= False info= {}\n",
            "step i 196 action= 1\n",
            "obs= [ 2.08094358e+00  1.64214241e+00  9.94511373e-02 -7.82956349e-04] reward= 1.0 done= False info= {}\n",
            "step i 197 action= 0\n",
            "obs= [2.11378643 1.44574524 0.09943548 0.32154761] reward= 1.0 done= False info= {}\n",
            "step i 198 action= 1\n",
            "obs= [2.14270133 1.63932095 0.10586643 0.06180422] reward= 1.0 done= False info= {}\n",
            "step i 199 action= 1\n",
            "obs= [ 2.17548775  1.83277827  0.10710251 -0.19569051] reward= 1.0 done= True info= {'TimeLimit.truncated': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpAuQGG7WCV0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}